[["index.html", "A Primer of Ecosystem Modeling - a work in progress 1 Prerequisites 1.1 Must do’s 1.2 A couple of useful citations", " A Primer of Ecosystem Modeling - a work in progress Hank Stevens, BIO 672 2022-01-10 1 Prerequisites For this R primer, you’ll need to read several chapters of Soetaert and Herman (2009) which are available on the Canvas website. Their book is an excellent source for those interested in ecosystem modeling, especially in aquatic systems. We will use bits of it along with this online text to get a little more comfortable with ecosystem processes and models. I also assume that you are reading a book on ecosystem ecology such as Chapin III, Matson, and Vitousek (2011). 1.1 Must do’s Read background material as needed. Install or update R (4.0.3, as of Jan 2021) Install or update RStudio Install select R packages: ‘deSolve,’ ‘tidyverse,’ ‘LakeMetabolizer`, ’lubridate’ Also, before doing most tasks in this primer, load deSolve and tidyverse with library(deSolve); library(tidyverse). The R working directory is the place R automatically looks for files. For simplicity, I will refer to the R working directory as Rwork. Therefore, Rwork/data would be a folder or directory called data inside Rwork. At any point in time, you can find out what your working directory is using getwd(). You can set your working directory using setwd(\"HD/MyStuff/Rwork\"), where “HD/MyStuff/Rwork” is the path to what you want your working directory to be. You can also use the “Session” pull-down menu in RStudio. 1.2 A couple of useful citations Ten simple rules for biologists learning to program. PLoS Computional Biology 14(1): e1005871. https://doi.org/10.1371/journal.pcbi.1005871 Ten simple rules for tackling your first mathematical models: A guide for graduate students by graduate students. PLOS Computational Biology 17(1): e1008539. https://doi.org/10.1371/journal.pcbi.1008539 References "],["intro.html", "2 Introduction ecosystem models 2.1 Why models? 2.2 What’s an ecosystem? 2.3 What’s a model? 2.4 Steps in modeling 2.5 An example in R", " 2 Introduction ecosystem models Primary source: pp. 1-17, (in Chapters 1, 2 of Soetaert and Hermann 2009) In this course, we’ll cover the very basics of ecosystem modeling. There are several goals I have for you, the reader. I hope that you, become more fluent in the discipline of ecosystem ecology; that you understand and can use basic terminology, and can identify quantitative pieces of the literature you read, and presentations you hear and see; understand and describe the quantitative and qualitative features of ecosystem dynamics and models of those dynamics; assess the relative merits of different modeling approaches and different mathematical formalisms of those approaches; create models of ecosystem dynamics of your own; write R code to implement ecosystem models. To do all this, the following text relies heavily on selected secondary sources especially Soetaert and Herman (2009). I also cite selected primary sources where appropriate. 2.1 Why models? In general, models are simplifications of reality. Useful models capture something useful about the reality we are interested in. A road map is a useful model of a road network. It captures just what we need. We model to aid understanding, because, at some level, the model is a quantitative and qualitative explanation for some phenomenon (Fig. 2.1). We can use models to test hypotheses, guide experiments, predict dynamics, and manage ecosystems and populations (Soetaert and Herman 2009). Figure 2.1: We use conceptual and mathematical models to interpret reality. (Fig. 1.3 in Soetaert and Hermann, 2009). 2.2 What’s an ecosystem? You can find definitions of an ecosystem elsewhere, but here we want to emphasize the abstraction of that ecosystem (Fig. 2.2). We will think of an ecosystem as a set of one or more compartments containing some mass of an element of interest, such as carbon or nitrogen. That element is the currency of the ecosystem. These compartments (a.k.a. pools, stocks) are connected by fluxes, or flows, the transfer per unit time of some mass of the element. When these transfers come from entirely outside the ecosystem, we refer to them as imports. When the transfer exits the system entirely, we refer to it as an exports. We model or describe an ecosystem as a set of pools or compartments, connected by fluxes, that is, transfers of energy or materials among pools. Figure 2.2: An ecosystem perspective of lake, in terms of phosphorus (Carpenter et al. 1992). Boxes are ecosystem compartments and the quantities are pools (a.k.a. stocks, units are mass per unit area or volume). Arrows are fluxes (units are mass per unit time). In Fig. 2.2, all pools are receiving imports from outside the system, represented by arrows coming from the amorphous cloud. All pools save dissolved P appear to export P back out. Dissolved P is receiving fluxes from all the animals in the system, and losing P to seston, which is primarily phytoplankton. Ecosystem fluxes or flows are influenced by state factors such as temperature, time, or disturbance which act as constraints that can limit or speed up the fluxes or determine the current states of the variables (Fig. 2.3). Figure 2.3: The current state of an ecosystem depends on state factors (Chapin III, Matson, and Vitousek 2011). 2.3 What’s a model? You’ve already seen ecosystem models. An ecosystem model consists of the compartments and fluxes we saw above. We refer to the currencies in different compartments as variables because they can vary or change through time. We use mathematical equations and computational controls to represent fluxes between compartments. In the equations, there are constants that we call parameters that control the rate of these fluxes. In principle, variables can usually be observed, whereas we cannot directly observe parameters and have to estimate them. We describe an ecosystem using balance equations for each pool (p. 18, Soetaert and Herman 2009). A balance equation is simply a bookkeeping or budgeting device to keep track of fluxes and estimate parameters: \\[\\mathrm{Change~in~pool} = \\mathrm{Sources - Sinks}\\] As models increase in complexity, we usually use differential equations, or time derivatives, to represent the balance equation for each pool or state variable in a model. For instance, the dissolved P in Fig. 2.2 could look like \\[\\frac{dD}{dt} = I + e_HH + e_N N + e_1 F_1 + e_2 F_2 - u_SS\\] where \\(D\\) is the amount of P in the dissolved pool, perhaps in mg\\(\\cdot\\)m\\(^{-2}\\), \\(I\\) is import, or source, from outside the system (dust? lake inflow?), and \\(H\\), \\(N\\), \\(F_1\\), and \\(F_2\\) are the other pools. The lower case letters are parameters or rate constants that are the mass-specific fluxes. The units of \\(dD/dt\\) are mg\\(\\cdot\\)m\\(^{-2}\\cdot\\)d\\(^{-1}\\), and so each term (e.g., \\(I\\) or \\(e_H H\\)) in the equation must also have these same units. units of \\(e_H H\\) must be mg\\(\\cdot\\)m\\(^{-2}\\)\\(\\cdot\\)d\\(^{-1}\\). units of \\(H\\) are mg\\(\\cdot\\)m\\(^{-2}\\). Ergo, units of \\(e_H\\) are d\\(^{-1}\\). More intuitively, the units of these rate constants are milligrams per square meter per day, per milligram per square meter of lake (mg\\(\\cdot\\)m\\(^{-2}\\cdot\\)d\\(^{-1}\\) / mg\\(\\cdot\\)m\\(^{-2}\\) in the lake). Note the quantities cancel out, and we are left with d\\(^{-1}\\). See Soetaert and Herman (2009) section 2.1.4 for further explanation. One nice feature of ecosystem ecology and ecosystem models is the emphasis on the conservation of mass and energy. We aim to track where everything comes from and where it goes. If we add up all of the rates of change for each pool, we should see that most of them cancel out, and we are left with only imports and exports. Doing this summation will tell us whether our model makes sense or we made a mistake. If we did not make any mistakes, it will tell us whether the entire system is a net sink or source of our currency. For instance, after simplifying the lake ecosystem model, we would have, \\[\\frac{dD}{dt} + \\frac{dS}{dt} + \\frac{dH}{dt} + \\frac{dN}{dt} + \\frac{dF_1}{dt} + \\frac{dF_2}{dt} = \\ldots \\ldots = \\mathrm{Imports - Exports}\\] See Soetaert and Herman (2009) section 2.1.3 for further explanation. State factors also enter into the model, typically altering the fluxes. For instance, increasing atmospheric temperature might force a predictable change in our rate constants (e.g., \\(e_H\\), \\(u_S\\)) in the above model. Because temperature is an external factor forcing a change to a parameter, we often call its role in a model a forcing function. We will add temperature to a model later in the book. To review: variables are quantities that change with time, such as the amount of carbon in the atmosphere, or the amount of phosphorus in the primary producers in a lake. Parameters are (typically) constants in equations that quantify how quickly the variables change. Forcing functions represent state factors that we think of as external to the ecosystem compartments. No models apply everywhere, all the time. All models are limited for a specific domain and with specific boundaries. These are the spatial, temporal and conceptual limits on a model. 2.3.1 Other ideas Statistical models (e.g., regression) often describe patterns in natural systems and help test hypotheses. They often represent a mechanistic process, but that is not typically their main goal. Process models (e.g., stock and flow models) also describe patterns in natural systems, but they include more mechanism and seek to describe mechanism and understand process. People sometimes call these mechanistic models. Figure 2.4: A statistical model of aboveground plant biomass as a function of available soil nitrogen. A theory is a well-supported hierarchical conceptual framework that contains clearly formulated postulates, from which a set of predictions logically follows. Efficient theory (Marquet et al. 2014) is based on first principles, and relies on as few assumptins as possible. In contrast to theory, models are specific implementations of theory and specific descriptions of nature. Remember that in principle, all models are wrong, but some are useful1. Exercise: Ask yourself whether a lake is a carbon sink or a source. Draw an appropriate compartment model to address this question. After having done so, ask yourself what assumptions you’ve made about the temporal and spatial scales. What mechanisms have you included? Why? 2.4 Steps in modeling Soetaert and Herman (2009) identify a series steps that guides model development and ultimately improve understanding and prediction (Fig. 2.5). Figure 2.5: It is helpful to use a series of steps in improving our models. This was Figure 1.7 in Soetaert and Hermann (2009) but I chopped it in half. Read the left half (top to bottom), then the right half (top bottom). In brief, I paraphrase or plagiarize their steps thus: Problem statement or question = narrow the focus of interest in a real system. System conceptualization = represent a real system as an abstraction. Model formulation = represent the conceptual model in mathematical form. Model verification = check internal consistency, consistency with physical laws of conservation, and ability to reproduce known solutions. Parameterization = use the literature or other sources to help determine numerical values for parameters; fine tuning these on the basis of model output is model calibration. Model calibration = use model output to fine tune model parameters, often referred to, or related to, model fitting. Sensitivity analysis = use model output to understand the effect of variation in parameters and state variables on dynamics. Note that this figure contains a loop – after calibration and sensitivity analysis, we loop back the model structure and its parameters. In this class, you will use sensitivity analysis to identify the most interesting or most important parts of your model. 2.5 An example in R Here we create a simple model to illustrate some of what we’ve been describing. Consider our lake above, but as a whole – just one big pool of P. We’ll let the imports, \\(I\\), be constant and independent of the amount of the P in the lake. Our exports will depend on the amount of the P in the system-the more P in the system, to more can be exported. We will assume that a constant fraction, \\(a\\), is exported, \\[\\frac{dP}{dt} = I - a P\\] In R, we write a function for the system of differential equations, and include descriptive comments. ## A function called &#39;lake&#39; to represent an ordinary differential equation (ODE) ## to use with ode() in the deSolve package. lake &lt;- function( time, state_vars, parameters ){ ## ODE function for a single pool (e.g., a lake) with a constant ## rate of import, and first-order export function (depends on the pool). ## The arguments time, state_vars, and parameters will be used by ode() ## to solve (numerically integrate) the system. ## I is the constant input rate ## a is the constant mass-specific loss rate ## P is the state variable P &lt;- state_vars[1] # state variable(s) with(as.list(parameters), # tell R to look inside &#39;parameters&#39; { dP.dt &lt;- I -a*P # the balance eq. or rate func. return( list( dP.dt ) ) # return the value of the rate function }) } Next, we tell R what the values of the parameters are that we want. Let’s say the the import is 3 mg\\(\\cdot\\)m\\(^{-2}\\cdot\\)d\\(^{-1}\\), and that the fraction (or rate, really) is 0.5 d\\(^{-1}\\), or more intuitively, mg\\(\\cdot\\)m\\(^{-2}\\cdot\\)d\\(^{-1}\\) per mg\\(\\cdot\\)m\\(^{-2}\\) in the lake. Note the quantities cancel out, and we are left with d\\(^{-1}\\). ## parameters p &lt;- c(I = 10, # constant input rate mg / m^2 / day a = .5 # mass-specific output rate, day^-1 ) Now the exciting part. We tell R our starting point and what time points we want to integrate for, and then solve the differential equation and have R return the result for the time points we want. After that, we display the first five time points. initial.state &lt;- c(P=1) # Initial concentration of P in the lake t &lt;- seq(from=0, to=10, by=1/24) # 10 days, in hourly increments ## solve the equation in func, using parameter values in p, ## starting at &#39;initial.state&#39; and return the time points t out &lt;- ode(y=initial.state, times = t, func = lake, parms = p) ## display the first five rows of the solution out[1:5,] ## time P ## [1,] 0.00000000 1.000000 ## [2,] 0.04166667 1.391740 ## [3,] 0.08333333 1.775400 ## [4,] 0.12500000 2.151151 ## [5,] 0.16666667 2.519155 Pictures are informative, so here we plot the result. ## simple graph of the time series plot(out) We can also use ggplot2 to make a pretty graph, and then save it. out2 &lt;- as.data.frame(out) # re-classify the data set ggplot(data=out2, aes(x=time, y=P)) + geom_line() ggsave(&quot;myLake.png&quot;, height=4, width=4) In very simple situations, we can solve the equilibrium by hand. By definition, the equilibrium is a state at which the system stops changing, that is, its rate of change is zero. To find a value of \\(P\\) which is an equilibrium, we set the balance equation equal to zero, and solve for \\(P\\): \\[\\frac{dP}{dt} = 0 = I -aP\\] \\[P^* = \\frac{I}{a}\\] By convention, we denote the equilibrium value of \\(P\\) with an asterisk, or “star,” as in “P-star.” We call this the analytical solution. Questions: Is our graph consistent with the analytical prediction of \\(P^*\\)? Determine the units of \\(I\\) and \\(a\\); show your work and explain it to your cat. In more complex systems, we often (typically?) can’t solve for the analytical solution. Instead, to find the same result, we usually run a model for a long period of time, until the state variables stop changing very much. Alternatively, if we have a model of a real ecosystem, we made be interested in its short term dynamics, in addressing questions related to experimental results, making predictions about the consequences of landscape management, or in using a time series to compare different models. References "],["N.html", "3 Describing a nitrogen budget 3.1 Getting started 3.2 Mathematical forms 3.3 Paramaterization 3.4 Mathematical solution 3.5 Add self-limitation", " 3 Describing a nitrogen budget Background Readings: pp. 17-35 (Soetaert and Hermann 2009) Bormann et al. 1977. Nitrogen Budget for an aggrading northern hardwood forest ecosystem. Science 196:981-983. Figure 3.1: Nitrogen budget for a temperate northern hardwood forest (Hubbard Brook Watershed 6, Bormann et al. 1977). Figure 3.2: A simpler compartment model for Hubbard Brook Watershed 6, based on Bormann et al. (1977). 3.1 Getting started (S&amp;H 15-17) For any ecosystem, we want to start with paper and pencil and sketch out the pools and the fluxes that we think are important (see example, Fig. 3.2). Don’t sweat the details yet – you’ll revise it anyway. Go ahead an label the compartments and the fluxes with with both biological and physical processes as well as some simple abstract notation. Write balance equations (S&amp;H, 3, 17-20). For each flux, identify the underlying biological and physical processes. If you don’t completely know, don’t sweat it. Here are the balance equations for Fig. 3.2: Rate of change of vegetation = uptake - exudates and through fall - leaf and root litter loss \\[\\frac{dV}{dt} = f_1 - f_2 - f_3\\] Rate of change in available pool in soil solution = bulk precip + exudates and through fall + net mineralization - uptake - stream export \\[\\frac{dA}{dt} = I_1 + f_2 + f_4 - f_1 - E_1\\] Rate of change in the bound pool = net N fixation + leaf and root litter loss - net mineralization - stream export \\[\\frac{dB}{dt} = I_2 + f_3 - f_4 - E_2\\] Assess the total load or mass budget (S&amp;H, 19-22). For Fig. 3.2, we have \\[\\frac{d\\left(V+A+B\\right)}{dt} = \\frac{dV}{dt} + \\frac{dA}{dt} + \\frac{dB}{dt} = (f_1 - f_2 - f_3) + (I_1 + f_2 + f_4 - f_1 - E_1) + (I_2 + f_3 - f_4 - E_2)\\] \\[\\frac{d\\left(V+A+B\\right)}{dt} = I_1 + I_2 + f_1 - f_1 + f_2 - f_2 + f_3 - f_3 + f_4 - f_4 - E_1 - E_2\\] \\[\\frac{d\\left(V+A+B\\right)}{dt} = I_1 + I_2 - E_1 - E_2\\] From this we see that the total mass budget for this watershed depends simply on import or inputs, and exports or outputs. For each flux, decide which pools directly influence the flux. In a forest, the flux of nitrogen from vegetation to soil will depend enormously on the amount of vegetation present as leaves senesce and fall, fine roots die back, and water leaches nutrients out of leaves and bark. The flux is very unlikely to depend directly on the amount of nitrogen already in the soil. In a lake, the flux of phosphorous from the water column into phytoplankton depends on the amounts of both the amount of phytoplankton and the amount of phosphate in the water at any given instant. For the vegetation pool in Fig. 3.2, we see each flux (\\(f\\)) is a Function (\\(F()\\)) of one or two pools: Uptake: \\(f_1 = F(V,A)\\) Leaf and root litter, and throughfall: \\(f_3 = F(V)\\) Root exudates, and throughfall: \\(f_4 = F(V)\\) Next, we want to represent these fluxes in a mathematical form. 3.2 Mathematical forms A common starting point for dynamics depending on two pools is the law of mass action. This states that the reaction rate is proportional the product of the pools. In the case of plant uptake of N, which depends on the amounts of N in the available pool and the vegetation pool, this would be \\(aVA\\), where \\(a\\) is a proportionality constant. In some circumstances, these pools might also have exponents different than one (\\(aV^1A^1\\)), such as \\(aVA^2\\). This occurs in chemistry when a reaction requires two molecules of “A” for each molecule of “V.” It might occur in ecology if a rate depends differentially on A and B. Using the law of mass action for plant uptake, we will describe the fluxes in the simple N budget above (Fig. 3.2) with the following expressions. \\[\\begin{align} \\frac{dV}{dt} &amp;= a_{1}AV - a_{2}V - a_3 V\\\\ \\frac{dA}{dt} &amp;= I_1 + a_{2}V + a_{4}B - a_{5}A - a_{1}AV\\\\ \\frac{dB}{dt} &amp;= I_2 + a_{3}V - a_{4}B - a_{6}B \\end{align}\\] Take the time to identify each term and think about the biology or physics that might govern each term. 3.3 Paramaterization Parameteriztion is what we call assigning numerical values to mathematical parameters. Here, we find initial estimates for the parameters in our model. We use the literature for this purpose (Bormann, Likens, and Melillo 1977). If we have the data (we do) and relatively simple mathematical forms (we do), it is fairly straightforward to estimate parameters. For instance, we decided that net mineralization would be directly proportional to the size of the organic pool, \\(B\\), that is, \\(F_4 = a_4B\\). To calculate \\(a_4\\), we substitute data where we can, and solve what we need. \\[\\begin{align*} F_2 &amp;= a_4 B\\\\ 69.6 &amp;= a_4 4700 \\\\ a_4 &amp;= \\frac{69.6}{4700}\\\\ a_4 &amp;\\approx 0.015 \\end{align*}\\] We use the same approach for second order equations as well. Table 3.1: Parameters, variables, units and estimates for a simplified model of Bormann et al. (1977). All fluxes (\\(dX/dy\\)) are in units of kg ha\\(^{-1}\\) y\\(^{-1}\\). (Note that calculations should not be included in your final table, but are presented here for comparison to your own calculations.) p.or.V units Estimate \\(A, B, V\\) state variables kg ha\\(^{-1}\\) \\(26, 4700, 532\\) \\(a_1\\), uptake rate by V from A (kg ha\\(^{-1}\\))\\(^{-1}\\) y\\(^{-1}\\) \\(79.6 / (26 \\cdot 532) = 0.0058\\) \\(a_2\\), loss rate from V to A y\\(^{-1}\\) \\((6.6 + 0.8) / 532 = 0.014\\) \\(a_3\\), loss rate from V to B (kg ha\\(^{-1}\\))\\(^{-1}\\) y\\(^{-1}\\) \\((54.2 + 2.7 + 0.1 + 6.2 ) / 532 = 0.00022\\) \\(a_4\\), mineralization y\\(^{-1}\\) \\(69.6 / 4700 = 0.015\\) \\(a_5\\), export from A y\\(^{-1}\\) \\(3.9 /26 = 0.15\\) \\(a_6\\), export from B y\\(^{-1}\\) \\(0.1/4700 = 0.000021\\) \\(I_1\\), bulk precip kg ha\\(^{-1}\\) y\\(^{-1}\\) \\(6.5\\) \\(I_2\\), N fixation kg ha\\(^{-1}\\) y\\(^{-1}\\) \\(14.2\\) Enter parameters into R. params &lt;- c( i1 = 6.5, # precip i2 = 14.2, # fixation a1 = 79.6 / (26 * 532), # uptake a2 = (6.6 + 0.8) / 532, # throughfall and inorganic exudates a3 = (54.2 + 2.7 + 0.1 + 6.2 ) / 532, # litter, throughfall, organic exudates a4 = 69.6 / 4700, # net mineralization a5 = 3.9 /26, # export from available a6 = 0.1/4700 #export from bound ) # close parentheses params ## i1 i2 a1 a2 a3 a4 ## 6.500000e+00 1.420000e+01 5.754772e-03 1.390977e-02 1.187970e-01 1.480851e-02 ## a5 a6 ## 1.500000e-01 2.127660e-05 3.4 Mathematical solution The mathematical solution is the process of making the predictions using our model and our parameters. We solve the model. With simple models, we can sometimes find analytical solutions. For most ecosystem models, we have to solve the models numerically using numerical integration. Here we write a function that includes our system of differential equations. This will allow R to integrate change through time. bormann1 &lt;- function(t, y, p) { # time, vector of state variables and parameters must be in this order # we can use as.list for both the state variables and parameters # a1 = uptake # a2 = loss from veg to avail # a3 = loss from veg to bound # a4 = net mineralization # a5 = export from avail # a6 = export from bound with( as.list( c(y, p) ), { dV.dt &lt;- a1 * A * V - a2 * V - a3 * V dA.dt &lt;- i1 + a2 * V + a4 * B - a1 * A * V - a5 * A dB.dt &lt;- i2 + a3 * V - a4 * B - a6 * B # Here we return a LIST whose first element is the vector of # rates of change for the state variables. The first element must be these rates, # in the same order as the state variables in y # The second element, total, is the total N in the system return(list( c(dV.dt, dA.dt, dB.dt), total = V + A + B ) )}) } Now that we have the function, we tell R what to do with it. We will define the initial state of the system, and then tell R which time point we want it to return. The initial state of the system is the set of starting values for the state variables. We could choose any values, but I select the values given in Bormann, Likens, and Melillo (1977). ## starting values in kg/ha initial.state &lt;- c( V = 532, A = 26, B = 4700) Finally, let’s solve the system, and have R return the first 20 years. time &lt;- 0:20 out &lt;- ode(y = initial.state, times=time, func=bormann1, parms = params) head(out) # the first 6 lines ## time V A B total ## [1,] 0 532.0000 26.00000 4700.000 5258.000 ## [2,] 1 540.7799 25.76598 4708.168 5274.714 ## [3,] 2 548.7920 25.47149 4717.208 5291.472 ## [4,] 3 556.0406 25.21597 4727.013 5308.270 ## [5,] 4 562.6178 24.99710 4737.489 5325.104 ## [6,] 5 568.6085 24.80918 4748.550 5341.968 Use pivot_longer() and ggplot() to make a graph. We use pivot_longer() gather multiple columns into one with a new name (values_to=kg.N), keeping track of the names of the original columns in a new column (names_to=State.var). We can use pivot_wider() if we ever want to spread those columns back out. outL &lt;- out %&gt;% # select the data set as.data.frame() %&gt;% # make sure it is a data frame and not a matrix ## and the rearrange pivot_longer( cols=-time, names_to=&quot;State.var&quot;, values_to=&quot;kg.N&quot;) ## plot the dynamics ggplot(outL, aes(x=time, y=kg.N)) + # select variables to plot geom_line() + # select the form of the graph facet_wrap(~State.var, scale=&quot;free_y&quot;) # separate each state variable and plot each on its on scale. Figure 3.3: Dynamics of a simple N budget, based on Bormann et al. (1977). In some ways, we have been moderately successful in our first pass at converting a purely budgetary model into a dynamic process model. We mimicked total load, and see similar changes through time of all the state variables. Questions to ponder We replicated approximately the N budget of Bormann et al. (1977), but clearly vegetation cannot keep accumulating N indefinitely. What are our next steps? 3.5 Add self-limitation One logical step is to assume that as vegetation eventually gets limited by some factor or resource that is not in our model. If, at first approximation, the vegetation reaches a carrying capacity independent of high resource availability, we can use an approximation suggested by Soetaert and Hermann (2009) for self-limitation, \\[f(X)V\\left(1-\\frac{V}{K}\\right)\\] where \\(f(X)\\) is everything else that regulates mass-specific growth rate. Exercise Include self-limitation in your model of vegetation, estimate \\(K\\), and produce output. Remember that following our template, we have a maximum rate times resource and self limitation, and inhibition. Currently, we have \\[\\frac{dV}{dt} = a_{1}AV - a_{2}V - a_3 V\\] and rearranging, \\[\\frac{dV}{dt} = \\left(a_{1}A - a_{2} - a_3\\right) V\\] If we add self-limitation, we get \\[\\frac{dV}{dt} = \\left(a_{1}A - a_{2} - a_3\\right) V \\left(1-\\frac{V}{K}\\right)\\] where \\(K\\) is the maximum amount of live vegetation that the ecosystem can sustain, in kg,N,ha\\(^{-1}\\). We don’t know exactly what that is yet, but we may be able to get estimates from the literature. For know we can pretend that it is just a bit more than was there in the mid-1970s, say, \\(K=600\\). Now we rewrite the R function with self-limitation. bormann2 &lt;- function(t, y, p) { # time, vector of state variables and parameters must be in this order # we can use as.list for both the state variables and parameters # a1 = uptake # a2 = loss from veg to avail # a3 = loss from veg to bound # a4 = net mineralization # a5 = export from avail # a6 = export from bound with( as.list( c(y, p) ), { dV.dt &lt;- (a1 * A - a2 - a3) * V * (1-V/K) dA.dt &lt;- i1 + a2 * V + a4 * B - a1 * A * V - a5 * A dB.dt &lt;- i2 + a3 * V - a4 * B - a6 * B # Here we return a list whose first element is the vector of # rates of change for the state variables. The first element must be these rates, # in the same order as the state variables in y # The second element is the total N in the system return(list( c(dV.dt, dA.dt, dB.dt), total = V + A + B ) )}) } We can add a new parameter to our vector of parameters, and then solve our new function bormann2 for the same time interval, and plot it. params[&quot;K&quot;] &lt;- 600 ## starting values in kg/ha initial.state &lt;- c( V = 532, A = 26, B = 4700) time &lt;- 0:200 out &lt;- ode(y = initial.state, times=time, func=bormann2, parms = params) outL2 &lt;- out %&gt;% as.data.frame() %&gt;% pivot_longer(cols=-time, names_to=&quot;State.var&quot;, values_to=&quot;kg.N&quot;) ggplot(outL2, aes(time, kg.N)) + geom_line() + facet_wrap(~State.var, scale=&quot;free_y&quot;) Figure 3.4: Dynamics of an N budget, assuming density-dependence in vegetation with a fixed carrying capacity (Bormann et al. 1977). Unlike our first model of this system, we see state variables on curved trajectories and perhaps reaching asymptotes. This makes greater intuitive sense - over the short term, it is the same as the simple N budget shown in Bormann, Likens, and Melillo (1977) and it also shows a reasonable longterm trajectory for the vegetation, and the predicted consequences for the available and bound pools. Save your own R script of ‘bormann2’ Make a new script that contains nothing but the bormann2 function. Copy the block of text where we define it, paste it into a new R script, and save it as bormann2.R. We will use this function in a latter section. References "],["lake-metabolism.html", "4 Lake Metabolism 4.1 Estimating Productivity 4.2 By hand, in R 4.3 The LakeMetabolizer package", " 4 Lake Metabolism In this chapter, you’ll study how lakes breath. You’ll get real data from Acton Lake, look it, and measure the rate of an inhale and an exhale. You’ll do that by hand, by hand with a spreadsheet, by hand with R, and finally using an R package, LakeMetabolizer, which is designed to do that and much more. Figure 4.1: We can use lake oxygen level to measure net ecosystem production. 4.1 Estimating Productivity Most cells respire to do the work of growth and maintenance by consuming oxygen and using it as the final electron acceptor when O\\(_2\\) is reduced, creating water. Because individuals comprise cells, and ecosystems comprise individuals, ecosystems respire too, and we can measure their metabolic rate using oxygen consumption and production. We measure whole ecosystem metabolic rate as net ecosystem productivity which is the difference between gross primary productivity and respiration. Putting all of this in the same units of oxygen allows us to measure the rate, \\[NEP = GPP - R\\] If we assume that water column oxygen is correlated with the rates of photosynthesis, respiration, and net primary production, then we can think of the slopes of oxygen concentration vs. time as rates of respiration and net primary production. If we assume that respiration is constant throughout the 24 h cycle, we calculate GPP as the sum of NEP and R. 4.2 By hand, in R Let’s practice R by importing, wrangling, and graphing data, then calculating slopes and estimating respiration, NPP, and GPP. Start by obtaining data for Acton Lake (acton.csv). Place it inside a folder labelled ‘data’ inside your working directory. Here you read in data and check whether it loaded properly. ## Either change the path name in this function, or ## make sure you have a folder named &#39;data&#39; inside your working directory acton &lt;- read_csv(&quot;data/acton.csv&quot;, ## tell R the format of one of your variables col_types = cols(date_time = col_datetime(format = &quot;%m/%d/%y %H:%M&quot;)), skip = 1# skips the first line of metadata ) # acton &lt;- read_csv(&quot;data/acton.csv&quot;, # ## tell R the format of one of your variables # col_types = cols(date_time = # col_character()), # skip = 1 # skips the first line of metadata # ) # acton$date_time &lt;- as.POSIXlt(acton$date_time, # format=&quot;%m/%d/%y %H:%M&quot;) # summary(acton$date_time) # acton &lt;- acton %&gt;% # mutate(date_time = mdy_hm(date_time)) # summary(acton) # library(lubridate) # date_time &lt;- mdy_hm(acton$date_time) # summary(date_time) Let’s check our understanding of lake oxygen dynamics by plotting the time series. Ggplot understands what to do with dates. ggplot(acton, aes(x=date_time, y=O_mg.L)) + geom_path() Figure 4.2: *Oxygen dynamics from Acton Lake. If we want to calculate the slope of the night time oxygen concentration, then we should identify what the endpoints of “day time” are so that we can do analyses on just daylight or nighttime data. Working with times in R is a little tricky, because times and dates, periods, durations, and intervals are inherently tricky. The date_time variable in the original data set contains all the information, and we extra tidbits from it. # determine day/night intervals ## Sunrise morning &lt;- 6.25 ## sunset evening &lt;- 21 acton &lt;- mutate(acton, ## dates, as factors. date = as.factor( format(date_time, &quot;%Y-%m-%d&quot;, tz=&quot;America/New_York&quot;) ), ## hour in decimal format hour.d = as.integer( format(date_time, &quot;%H&quot;)) + as.numeric(format(date_time, &quot;%M&quot;))/60, ## just the daylight hours daytime.d = ifelse(hour.d &gt;= morning &amp; hour.d &lt; evening, hour.d - morning, NA), ## just the nighttime hours nighttime.d = ifelse( hour.d &gt;= evening, hour.d-evening, ifelse(hour.d &lt; morning, 24 - evening + hour.d, NA)), ## an indicator of whether it is day or night daylight = as.factor( ifelse(hour.d &gt;= morning &amp; hour.d &lt; evening, &quot;day&quot;, &quot;night&quot;)) ) ## identify different day/night cycles, one cycle = one day ## first measurement of the day ( first &lt;- which(acton$hour.d==6.25) ) ## [1] 1 97 193 289 385 481 577 ## day 1 is in rows 1-96, day 2 is in rows 97-192, etc. ( rows.per.day &lt;- diff(first) ) ## [1] 96 96 96 96 96 96 ## there are 96 observations of each cycle ## make a new categorical variable, day acton &lt;- acton %&gt;% mutate(day=as.factor( rep(1:7, each=96))) The slope of oxygen concentration at night is respiration, \\(R\\). The slope of the oxygen concentration during the day is gross primary production minus respiration, or net ecosystem production. We will pull out one day’s worth of data and make a quick plot of the time series. acton2 &lt;- filter(acton, day==&quot;1&quot;) ## or # library(lubridate) # acton2 &lt;- filter(acton, # date_time &gt;=ymd_hms(&quot;2013-06-27 06:15:00&quot;) &amp; # date_time &lt;= ymd_hms(&quot;2013-06-28 06:00:00&quot;) ) qplot(x = date_time, y=O_mg.L, data=acton2, geom = &quot;path&quot;) The nighttime slope is the regression line for just the nighttime data # select the data frame, filter for &#39;date&#39; values within a range of dates names(acton2) ## [1] &quot;date_time&quot; &quot;cum_h&quot; &quot;O_mg.L&quot; &quot;O_perc&quot; &quot;temp&quot; ## [6] &quot;date&quot; &quot;hour.d&quot; &quot;daytime.d&quot; &quot;nighttime.d&quot; &quot;daylight&quot; ## [11] &quot;day&quot; night.R &lt;- lm(O_mg.L ~ nighttime.d, data=acton2) ## intercept and slope of nighttime regression coef(night.R) ## (Intercept) nighttime.d ## 10.2093741 -0.2405216 day.NEP &lt;- lm(O_mg.L ~ daytime.d, data=acton2) ## intercept and slope of daytime regression coef(day.NEP) ## (Intercept) daytime.d ## 7.447288 0.294097 To calculate time-averaged, 24 hour NEP, we assume that all GPP occurred during the day, and respiration is constant. Our day is 14 h 45 min, or 14.75. ## as.numeric() drops the name of the slope we used R24 &lt;- as.numeric( abs( coef(night.R)[2] ) ) NEP24 &lt;- as.numeric( coef(day.NEP)[2] * 14.75/24 ) GPP24 &lt;- NEP24 + R24 R24; NEP24; GPP24 ## [1] 0.2405216 ## [1] 0.1807471 ## [1] 0.4212687 These units are mg O\\(_2\\) / L / h. Here is what every night looks like. nights &lt;- filter(acton, daylight==&quot;night&quot;) ggplot(nights, aes(x=nighttime.d, y=O_mg.L, colour=day, linetype=day)) + geom_point() + # plot points geom_smooth(method=&quot;lm&quot;, se=FALSE) # fit linear models each night We can calculate the average slope for all nights, forcing a straight line through each night’s data. The estimates of uncertainty and the P values won’t make sense because the data are horribly autocorrelated, but we can rely on the estimates of the coefficients, and the average slope, in particular. ntd &lt;- acton$nighttime.d/24 m.resp &lt;- lm(O_mg.L ~ nighttime.d + day, data=acton) night.O2.rate &lt;- coef(m.resp)[&quot;nighttime.d&quot;] night.O2.rate ## nighttime.d ## -0.1969112 The estimate for night.d, -0.197, is our estimate of the respiration rate in mg_O\\(_2\\)/L per hour. We usually report this per day, which would just be 24 times as great. Here is what each day looks like. Recall the at daytime oxygen increase is GPP-R or NEP. days &lt;- filter(acton, daylight==&quot;day&quot;) ggplot(days, aes(x=daytime.d, y=O_mg.L, colour=day)) + geom_point() + # plot points geom_smooth(method=&quot;lm&quot;, se=FALSE) # fit linear models each night If we want to, we could throw out the days that have negative slopes, that is, days 6 and 7. acton3 &lt;- filter(acton, !(day == 6 | day == 7) ) # NOT day 6 OR 7 md.resp &lt;- lm(O_mg.L ~ daytime.d + day, data=acton3) NEP.day &lt;- coef(md.resp)[&quot;daytime.d&quot;] NEP.day ## daytime.d ## 0.2464898 To get 24 hour NEP, we have to weight daytime and nighttime GPP by the daylength. We stipulated that our morning began at 6:15 AM and ended at 9 PM, or a day time of 14 h 45 min, or about 61% of the 24 h cycle. Also, remember that respiration is positive, even though we are measuring it with a negative slope. NEP24 &lt;- as.numeric( NEP.day * 14.75/24 ) R24 &lt;- as.numeric( abs( night.O2.rate ) ) GPP24 &lt;- NEP24 + R24 # as.numeric drops the element name GPP24 ## [1] 0.3483997 Recall these are units of mg O\\(_2\\) / L / h. For units per day, we have c(GPP24, NEP24, R24) * 24 ## [1] 8.361593 3.635724 4.725869 4.3 The LakeMetabolizer package Here we do something similar but in a much more sophisticated way. To estimate the net ecosystem productivity, we need to know, at least, gas exchange rates, equilibrium oxygen saturation, the mixing depth, and the daylight hours. The metab function in LakeMetabolizer calculates GPP, R, and NEP given requisite data. The function can use several different approaches, depending upon what data you have and your quantitative preferences. Here we use the simplest approach, which the authors refer to as simple bookkeeping. First we load the package and then examine the help page for metab.bookkeep. # install.packages(&quot;LakeMetabolizer&quot;, dep=TRUE) library(LakeMetabolizer) ?metab.bookkeep On the help page you learn about how to use this function. Here we walk through the steps for acquiring or making educated guesses about the data we need. Here we acknowledge that the change in dissolved oxygen is a function of NEP and also the flux of oxygen due to diffusion, \\[\\Delta \\mathrm{DO} = \\mathrm{NEP}_{t-1} \\cdot \\Delta t + F_{t-1}\\] where \\(F_{t-1}\\) is the flux of oxygen due to diffusion (Winslow et al. 2016). This rate of diffusion depends on wind speed, temperature, lake size, and, ultimately, how much the of the lake waters mix (Winslow et al. 2016). Here we will simply provide a value for the gas exchange constant and a mixing depth. From its calculations, LakeMetabolizer can give us NEP, R, and GPP. This is because average respiration is the average difference between DO and \\(F\\), and NEP is the difference between GPP and R. LakeMetabolizer needs data in a particular form, and so we build that next. ## pick a reasonable gas exchange constant and mixing depth k.gas &lt;- 0.4 z.mix &lt;- 3 # meters of well mixed surface waters ## let midday PAR be 1300 mol photons/m^2/sec at water surface ## estimate ### dissolved oxygen at saturation (equilibrium) using the function, o2.at.sat.base() ### irradiance in PAR acton.LM &lt;- acton %&gt;% mutate( do.sat=o2.at.sat.base(temp, altitude=283), #saturated DO day= as.numeric(is.day(date_time, lat=39.5)), # day vs. night par = 1300 * sin(daytime.d/14.5 * pi), # PAR irr = ifelse(is.na(par), 0, par), # recode par to make night = 0 z.mix = z.mix, # add mixing depth k.gas = k.gas, # add gas exchange coefficient wtr=temp # rename observed water temperature ) %&gt;% ## select only some of the columns select(datetime=date_time, do.obs=O_mg.L, do.sat=do.sat, k.gas=k.gas, z.mix=z.mix, irr=irr, wtr=wtr) %&gt;% as.data.frame() # simplify the data structure (class tbl_df screws things up) One of the methods in LakeMetabolizer, ‘bookkeep,’ requires irradiance to be 0 or 1. We make a new data set with that. acton.bk &lt;- acton.LM %&gt;% mutate(irr = as.numeric( irr&gt;0 ) ) # converts TRUE/FALSE to 1,0 Finally, we use three different methods to estimate GPP, R, and NEP. ## calculate GPP, R, NEP in mg O2 / L / day out.b &lt;- metab(acton.bk, method=&quot;bookkeep&quot;) ## [1] &quot;Points removed due to incomplete day or duplicated time step: 96&quot; ## [1] &quot;NA&#39;s added to fill in time series: 0&quot; out.k &lt;- metab(acton.LM, method=&quot;kalman&quot;) ## [1] &quot;Points removed due to incomplete day or duplicated time step: 96&quot; ## [1] &quot;NA&#39;s added to fill in time series: 0&quot; out.m &lt;- metab(acton.LM, method=&quot;mle&quot;) ## [1] &quot;Points removed due to incomplete day or duplicated time step: 96&quot; ## [1] &quot;NA&#39;s added to fill in time series: 0&quot; ## combine data sets - first label each dataset out.b$method &lt;- &quot;bookkeep&quot; out.k$method &lt;- &quot;kalman&quot; out.m$method &lt;- &quot;mle&quot; ## now combine out.all &lt;- full_join( full_join(out.b, out.k), out.m) ## ... and stack the variables into a long form out.l &lt;- out.all %&gt;% pivot_longer(cols=GPP:NEP) ggplot(out.l, aes(doy, value, color=method)) + geom_line() + facet_grid(.~name) How does this compare with our previous estimate? What would happen if our gas exchange constant differs (Cole et al. 2010)? "],["npzd---a-simple-aquatic-ecosystem.html", "5 NPZD - a simple aquatic ecosystem 5.1 This task", " 5 NPZD - a simple aquatic ecosystem This section relies entirely on selections in pages 31-58 of Soetaert and Herman (2009). The object of this chapter is to understanding and explore a simple model of a nitrogen-limited aquatic ecosystem consisting of an inorganic pool, phytoplankton, zooplankton, and particulate detritus. The background reading is the following sections: 2.5-2.5.5: Functional responses and rate limitations. 2.6-2.6.2: Couple model equations. 2.7.2: Closure. 2.8.2: Light, as a physical factor. 2.9.1: NPZD, a simple model of an aquatic ecosystem. The background reading describes the anatomy of interactions and the use of rate limiting functions. One of the most common ways to describe these limitations is closely related the Michaelis-Menten model originally proposed to describe enzyme kinetics (Fig. 5.1). There, the rate of change in a product, \\(Y\\), is a decelerating function of substrate concentration \\(X\\), \\[\\frac{dY}{dt} = V_m\\frac{X}{k+X}\\], where \\(V_m\\) is the maximum rate of change, and \\(k\\) is the half-saturation constant. The half-saturation constant is the value of the substrate \\(X\\) at which the rate is half the maximum, because it simplifies the expression to \\(V_m\\frac{X}{X+X} = \\frac{1}{2}V_m\\). Figure 5.1: ‘Limitation’ arises when the benefit of a resource declines as the resource becomes super-abundant. ‘Inhibition’ is formulated as 1-limitation and arises when some factor such as a waste product or toxin increases in concentration. 5.1 This task Below, I supply some code from Soetaert and Herman (2009). Your assignment is to make a copy of it, and document heavily it with your own comments. Nearly every line of code should have a comment. Use comments to demarcate different sections of the script. Don’t forget to start with comments about the what the document is, your name, and the source of the model. Turn in your script, and the figure it should generate. To begin, create or obtain an R script of the following code. library(deSolve) NPZD&lt;-function(t,state,parameters) { with( as.list(c(state,parameters)), { PAR &lt;- 0.5*(540+440*sin(2*pi*t/365-1.4)) din &lt;- max(0,DIN) Nuptake &lt;- maxUptake * PAR/(PAR+ksPAR) * din/(din+ksDIN)*PHYTO Grazing &lt;- maxGrazing * PHYTO/(PHYTO + ksGrazing)*ZOO Faeces &lt;- pFaeces * Grazing Excretion &lt;- excretionRate * ZOO Mortality &lt;- mortalityRate * ZOO * ZOO Mineralisation &lt;- mineralisationRate * DETRITUS Chlorophyll &lt;- chlNratio * PHYTO TotalN &lt;- PHYTO + ZOO + DETRITUS + DIN dPHYTO &lt;- Nuptake - Grazing dZOO &lt;- Grazing - Faeces - Excretion - Mortality dDETRITUS &lt;- Mortality - Mineralisation + Faeces dDIN &lt;- Mineralisation + Excretion - Nuptake list( c(dPHYTO,dZOO,dDETRITUS,dDIN), c(Chlorophyll = Chlorophyll, PAR=PAR, TotalN= TotalN) ) }) } ## THE FLUXES ARE PER DAYS. SCAlE PARAMETERS ACCORDINGLY. parameters&lt;-c(maxUptake =1.0, # ksPAR =140, # ksDIN =0.5, # maxGrazing =1.0, # ksGrazing =1.0, # pFaeces =0.3, # excretionRate =0.1, # mortalityRate =0.4, # mineralisationRate =0.1, # chlNratio =1) # state &lt;-c(PHYTO =1, # ZOO =0.1, DETRITUS=5.0, DIN =5.0) times &lt;-c(0,365) out &lt;- as.data.frame( ode(state,times, NPZD, parameters) ) out num &lt;- length(out$PHYTO) # last element state &lt;- c(PHYTO=out$PHYTO[num], ZOO=out$ZOO[num], DETRITUS=out$DETRITUS[num], DIN=out$DIN[num]) times &lt;-seq(0,730,by=1) out &lt;-as.data.frame(ode(state, times, NPZD, parameters)) out.long &lt;- out %&gt;% as.data.frame() %&gt;% pivot_longer(-time, names_to=&quot;State_vars&quot;, values_to=&quot;Value&quot;) ggplot(data=out.long, aes(time, Value)) + geom_line() + facet_wrap(~State_vars, scales=&quot;free_y&quot;) ## provide a unique name for your PNG file ggsave(&quot;myNPZDplot.png&quot;) 5.1.1 Create an R script of NPZD Create an R script of just the system of ODEs of NPZD. Name the function NPZD, and name the script “NPZD.R.” We will use it in later chapters. "],["model-sensitivity.html", "6 Model Sensitivity 6.1 Model Sensitivity 6.2 Local sensitivity 6.3 Assessing sensitivity in a nitrogen budget model 6.4 Sensitivity in an aquatic ecosystem 6.5 Letting Sensitivity Analysis Guide Next Steps", " 6 Model Sensitivity Reading: Chapter 11, Soetart and Herman (2009) Model sensitivity is part of model testing and validation (Soetart and Herman 2009). In this chapter, we assess model sensitivity, that is, the sensitivity of model outcomes to model inputs. Sometimes, those inputs are small perturbations of the state variables, and we often refer to this as local stability analysis. In contrast, here we investigate the sensitivity of model output to the values of parameters we use as input. This is what we do when we calculate the sensitivity and elasticity of a demographic projection matrix. One of the reasons we assess the sensitivity of model output to the values of parameters is to determine which parameters are most important in driving the dynamics of the model. Identifying the most important parameters provides understanding of the model dynamics guides future research in making sure we have good estimates of those particular model parameters. Stability and sensitivity are, broadly speaking, opposites of each other. Stability is the tendency to remain intact, to persist, or return to a steady state, following a perturbation. Ecologists have a large lingua jargona to describe different types of stability. Sensitivity is the degree to which model outputs tend to deviate if one or more of their inputs change. Before we begin, it is worth listing questions we should ask ourselves about any model. Testing Solution Correctness Is there an analytical solution against which we can check the model output? Is there known data with which we can compare the output? Internal Logic Do the state variables remain in their expected range (e.g., biomass &gt;= 0)? If it is a closed system, do the state variables remain at zero if they are all set to zero? Is mass balance preserved? Model Verification and Validity. Our data and model don’t agree. How do we proceed? Are the data accurate and/or precise? Does the structure of the model approximate the important processes? Do the parameter values correspond to actual rates? These are important questions. We would also like to know how certain we need to be about parameter values. Model output can be very insensitive or very sensitive to small changes in the values of model parameters. Sensitivity analysis helps us pinpoint parameters to which model output might be most sensitive, that is, parameters that can have big effects on model outcomes. 6.1 Model Sensitivity How robust or stable is our model to inevitable untruths? Is our model likely to give approximately correct answers given only approximately correct structure and parameters? There are two forms of sensitivities that we can assess: responses to changes in parameters responses to changes in state variables Also, we can assess global or local behavior: Global changes broad, systematic variation in a parameter. permanence - do all state values remain &gt; 0? Local changes responses to very small changes in parameters. responses to very small changes to state variables at equilibria. 6.2 Local sensitivity Here we will assess the local sensitivity to parameters in our model of the Hubbard Brook nitrogen cycle. To assess the local sensitivity of our model to parameters, we will use code that does the following: Create a baseline, or reference data set of model output using our original parameter values. To assess long term average behavior, we typically run the model a long time, throw away early transient dynamics. We then consider the remaining output as our baseline. We could also assess short term responses instead, if we had a particular scenario in mind. Create a new parameter set that changes one parameter by a little bit. This deviate, or “little bit,” could be a very small percentage of the original value, or a small fixed value. Rerun the model using the new parameter set, and an initial state that is the same as the first state of the baseline. Calculate the difference between the baseline and new model outputs at each time step. This “difference” may take several forms, such as the absolute difference, the difference relative to the original, the squared difference, or others. Summarize the differences across time using the mean or median, or something else. Rinse and repeat steps 1-5, for each parameter in the model. Save the results in a form we can use. The above steps are an example of pseudocode. Pseudocode is a series of steps written in plain language that describe intermediate steps toward achieving some goal. The function we use follows these steps. It relies heavily on code in Chap. 11 of Soetart and Herman (2009). Below I describe arguments of the function. Let’s start by loading it. Obtain a copy of sensitivity.R and sens.fig.R and put them in an appropriate directory. I use a folder called code. The R function source() runs an R script; if the script defines a function (as these do), you will then be able to use that function. ## This script loads a function called, oddly enough, &quot;sensitivity&quot; source(&quot;code/sensitivity.R&quot;) ## ask for the arguments of the function args(&quot;sensitivity&quot;) ## function (y.initial, times, func, parms, burnin = NULL, tiny = 0.001, ## dev.type = &quot;normalized&quot;, summary.type = &quot;arithmetic_mean&quot;) ## NULL Some of the arguments are the same as for ode().Here is a bit of how the function works: y.initial is a named vector the starting point for the ODEs. By ‘named,’ we mean that the elements have names, as in y=c(A=1, B=2). times is the vector of times for which the user wants to assess sensitivity. func is the system of ODEs written for use with ode(). parms is the vector of parameter values used in func. burnin is the number of initial time steps to throw away. NULL causes the function to throw away the first half. Otherwise set an actual number (integer). tiny is the proportional deviation to the parameter (0.1 = 10% increase, -0.1 = 10% decrease). summary.type refers to how the time series of deviates are summarized. The current options are ‘arithmetic_mean,’ ‘mean_absolute’ (the arthmetic means of absolute values of the deviates), or ‘RMSE’ (the root mean squared error). dev.type refers to the type of sensitivity, either the simple ‘deviate’ (dy), ‘sensitivity’ (dy/dp), ‘elasticity’ (d(log[y])/d(log[p])), ‘normalized’ ( [dy/y] / [dp/p]). Default is ‘normalized’ which is nearly identical to ‘elasticity.’ The function returns a named list containing several elements: deviation.summary is a data frame of the summarized deviates for each parameter (rows), for each state variable (columns). dev.type is the type of deviate used. summary.type is the type of summary of the time series used. tiny is size of proportional perturbation to parameters sens.list a list of each of the original time series. 6.3 Assessing sensitivity in a nitrogen budget model Consult our previous chapter on the nitrogen budget of Hubbard Brook, watershed 6. If you have not already done so, make a script that contains nothing but the bormann2 function. Copy the block of text where we define it, paste it into a new R script, and save it as bormann2.R. Next we prepare our workspace, by removing extraneous objects. rm( list = ls() ) # delete (remove) everything in the workspace. The next line of code loads and runs two files. ## I put the script in a folder called &#39;code&#39; that resides inside ## my working directory. The following line of code runs the code in ## the script, and the script is located at code/bormann2.R source(&quot;code/bormann2.R&quot;) ## if you want to make sure that it loaded the right thing, type ## bormann2 on your command line, or highlight the word, and run it with ## Control-Enter ## We also load code to calculate the sensitivities, ## and also to create a time series figure of the deviations. source(&quot;code/sensitivity.R&quot;) # sensitivities of all variables source(&quot;code/sens_fig.R&quot;) # graph time series of sensitivity of one variable Next we begin to run it. params &lt;- c( i1 = 6.5, # precip i2 = 14.2, # fixation a1 = 79.6 / (26 * 532), # uptake a2 = (6.6 + 0.8) / 532, # throughfall and inorganic exudates a3 = (54.2 + 2.7 + 0.1 + 6.2 ) / 532, # litter, throughfall, organic exudates a4 = 69.6 / 4700, # net mineralization a5 = 3.9 /26, # export from available a6 = 0.1/4700, #export from bound K=600 ) # close parentheses initial.state &lt;- c(V = 532, A = 26, B = 4700) t &lt;- seq(from = 0, to = 500, by = 1) Next we run the sensitivity analysis. We start with parameters and initial conditions. params &lt;- c( i1 = 6.5, # precip i2 = 14.2, # fixation a1 = 79.6 / (26 * 532), # uptake a2 = (6.6 + 0.8) / 532, # throughfall and inorganic exudates a3 = (54.2 + 2.7 + 0.1 + 6.2 ) / 532, # litter, throughfall, organic exudates a4 = 69.6 / 4700, # net mineralization a5 = 3.9 /26, # export from available a6 = 0.1/4700, #export from bound K=600 ) # close parentheses initial.state &lt;- c(V = 532, A = 26, B = 4700) t &lt;- seq(from = 0, to = 500, by = 1) Next we do the sensitivity analysis. We will start with a graph, often a good place to start. The graph will show the actual deviations that arise when we alter each parameter one at a time by one percent. out &lt;- sens_fig(&quot;A&quot;, y.initial=initial.state, times=0:1000, func=bormann2, parms=params, burnin=0, tiny=0.01, relative=FALSE ) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ggplot(out, aes(Time, y, colour=Perturbation)) + geom_line() + facet_wrap(~Parameter, scales=&quot;free_y&quot;) Figure 6.1: Observed responses of the available N pool to a one percent change to each parameter. Note two features of these graphs. First, the change due to the system dynamics is relatively large and so obscures the change due the sensitivity. Second, note that sometimes a smaller parameter (here in blue) results in a higher value of the state variable. It might be more informative the (i) focus on model output after it has approach the steady state, and (ii) examine the relative departure from the reference state. Relative change of 1.0 would mean that for a 1% change in a parameter, the state variable would also change by 1%. out &lt;- sens_fig(&quot;A&quot;, y.initial=initial.state, times=0:1000, func=bormann2, parms=params, burnin=500, tiny=0.01, relative=TRUE ) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ggplot(out, aes(Time, y, colour=Perturbation)) + geom_line() + facet_wrap(~Parameter, scales=&quot;free_y&quot;) Figure 6.2: Relative responses of the available N pool to a one percent change to each parameter. Note big differences in the values of the y-axes. These graphs show that the state variable responds very consistently through time. In systems with more complicated dynamics, that will not always be the case. Next, we can assess each of the state variables and average across time. b1 &lt;- sensitivity(y.initial=initial.state, times=t, func=bormann2, parms=params, dev.type=&#39;normalized&#39;, summary.type=&quot;arithmetic_mean&quot;) The output generates a list with seven elements, but to see this structure you need to use str. ## Peak at the structure of the output str(b1) ## List of 7 ## $ deviation.summary:&#39;data.frame&#39;: 9 obs. of 5 variables: ## ..$ parameters: chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## ..$ V : num [1:9] 3.71e-05 2.92e-05 2.94e-05 -2.01e-07 -2.68e-04 ... ## ..$ A : num [1:9] 0.0648 0.1042 -0.9535 0.0831 0.5228 ... ## ..$ B : num [1:9] 3.79e-05 1.23e-01 3.73e-05 9.81e-06 6.15e-01 ... ## ..$ total : num [1:9] 0.000319 0.110914 -0.004113 0.000371 0.556306 ... ## $ dev.type : chr &quot;normalized&quot; ## $ summary.type : chr &quot;arithmetic_mean&quot; ## $ burnin : NULL ## $ tiny : num 0.001 ## $ parms : Named num [1:9] 6.5 14.2 0.00575 0.01391 0.1188 ... ## ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## $ sens.list :List of 4 ## ..$ V : num [1:251, 1:9] 0.00 4.51e-06 1.04e-05 1.61e-05 2.32e-05 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## ..$ A : num [1:251, 1:9] 0 0.0635 0.0654 0.0654 0.0654 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## ..$ B : num [1:251, 1:9] 0.00 5.70e-08 1.99e-07 8.61e-07 5.71e-06 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## ..$ total: num [1:251, 1:9] 0 0.000277 0.000286 0.000287 0.000292 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;sensitivity&quot; &quot;list&quot; However, if you just type b1, it will spit out just the summary of the deviations. b1 ## parameters V A B total ## 1 i1 3.705840e-05 0.0647565594 3.790716e-05 0.0003194970 ## 2 i2 2.918269e-05 0.1041528053 1.225100e-01 0.1109138069 ## 3 a1 2.940582e-05 -0.9535105388 3.728004e-05 -0.0041134439 ## 4 a2 -2.014821e-07 0.0831473008 9.812943e-06 0.0003706962 ## 5 a3 -2.681428e-04 0.5227930990 6.145104e-01 0.5563064925 ## 6 a4 2.992411e-04 0.2238495782 -7.329505e-01 -0.6598425953 ## 7 a5 -2.021901e-05 -0.0414509485 -5.222392e-06 -0.0001870163 ## 8 a6 2.264389e-06 -0.0008868526 -1.040165e-03 -0.0009414854 ## 9 K 8.470909e-01 -0.3190643353 4.950298e-01 0.5246217855 If we want to use this summary, however, we need to specify b1$deviation.summary Next we would like to graph the summaries, for ease of interpretation. b1L &lt;- b1$deviation.summary %&gt;% pivot_longer(cols=&quot;V&quot;:&quot;total&quot;) ggplot(data=b1L, aes(x=parameters, y=value)) + geom_col() + facet_wrap(~name, scales=&quot;free_y&quot;) Figure 6.3: Model output is more sensitive to some parameters than others. In addition, different state variables are respond differently to different parameters. Note that the y-axis differs among the state variables. We can also summarize across state variables, so that we have one effect of each parameter. Here we use just V, A, and B, and calculate the root mean squared error. rmse.SV &lt;- b1$deviation.summary %&gt;% select(V:B) %&gt;% apply(MARGIN=1, function(x) sqrt( x*x / 3 )) SV &lt;- b1$deviation.summary %&gt;% select(V:B) rmse.SV &lt;- t( apply(SV, MARGIN=1, function(x) { m &lt;- mean(x) sq &lt;- (x-m)^2 sqrt( mean( sq )) }) ) rmse &lt;- data.frame(p=b1$deviation.summary$parameters, rmse=as.numeric(rmse.SV)) qplot(p, rmse, data=rmse, geom=&quot;col&quot;) So,…into what should we direct our effort? What data do we want most want to collect? These sensitivities are likely to help us direct our attention where it would be most useful. 6.4 Sensitivity in an aquatic ecosystem Here we revisit the aquatic ecosystem model, NPZD, of Soetaert and Herman. We load it, and define the parameters and initial state. Obtain a copy of the model, NPZD.R. ## load the model source(&quot;code/NPZD.R&quot;) #-----------------------# # define the model parameters: # #-----------------------# parameters&lt;-c(maxUptake =1.0, # /day ksPAR =140, # muEinst/m2/s ksDIN =0.5, # mmolN/m3 maxGrazing =1.0, # /day ksGrazing =1.0, # mmolN/m3 pFaeces =0.3, # - excretionRate =0.1, # /day mortalityRate =0.4, # /(mmolN/m3)/day mineralisationRate =0.1, # /day chlNratio =1) # mgChl/mmolN #-------------------------# # the initial conditions: # #-------------------------# state &lt;-c(PHYTO =1, # state variable initial conditions, units mmolN/m3 ZOO =0.1, DETRITUS=5.0, DIN =5.0) Next we run the dynamics for two years. times &lt;- 0:730 out &lt;- ode(state, times, NPZD, parameters) out %&gt;% as.data.frame() %&gt;% pivot_longer(cols=-time, names_to=&quot;State.var&quot;, values_to=&quot;value&quot;) %&gt;% ggplot(aes(time, value)) + geom_line() + facet_wrap(~State.var, scale=&quot;free_y&quot;) Next we check model sensitivity. outf &lt;- sens_fig(&quot;DIN&quot;, y.initial=state, times=0:1000, func=NPZD, parms=parameters, burnin=0, tiny=0.01, relative=TRUE ) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 ggplot(outf, aes(Time, y, colour=Perturbation)) + geom_line() + facet_wrap(~Parameter, scales=&quot;free_y&quot;) In this figure, we see that sensitivity varies through time. For most parameters, it can be small at some times and quite large at other times. We can see how sensitivity can depend to a huge degree on the time points at which we evaluate that sensitivity. Here we summarize those across time. # args(sensitivity) outd &lt;- sensitivity(state, times, NPZD, parameters, dev.type = &quot;normalized&quot;, summary.type = &quot;arithmetic_mean&quot;) outd$deviation.summary %&gt;% pivot_longer(cols=-parameters) %&gt;% ggplot(aes(x=parameters, y=value)) + geom_col() + facet_wrap(~name, ncol=2, scales=&quot;free&quot;) + coord_flip() # Flip x and y so we can see the parameter names Which parameters appear to have the greatest effect on model predictions? 6.5 Letting Sensitivity Analysis Guide Next Steps So, let’s say you find that the half saturation constant for grazing rate of phytoplankton by zooplankton, \\(ks_{\\mathrm{Grazing}}\\), is relatively important in governing several state variables. You also believe that it is very poorly estimated. Because you believe that your estimate is very uncertain, you should probably investigate and reveal to your collaborators the effect of wide variation in this parameter. . Make sure that you have exhausted your own data in its ability to estimate that constant. Perhaps we’ll explore that elsewhere. Make sure that you have exhausted the literature as you combed through older papers that might have estimates. * Make sure that you’re exhausted generally. One of the trickiest aspects of gathering parameter estimates from the literature or from other people’s data is converting their units to units you need. What units do you need? Are you trying to estimate a flux (the mass of material flowing from one compartment to another per unit time)? Are you trying to estimate a mass-specific rate constant? What do others provide? Estimates of moles or grams? Molecular or elemental quantities (e.g., NH\\(_4\\) or N)? All or at least the important sources of elemental quantities (NH\\(_4\\) and NO\\(_3\\) vs. just one)? Per unit area or volume? What is their time scale? Conversion between what others provide and what you need can bee tricky, but it is essential to get it right. Assuming that you have worked hard at the above tasks but are still more in the dark than you would like, what is left to you? What folks often do is to rely on first principles and logical constraints to find upper and lower limits to the parameter, and display or summarize numerical the variation in outputs that result. The local sensitivity analysis showed us the potential effects of variation in parameters. Here what we want to do is determine for ourselves the potential variation in the parameter itself, and then use simulation to discover the consequences of the a priori variation in the parameter. So, our current estimate of the grazing half saturation constant, \\(ks_{\\mathrm{Grazing}}\\), is 1.0. If we examine NPZD(), we find that the shape of this resource limitation function: ksGr &lt;- 1 ## let Phytoplankton be x, and vary between 0 and 6 mmolN/m3 ## because that is the approximate range we see in the dynamics curve(x/(ksGr + x), 0, 6) If we increase or decrease the half saturation constant, what will we see? ksGr &lt;- c(.5, 1, 2) # let Phytoplankton vary more curve(x/(ksGr[1] + x), 0, 10) curve(x/(ksGr[2] + x), 0, 10, add=TRUE, lty=2) curve(x/(ksGr[3] + x), 0, 10, add=TRUE, lty=3) legend(&quot;bottomright&quot;, legend=paste(ksGr), lty=1:3) Given this range, is there some basis for selecting one of these? Recall that this is multiplied by the maximum grazing rate (and zooplankton); therefore, we might want to ask ourselves how that maximum rate was determined. For instance, was it estimated at moderate phytoplankton concentrations or when phytoplankton were truly unlimited? If we have some data from the literature suggesting particular values, we might use these as maxima and minima, or we might go beyond them. Sometimes, all we have is a guess. If a parameter appears to be important, then we might simply pick a value and then double it and halve it, or investigate 10-fold increases. Here we do the former. Let the half saturation constant vary between 0.5 and 2. Generate the dynamics. Plot outputs together. ## reference run ## make a copy of the parameters myP &lt;- parameters ## integrate and make a data frame out_1 &lt;- as.data.frame( ode(state, times, NPZD, myP) ) ## add identifier out_1 &lt;- out_1 %&gt;% ## make a new column mutate(ksGrazing = 1.0) # code as text ## Change the half saturation constant myP[&quot;ksGrazing&quot;] &lt;- 0.5 ##rerun and make a data frame out_0.5 &lt;- as.data.frame( ode(state, times, NPZD, myP) ) ## add identifier out_0.5 &lt;- out_0.5 %&gt;% ## make a new column mutate(ksGrazing = 0.5) # code as text ## Change the half saturation constant myP[&quot;ksGrazing&quot;] &lt;- 2 ## rerun and make a dataframe out_2&lt;- as.data.frame( ode(state, times, NPZD, myP) ) ## add identifier out_2 &lt;- out_2 %&gt;% ## make a new column mutate(ksGrazing = 2.0) # code as text out.all &lt;- rbind(out_0.5, out_1, out_2) out.all %&gt;% pivot_longer(cols=PHYTO:TotalN, names_to=&quot;State.var&quot;, values_to=&quot;value&quot;) %&gt;% ggplot(aes(time, value, colour=as.factor(ksGrazing))) + geom_line() + facet_wrap(~State.var, scale=&quot;free_y&quot;) Well, then… . Aren’t you glad you tried that! 6.5.1 Your turn Take a stab at varying something in your preferred model. You can use anyone’s data - we are all in this together. Just give proper attribution. You can also come up with creative ways to quantify an output difference from a reference in an output. Often, we come up with a useful quantitative metric, run a larger number of scenarios (e.g., letting a parameter take 20 different values), and then plot the relation between the parameter and your quantitative measure of difference in output. Be creative. See what you find. "],["MEL.html", "7 Multiple Element Limitation 7.1 Rastetter et al. 1997 7.2 Implementing MEL", " 7 Multiple Element Limitation 7.1 Rastetter et al. 1997 Readings: Chapters 1, 2 Bormann, Likens, and Melillo (1977) Rastetter, Agren, and Shaver (1997) Soetaert and Herman (2009), pages … Rastetter and Shaver (1992) proposed a model of multiple-element limitation of vegetation growth (MEL). Rastetter, Agren, and Shaver (1997) extended that to include soil pools. We replicate this here. It is similar to our model of (Bormann, Likens, and Melillo 1977), but now includes both carbon and nitrogen.2 By now, you’ve had some practice working with models, so take a gander at this one (Fig. 7.1). Figure 7.1: Fig. 1 from Rastetter et al. 1997. Variables \\(B\\), \\(E\\), and \\(D\\) elements in vegetation biomass, inorganic form (e.g., soil solution, or atmospheric CO\\(_2\\)), and organic form in soil detritus. Variable \\(V_i\\) is allocation related to nutrient acquistion. \\(U\\) is uptake by vegetation or microbes in detritus (broadly defined). \\(R\\) is renewal into the inorganic pools. \\(R_{Ne}\\) is N deposition from an external source, and \\(L_{Ne}\\) is loss via leaching or denitrification. The little triangles or bowties on fluxes refer to some rate-controlling function; \\(B\\) and \\(V\\) exchange information (dotted lines) that influences fluxes. Other terms are explained in the Table below. Figure 7.2: Table 1 (Rastetter et al 1997). MEL uses the parameter \\(A\\) for “acclimation,” which we can think of as compensation by vegetation that is associated with homeostatic responses to resource supply rates that differ from a stoichiometric ideal. Acclimation has a perfectly good meaning in ecophysiology, which is short- to near-term reversible physiological adjustment in response to changing conditions. In MEL, “acclimation” is much broader, and in this early version, it means shifts in the relative rates of C and N accumulation in vegetation, resulting in and from changes in the C:N ratio of vegetation. This occurs due to the ratio of wood to non-woody tissue, defined by \\(q\\). Compensatory dynamics by vegetation, or “acclimation,” is a very important part of this model. “Vegetation” is a complex adaptive system that undergoes diverse changes across many time scales. This could happen through physiological, ecological, and evolutionary changes in plant species composition, ratios of root, stem, leaves and reproductive structures, or changes in tissue C:N ratios. “Acclimation” covers a range of responses with very different time scales, and the variable \\(A\\) is how these changes are incorporated into this ecosystem model. In a later section, we explore the meaning and implications of MEL acclimation. In MEL, vegetation “acclimates” through differential allocation to carbon vs. nitrogen uptake functions. Rastetter et al. refer to this as uptake “effort.” They use this anthropomorphic term “effort” for a pattern of allocation, as in “…increasing uptake effort to acquire limiting soil nutrients.” MEL uses the state variables, \\(V_C\\) and \\(V_N\\), to represent this “effort” or allocation of uptake of C and N respectively. In a later section, we explore the meaning and implications of MEL effort. Both vegetation and microbes acclimate in MEL, in a manner inversely proportion to the relative concentrations of nutrients. In both vegetation and microbes, this results in a tendency toward maintenance of optimal C:N ratios. However, the model handles them differently. It assumes that the acclimation in vegetation is slow while the acclimation in microbes is instantaneous. Exercise Identify terms or expressions in the model that link the two element cycles (C, N). That is, find expressions that are functions of state variables of the two elements. See if you can get a sense of the dependencies. Exercise Find examples of expression that are linear functions of state variables (Y = aX). Find examples of nonlinear functions of the Michaelis-Menten form (aka Type II functional response) (Y=a X/(k+X)). 7.1.1 The model Here we write out the model nearly as written in Rastetter et al. (1997), but with some reordering. By default, R is an interpreted language rather than a compiled language. What that means for us is that R processes commands one line at a time. Therefore, if a function F requires parameter x, we have to make sure parameter x is defined before we use function F. In our implementation of MEL below, Mc requires Ucm, so Ucm needs to be defined before Mc. Copy this into a new script and name it ‘mel2.R.’ In that script, add more comments to those I started. Once you have saved your script, you can use source('mel2.R') to load the function. mel2 &lt;- function(time, y, p){ with(as.list(c(y,p)), { ## Carbon is 1 and Nitrogen is 2 in ## E - inorganic; D - soil organic w/ microbes; B - veg biomass; ## V - acclimation ## Fig. 1 of Rastetter et al. (1997) ## Gross UPTAKE of inorganic nutrients by Microbes ## Note rearrangement of terms (cf. Rastetter et al. 1997) ## (uptake) X (carbon availability) X (stoichiometric optimization) Unm &lt;- alpha_n*En/(k_nm + En) * psi*Dc * Dc/(theta*Dn) Ucm &lt;- alpha_c*Ec/(k_cm + Ec) * psi*Dn * (theta*Dn)/Dc # alpha_c=0 ## Microbial consumption ## nutrient availability + gross uptake Mc &lt;- psi*Dc + Ucm Mn &lt;- psi*Dn + Unm Lambda_c &lt;- epsilon_c * theta*Mn/(Mc + theta*Mn) Lambda_n &lt;- epsilon_n * Mc/(Mc + theta*Mn) ## C:N ratio (units of C) q &lt;- q_w*Bc/(k_q + Bc) ## SURFACE area related to nutrient acquisition (0 &lt; z &lt; 1) Sc &lt;- (Bc + q*Bn)^zc Sn &lt;- (Bc + q*Bn)^zn ## UPTAKE by Vegetation ## uptake constant X rel. effort X Surface area X resource conc. Ucv &lt;- gc * Vc * Sc * Ec/(kc + Ec) Unv &lt;- gn * Vn * Sn * En/(kn + En) ## LOSSES Lcv &lt;- mc*Bc Lnv &lt;- mn*Bn Lce &lt;- beta_ce * Ec # beta_ce = 0 # export = 0 Lne &lt;- beta_ne * En # leaching or denitrification Lcd &lt;- beta_d * Dc Lnd &lt;- beta_d * Dn ## REPLENISHMENT to the inorganic pools (R) Rcm &lt;- Mc*(1-Lambda_c) # C respiration Rnm &lt;- Mn*(1-Lambda_n) # N mineralization Rcv &lt;- rc*Bc Rnv &lt;- rn*Bn ## Available elemental nutrients ## Set dE_i/dt = 0 to control the inorganic pools externally dE_N &lt;- Rne + Rnm + Rnv - Lne - Unm - Unv ## dE_C &lt;- Rce + Rcm + Rcv - Lce - Ucm - Ucv dE_C &lt;- 0 ## Nutrients in vegetation BIOMASS dB_C &lt;- Ucv - Rcv - Lcv dB_N &lt;- Unv - Rnv - Lnv ## Nutrients in DETRITUS dD_C &lt;- Lcv + Ucm - Lcd - Rcm dD_N &lt;- Lnv + Unm - Lnd - Rnm ## Acclimation (compensation) potential ## mass-specific growth rates Grc &lt;- dB_C/Bc Grn &lt;- dB_N/Bn ## symmetric C:N differential plus relative growth difference A &lt;- log(Bc/(q*Bn)) + tau * (Grc - Grn) ## Relative effort toward compensation ## if(A&gt;0) Vstar &lt;- Vc else Vstar &lt;- Vn Vstar &lt;- (A &gt; 0) * Vc + (A &lt; 0) * Vn dV_C &lt;- -1*a*A*Vstar dV_N &lt;- -1 * dV_C ## AUXILIARY ecosystem variables NPP &lt;- Ucv - Rcv Net_N_uptake &lt;- Unv - Rnv Ecosystem_N=Bn+En+Dn Ecosystem_C=Bc+Ec+Dc return(list(c(dE_C, dE_N, dB_C, dB_N, dD_C, dD_N, dV_C, dV_N), NPP=NPP, Net_N_uptake=Net_N_uptake, Ecosystem_C=Ecosystem_C) ) }) } 7.1.2 Parameters Here we start with a vector of model parameters whose description and units are found in Table 1 above. The numeric values come from Table 2 in Rastetter et al. (1997). You should copy these into your own script, and then add your own comments to each parameter, as I started to. ## Parameter set for a closed ecosystem p.c &lt;- c( # carbon, nitrogen Rce=0, Rne=0, # no renewal from outside the ecosystem gc=1116, gn =23.77, rc=0.02055, rn=0.01955, mc=0.02055, mn=0.08009, kc=350, kn=5, zc=0.11, zn=0.16, epsilon_c=0.6, epsilon_n=0.6471, alpha_c=0, alpha_n = 0.01092, k_cm = 1, k_nm=1, beta_ce = 0, beta_ne = 0, # Other a=5, tau=3, # damping coef q_w=439, k_q=26290, theta=8.628, psi=0.04321, beta_d=0 ) Exercise To help you better grasp what is going on in the model, redraw Fig. 7.1 on a chalkboard or a larger piece of paper and add as much detail as you can fit. Include model parameters and definitions and explanations – whatever helps you make sense of the processes, and their mathematical representation. 7.2 Implementing MEL Table 2 of Rastetter et al. also include initial values for the state variables, which we include here. init.states &lt;- c(Ec=350, En=1, # Inorganic nutrients g/m^2 Bc=22000, Bn=110, # Biomass nutrients g/m^2 Dc=13000, Dn=521, # Detritus nutrients g/m^2 Vc=0.5, Vn=0.5 # allocation &quot;effort&quot; unitless ) In addition to initial values of state variables, Rastetter, Agren, and Shaver (1997) (Table 2) list initial fluxes. We don’t have to worry about those; the model calculates the initial fluxes from parameters and from initial values of the state variables. 7.2.1 Increasing atmospheric carbon with events in ode() Rastetter et al. (1997) double CO\\(_2\\) concentrations at an early time point in their simulations. We do that using an events in the ode() function. An “event” in an ode() model occurs when the value of a state variable changes suddenly at a one or more time points. In one sense, these are like ecological disturbances. For instance, we could add 1 kilogram of phosphorus to a lake in a one-time pulse, or halve primary producer biomass at 10 random times over a one hundred year interval. We can create events using either a function, or with a data frame. Here we create a data frame, using the required four variables: ‘var’ selects the state variable to change. ‘time’ is the time at which the event occurs. ‘value’ is the numeric value associated with the change. ‘method’ is the operator that operates on the ‘value’ and the state variable: ‘add’ will add the value to the state variable, ‘mult’ will multiply the state variable by the value, and ‘replace’ will simply replace the state variable with the value (the value can be a positive or negative number). event.df &lt;- data.frame(var=&quot;Ec&quot;, time=0.005, value=2, method=&quot;mult&quot;) event.df ## var time value method ## 1 Ec 0.005 2 mult This event will double atmospheric carbon concentration very shortly after the start of the simulation. We will run the model for 1000 years as in Rastetter et al. (1997), and specify that R return values for a logarithm series of time steps. That way, we see detail in the short term response when the dynamics are quite rapid. ## 10^3 = 1000 years t=10^seq(-3, 3, by=.001) Next, we first run the model using parameters for a closed ecosystem, and then for a partly open system (Rastetter, Agren, and Shaver 1997). Check with Rastetter et al. to find out what they mean by closed vs. partly open, but I bet you can at least figure out what they mean by a closed system. ## Closed ecosystem out.c &lt;- ode(init.states, t, func=mel2, parms=p.c, events=list(data=event.df) ) ## Partly open p.o &lt;- p.c p.o[&quot;Rne&quot;] &lt;- 1 p.o[&quot;beta_ne&quot;] &lt;- 1 out.o &lt;- ode(init.states, t, func=mel2, parms=p.o, events = list(data = event.df) ) You will probably get a warning message that R has included our event time into the integration procedure. That is a good thing; if the time we selected for our event was not also a time for which we choose output, ode will include it automatically in order to generate the event. Next we convert the simulation data sets to data frames, add variables, combine, rearrange, and plot the results. ## convert and add a variable out.c &lt;- out.c %&gt;% as.data.frame() %&gt;% mutate(N.cycle = &quot;closed&quot;) out.o &lt;- out.o %&gt;% as.data.frame() %&gt;% mutate(N.cycle = &quot;open&quot;) ## combine &quot;row-bind&quot; out.all &lt;- rbind(out.c, out.o) ## rearrange to a &#39;tidy&#39; and long format outL &lt;- pivot_longer(out.all, cols=-c(time, N.cycle), names_to=&quot;State_var&quot;, values_to=&quot;g.sq.m&quot;) ## plot ggplot(outL, aes(x=time, y=g.sq.m, colour=N.cycle)) + geom_line() + scale_x_log10() + facet_wrap(~State_var, ncol=3, scales=&quot;free&quot;) Figure 7.3: Simulated dynamics of carbon and nitrogen in an ecosystem with both C- and N-limitation, in a closed and partly open ecosystem. Recreates Fig. 3 from Rastetter et al. (1997), in part. Cool, right? Compare this with Figure 3 (Rastetter et al. 1997). What does this leave out? Exercise Create a gradual rise in CO\\(_2\\) using ‘events.’ Determine a predicted CO\\(_2\\) scenario, and create a data frame with annual events for the next 100 y. Compare it to the first 100 years of instantaneous increase. Is this a reasonable comparison? References "],["optimization.html", "8 Optimization 8.1 Introduction 8.2 Bormann logistic model 8.3 Fitting a model to data 8.4 Two (or more) parameters at a time", " 8 Optimization 8.1 Introduction Sometimes we want to find values of parameters by picking those that cause the model output to match observed data as closely as possible. We call this optimization or calibration. Usually calibration refers to the entire modeling process, whereas optimization refers to the computational techniques used to make the model-data match as close as possible. We will take a couple of different approaches to optimization, starting with one parameter at a time, and moving to optimizing two or more parameters simultaneously. We will use our Bormann model with self-limiting vegetation. 8.2 Bormann logistic model Code to run the ODE model. First we load the model (source). If you want to view it in the console, you can do that. rm( list = ls() ) # clean the workspace. library(deSolve) # The next line of code loads and runs a file. It requires that this script # is in R&#39;s working directory. # Alternatively, you could include the entire path in the file name. # getwd() # will tell you what the working directory is. source(&quot;code/BormannLogistic.R&quot;) ## bormann.logistic Next we begin to run it. ## p &lt;- c( a20 = 6.5, a30 = 14.2, a12 = 79.6/(532*26), a21 = (6.6 + 0.8)/532, a31 = (54.2 + 2.7 + 6.2 + 0.1)/532, a23 = 69.6/4700, a02 = 3.9/26, a03 = 0.1/4700, K = 600) y &lt;- c(V = 532, A = 26, B = 4700) t &lt;- seq(from = 0, to = 500, by = 1) out &lt;- ode(y = y, times = t, func = bormann.logistic, parms = p) plot( out ) Figure 8.1: Our model’s output. Refer to an earlier chapter for the meanings and units of the parameters and state variables. 8.3 Fitting a model to data Fitting a model to data can mean a variety of things, but maybe most generally we mean that we adjust parameters until the differences between the model output and our data are as small as possible. There are many analytical and computational techniques to do this. In R, it is often best to use optimize() for 1-dimensional optimization, that is, when our goal is to adjust or optimization a single parameter while holding all others constant. There is a function optim() that is better when we want to optimize for more than one parameter. We want to minimize deviations between our model output and observed state variables. We have options: Do we compare trajectories, or just end points? Do we minimize deviations between all state variables and the data, or just those we are confident about? Do we keep the deviations so that they are on the original scale of the data, such that large pools carry more weight than small pools? Alternatively, we could scale them to give them more equal weight. In our case, we do not have trajectories of data through time - we have only a snapshot of the state of the ecosystem, so we cannot compare trajectories, only endpoints of model output vs. a snapshot of data. Let’s start by using all the state variables, on the scale of the raw data. We also need to choose a parameter to optimize. We might as well select one that is associated with high sensitivity, such as mineralization rate, \\(a_23\\). 8.3.1 Minimization is the new black In most optimization procedures, we create an objective function that measures deviation between a model and data and then uses one or another procedure to try different values and find those that minimization the deviations. Here is our example of an objective function that searches for values of a23 (mineralization rate) that minimize deviations between the model output (values of \\(V\\), \\(A\\), \\(B\\) at \\(t = 500\\)) and the data on pool size, provided by Bormann et al. (1977). sse.bormann.mineralization &lt;- function(params, data) { ## an objective function whose arguments are ## params - the parameter of interest ## data - data from Bormannm et al. ## Assign a new value of the parameter to our set of parameters p[&#39;a23&#39;] &lt;- params ## run the model out &lt;- ode(y = y, times = t, func = bormann.logistic, parms = p) ## store the last values of the state variables nr &lt;- nrow(out) model.output &lt;- out[nr,c(&quot;V&quot;, &quot;A&quot;, &quot;B&quot;)] ## Calculate the sum of the squared differences ## squaring the differences makes them positives and ## weights big differences even more heavily diffs &lt;- model.output - data diffs2 &lt;- diffs^2 sse &lt;- sum( diffs2 ) ## Return the SSE sse } Next, we use our objective function with optimize() to find the value of a23 that minimizes the objective function. First, we’ll define the data to which we compare the model. We need to use the same name that we used in the objective function. data = c(V = 532, A=26, B=4700) Next, we use optimize, where f is the objective function interval is a two-element vector with the lower and upper limits of parameter. params is an argument of our objective function, with which we tell optimize the value at which to start the search. fit0 &lt;- optimize(f = sse.bormann.mineralization, interval = c(0.01, 0.5), data=data) Now let’s examine the result of the optimization. fit0 ## $minimum ## [1] 0.018149 ## ## $objective ## [1] 4625.243 The minimum is the value of the parameter for which the minimum was reached. The objective is the value of the objective function, which was minimized by the parameter value minimum. (The value of mineralzation rate that we started with was \\(a_{23} = 69.6/4700 =\\) 0.015). Now let’s rerun the model with the optimized value, and look at the output. We first create a new parameter vector, and insert the optimized mineralization rate. p.opt &lt;- p p.opt[&#39;a23&#39;] &lt;- fit0$minimum Now we can rerun the ODE model with the original and the optimized values of the mineralization rate. out.original &lt;- as.data.frame( ode(y = y, times = 0:500, func = bormann.logistic, parms = p)) out.opt &lt;- as.data.frame( ode(y = y, times = 0:500, func = bormann.logistic, parms = p.opt)) When we plot both sets of ouput and the data, we see the differences among all three. layout(matrix(1:4, nrow=2) ) plot(out.original$time, out.original$V, type=&quot;l&quot;) lines(out.opt[,1], out.opt[,2], lty=2, lwd=1, col=2) abline(h=data[&#39;V&#39;], lty=3, lwd=1, col=4) plot(out.original$time, out.original$A, type=&quot;l&quot;, ylim = c(23,33)) lines(out.opt[,1], out.opt[,3], lty=2, lwd=1, col=2) abline(h=data[&#39;A&#39;], lty=3, lwd=1, col=4) plot(out.original$time, out.original$B, type=&quot;l&quot;, ylim=c(1000, 6000)) lines(out.opt[,1], out.opt[,4], lty=2, lwd=1, col=2) abline(h=data[&#39;B&#39;], lty=3, lwd=1, col=4) If we return to our choices described above, we might consider several things. First, we might think that we have relatively accurate and precise estimates of the vegetation and available pools, but poor estimates of the bound pool. Therefore, we might want to fit the model to just the vegetation and available pools. However, given that the system is all connected, we may not want to take such drastic action. Alternatively, we might want to at least weight the pools more equally by using the log-transformations of the output and data. This accomplishes two things. First, it weights the variables on a more similar scale. This may not matter for the result with some methods of optimization, but it can. Second, it makes the computation more stable and reliable to use both smaller numbers, and each on a more similar scale. This can make a big difference if you are having trouble getting the optimization to give reliable, repeatable results. Let’s try this option, and see if it makes a difference. We start by rewriting our objective function comparing the base 10 logs of the output and the data. The only difference is that diffs &lt;- model.output - data becomes diffs &lt;- log(model.output, 10) - log(data, 10). sse.bormann.mineralization2 &lt;- function(params, data) { ## an objective function whose arguments are ## params - the parameter of interest ## data - data from Bormannm et al. ## Assign a new value of the parameter to our set of parameters p[&#39;a23&#39;] &lt;- params[1] ## run the model out &lt;- ode(y = y, times = t, func = bormann.logistic, parms = p) ## store the last values of the state variables nr &lt;- nrow(out) model.output &lt;- out[nr,c(&quot;V&quot;, &quot;A&quot;, &quot;B&quot;)] ## Calculate the sum of the squared differences ## squaring the differences makes them positives and ## weights big differences even more heavily diffs &lt;- log(model.output, 10) - log(data, 10) diffs2 &lt;- diffs^2 sse &lt;- sum( diffs2 ) ## Return the SSE sse } Now we rerun the optimization and examine the output. fit0.log &lt;- optimize(f = sse.bormann.mineralization2, interval = c(0.001, 0.1), data=data) fit0.log ## $minimum ## [1] 0.0181761 ## ## $objective ## [1] 0.002796935 fit0 ## $minimum ## [1] 0.018149 ## ## $objective ## [1] 4625.243 Notice that the objective function output is much, much smaller. This is because the logarithms of the raw data (especially the bound pool) are much, much smaller. Second, we see tha \\(a_{23}\\) is only very slightly different than our other fitted value. Nonetheless, we will rerun the model again, using this new value. p.opt.log &lt;- p p.opt.log[&#39;a23&#39;] &lt;- fit0.log$minimum out.opt.log &lt;- as.data.frame( ode(y = y, times = 0:500, func = bormann.logistic, parms = p.opt.log)) Now we can compare all of our information, including the raw data, our original model output using parameters estimated from Bormann et al. (1977), and parameters fitted using either the raw or transformed data. rbind(data=data, Bormann = out[501, 2:4], raw=out.opt[501,2:4], log=out.opt.log[501,2:4]) ## V A B ## data 532.0000 26.00000 4700.000 ## Bormann 599.7254 25.46203 5753.202 ## raw 599.9441 25.49684 4702.931 ## log 599.9447 25.49695 4695.946 We see that the model calibrated with either the raw or transformed data is able to generate output more consistent with the current size of the bound pool. What does that mean? It gives us a new hypothesis about mineralization rate, and allows us to ask whether the bound pool is growing, whether our estimate of mineralization was too low, or something else. Nonetheless, we can now begin to ask more sophisticated questions of our system. 8.3.2 Your assignment, should you choose to accept it. Identify another parameter that would be useful to examine. Justify your choice. identify line of code you would need to edit in sse.bormann.mineralization2(), and propose the necessary change. Vegetation Propose a hypothesis for why the final size of the vegetation pool seems unaffected by changes in mineralization rate. What about the pool might vary with mineralization rate, and why? Describe a way to test your idea. 8.4 Two (or more) parameters at a time Here we use the same approach, in which we have an objective function, parameters of interest, and data. Yeah, baby! Here is our objective function, and you can see it is not very different. Find and explain the difference. sse.b.m.p2 &lt;- function(params, data, vars) { ## an objective function whose arguments are ## params - the parameter of interest ## data - data from Bormannm et al. ## Assign a new value of the parameter to our set of parameters p[&#39;a23&#39;] &lt;- params[1] p[&#39;a31&#39;] &lt;- params[2] ## run the model out &lt;- ode(y = y, times = t, func = bormann.logistic, parms = p) ## store the last values of the state variables nr &lt;- nrow(out) model.output &lt;- out[nr,c(&quot;V&quot;, &quot;A&quot;, &quot;B&quot;)] ## Calculate the sum of the squared differences ## squaring the differences makes them positives and ## weights big differences even more heavily diffs &lt;- log(model.output, 10) - log(data, 10) diffs2 &lt;- diffs^2 wdiffs2 &lt;- diffs2/vars sse &lt;- sum( wdiffs2 ) ## Return the SSE sse } Now things start to get a tiny bit different. We need a vector of parameters. What are these biological processes? params &lt;- c(a32 = 69.6/4700, a31 = 0.19) Now we use optim(). Unlike the 1-D optimze(), the first argument is the vector of parameters. The second argument is the function, and last we tell optim() what other stuff we need (in this case the data). vars &lt;- c(532/10, 26/10, 4700*2) fit2 &lt;- optim(par = params, fn = sse.b.m.p2, data=data, vars=vars) When we look at the values returned, we get a little more information than we do from optimize (see ?optim). That is because optim() can use a variety of optimization algorithms, and a wide variety of control functions (see ?optim for more information). fit2 ## $par ## a32 a31 ## 0.01823085 0.12182485 ## ## $value ## [1] 5.120701e-05 ## ## $counts ## function gradient ## 65 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The value par is, of course, the best set of parameters, and value is the value of the objective function. Convergence is just a code, and 0 indicates successful completion. Have at it. What can you learn today? What to we most want to know? Can we use the model to make hypotheses about processes that are not even in the model, like dentrification? "],["references.html", "9 References", " 9 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
