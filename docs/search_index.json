[["index.html", "A Primer of Ecosystem Modeling - a work in progress 1 Prerequisites 1.1 Must do’s 1.2 A couple of useful citations", " A Primer of Ecosystem Modeling - a work in progress Hank Stevens, BIO 672 2022-02-17 1 Prerequisites For this R primer, you’ll need to read several chapters of Soetaert and Herman (2009) which are available on the Canvas website. Their book is an excellent source for those interested in ecosystem modeling, especially in aquatic systems. We will use bits of it along with this online text to get a little more comfortable with ecosystem processes and models. I also assume that you are reading a book on ecosystem ecology such as Chapin III, Matson, and Mooney (2011). 1.1 Must do’s Read background material as needed. Install or update R (4.0.3, as of Jan 2021) Install or update RStudio Install select R packages: ‘deSolve,’ ‘tidyverse,’ ‘LakeMetabolizer`, ’lubridate’ Also, before doing most tasks in this primer, load deSolve and tidyverse with library(deSolve); library(tidyverse). The R working directory is the place R automatically looks for files. For simplicity, I will refer to the R working directory as Rwork. Therefore, Rwork/data would be a folder or directory called data inside Rwork. At any point in time, you can find out what your working directory is using getwd(). You can set your working directory using setwd(\"HD/MyStuff/Rwork\"), where “HD/MyStuff/Rwork” is the path to what you want your working directory to be. You can also use the “Session” pull-down menu in RStudio. 1.2 A couple of useful citations Ten simple rules for biologists learning to program. PLoS Computional Biology 14(1): e1005871. https://doi.org/10.1371/journal.pcbi.1005871 Ten simple rules for tackling your first mathematical models: A guide for graduate students by graduate students. PLOS Computational Biology 17(1): e1008539. https://doi.org/10.1371/journal.pcbi.1008539 References "],["intro.html", "2 Introduction ecosystem models 2.1 Why models? 2.2 What’s an ecosystem? 2.3 What’s a model? 2.4 Steps in modeling 2.5 An example in R", " 2 Introduction ecosystem models Primary source: pp. 1-17, (in Chapters 1, 2 of Soetaert and Hermann 2009) In this course, we’ll cover the very basics of ecosystem modeling. There are several goals I have for you, the reader. I hope that you, become more fluent in the discipline of ecosystem ecology; that you understand and can use basic terminology, and can identify quantitative pieces of the literature you read, and presentations you hear and see; understand and describe the quantitative and qualitative features of ecosystem dynamics and models of those dynamics; assess the relative merits of different modeling approaches and different mathematical formalisms of those approaches; create models of ecosystem dynamics of your own; write R code to implement ecosystem models. To do all this, the following text relies heavily on selected secondary sources especially Soetaert and Herman (2009). I also cite selected primary sources where appropriate. 2.1 Why models? In general, models are simplifications of reality. Useful models capture something useful about the reality we are interested in. A road map is a useful model of a road network. It captures just what we need. We model to aid understanding, because, at some level, the model is a quantitative and qualitative explanation for some phenomenon (Fig. 2.1). We can use models to test hypotheses, guide experiments, predict dynamics, and manage ecosystems and populations (Soetaert and Herman 2009). Figure 2.1: We use conceptual and mathematical models to interpret reality. (Fig. 1.3 in Soetaert and Hermann, 2009). 2.2 What’s an ecosystem? You can find definitions of an ecosystem elsewhere, but here we want to emphasize the abstraction of that ecosystem (Fig. 2.2). We will think of an ecosystem as a set of one or more compartments containing some mass of an element of interest, such as carbon or nitrogen. That element is the currency of the ecosystem. These compartments (a.k.a. pools, stocks) are connected by fluxes, or flows, the transfer per unit time of some mass of the element. When these transfers come from entirely outside the ecosystem, we refer to them as imports. When the transfer exits the system entirely, we refer to it as an exports. We model or describe an ecosystem as a set of pools or compartments, connected by fluxes, that is, transfers of energy or materials among pools. Figure 2.2: An ecosystem perspective of lake, in terms of phosphorus (Carpenter et al. 1992). Boxes are ecosystem compartments and the quantities are pools (a.k.a. stocks, units are mass per unit area or volume). Arrows are fluxes (units are mass per unit time). In Fig. 2.2, all pools are receiving imports from outside the system, represented by arrows coming from the amorphous cloud. All pools save dissolved P appear to export P back out. Dissolved P is receiving fluxes from all the animals in the system, and losing P to seston, which is primarily phytoplankton. Ecosystem fluxes or flows are influenced by state factors such as temperature, time, or disturbance which act as constraints that can limit or speed up the fluxes or determine the current states of the variables (Fig. 2.3). Figure 2.3: The current state of an ecosystem depends on state factors (Chapin III, Matson, and Mooney 2011). 2.3 What’s a model? You’ve already seen ecosystem models. An ecosystem model consists of the compartments and fluxes we saw above. We refer to the currencies in different compartments as variables because they can vary or change through time. We use mathematical equations and computational controls to represent fluxes between compartments. In the equations, there are constants that we call parameters that control the rate of these fluxes. In principle, variables can usually be observed, whereas we cannot directly observe parameters and have to estimate them. We describe an ecosystem using balance equations for each pool (p. 18, Soetaert and Herman 2009). A balance equation is simply a bookkeeping or budgeting device to keep track of fluxes and estimate parameters: \\[\\mathrm{Change~in~pool} = \\mathrm{Sources - Sinks}\\] As models increase in complexity, we usually use differential equations, or time derivatives, to represent the balance equation for each pool or state variable in a model. For instance, the dissolved P in Fig. 2.2 could look like \\[\\frac{dD}{dt} = I + e_HH + e_N N + e_1 F_1 + e_2 F_2 - u_SS\\] where \\(D\\) is the amount of P in the dissolved pool, perhaps in mg\\(\\cdot\\)m\\(^{-2}\\), \\(I\\) is import, or source, from outside the system (dust? lake inflow?), and \\(H\\), \\(N\\), \\(F_1\\), and \\(F_2\\) are the other pools. The lower case letters are parameters or rate constants that are the mass-specific fluxes. The units of \\(dD/dt\\) are mg\\(\\cdot\\)m\\(^{-2}\\cdot\\)d\\(^{-1}\\), and so each term (e.g., \\(I\\) or \\(e_H H\\)) in the equation must also have these same units. units of \\(e_H H\\) must be mg\\(\\cdot\\)m\\(^{-2}\\)\\(\\cdot\\)d\\(^{-1}\\). units of \\(H\\) are mg\\(\\cdot\\)m\\(^{-2}\\). Ergo, units of \\(e_H\\) are d\\(^{-1}\\). More intuitively, the units of these rate constants are milligrams per square meter per day, per milligram per square meter of lake (mg\\(\\cdot\\)m\\(^{-2}\\cdot\\)d\\(^{-1}\\) / mg\\(\\cdot\\)m\\(^{-2}\\) in the lake). Note the quantities cancel out, and we are left with d\\(^{-1}\\). See Soetaert and Herman (2009) section 2.1.4 for further explanation. One nice feature of ecosystem ecology and ecosystem models is the emphasis on the conservation of mass and energy. We aim to track where everything comes from and where it goes. If we add up all of the rates of change for each pool, we should see that most of them cancel out, and we are left with only imports and exports. Doing this summation will tell us whether our model makes sense or we made a mistake. If we did not make any mistakes, it will tell us whether the entire system is a net sink or source of our currency. For instance, after simplifying the lake ecosystem model, we would have, \\[\\frac{dD}{dt} + \\frac{dS}{dt} + \\frac{dH}{dt} + \\frac{dN}{dt} + \\frac{dF_1}{dt} + \\frac{dF_2}{dt} = \\ldots \\ldots = \\mathrm{Imports - Exports}\\] See Soetaert and Herman (2009) section 2.1.3 for further explanation. State factors also enter into the model, typically altering the fluxes. For instance, increasing atmospheric temperature might force a predictable change in our rate constants (e.g., \\(e_H\\), \\(u_S\\)) in the above model. Because temperature is an external factor forcing a change to a parameter, we often call its role in a model a forcing function. We will add temperature to a model later in the book. To review: variables are quantities that change with time, such as the amount of carbon in the atmosphere, or the amount of phosphorus in the primary producers in a lake. Parameters are (typically) constants in equations that quantify how quickly the variables change. Forcing functions represent state factors that we think of as external to the ecosystem compartments. No models apply everywhere, all the time. All models are limited for a specific domain and with specific boundaries. These are the spatial, temporal and conceptual limits on a model. 2.3.1 Other ideas Statistical models (e.g., regression) often describe patterns in natural systems and help test hypotheses. They often represent a mechanistic process, but that is not typically their main goal. Process models (e.g., stock and flow models) also describe patterns in natural systems, but they include more mechanism and seek to describe mechanism and understand process. People sometimes call these mechanistic models. Figure 2.4: A statistical model of aboveground plant biomass as a function of available soil nitrogen. A theory is a well-supported hierarchical conceptual framework that contains clearly formulated postulates, from which a set of predictions logically follows. Efficient theory (Marquet et al. 2014) is based on first principles, and relies on as few assumptins as possible. In contrast to theory, models are specific implementations of theory and specific descriptions of nature. Remember that in principle, all models are wrong, but some are useful1. Exercise: Ask yourself whether a lake is a carbon sink or a source. Draw an appropriate compartment model to address this question. After having done so, ask yourself what assumptions you’ve made about the temporal and spatial scales. What mechanisms have you included? Why? 2.4 Steps in modeling Soetaert and Herman (2009) identify a series steps that guides model development and ultimately improve understanding and prediction (Fig. 2.5). Figure 2.5: It is helpful to use a series of steps in improving our models. This was Figure 1.7 in Soetaert and Hermann (2009) but I chopped it in half. Read the left half (top to bottom), then the right half (top bottom). In brief, I paraphrase or plagiarize their steps thus: Problem statement or question = narrow the focus of interest in a real system. System conceptualization = represent a real system as an abstraction. Model formulation = represent the conceptual model in mathematical form. Model verification = check internal consistency, consistency with physical laws of conservation, and ability to reproduce known solutions. Parameterization = use the literature or other sources to help determine numerical values for parameters; fine tuning these on the basis of model output is model calibration. Model calibration = use model output to fine tune model parameters, often referred to, or related to, model fitting. Sensitivity analysis = use model output to understand the effect of variation in parameters and state variables on dynamics. Note that this figure contains a loop – after calibration and sensitivity analysis, we loop back the model structure and its parameters. In this class, you will use sensitivity analysis to identify the most interesting or most important parts of your model. 2.5 An example in R Here we create a simple model to illustrate some of what we’ve been describing. Consider our lake above, but as a whole – just one big pool of P. We’ll let the imports, \\(I\\), be constant and independent of the amount of the P in the lake. Our exports will depend on the amount of the P in the system-the more P in the system, to more can be exported. We will assume that a constant fraction, \\(a\\), is exported, \\[\\frac{dP}{dt} = I - a P\\] In R, we write a function for the system of differential equations, and include descriptive comments. ## A function called &#39;lake&#39; to represent an ordinary differential equation (ODE) ## to use with ode() in the deSolve package. lake &lt;- function( time, state_vars, parameters ){ ## ODE function for a single pool (e.g., a lake) with a constant ## rate of import, and first-order export function (depends on the pool). ## The arguments time, state_vars, and parameters will be used by ode() ## to solve (numerically integrate) the system. ## I is the constant input rate ## a is the constant mass-specific loss rate ## P is the state variable P &lt;- state_vars[1] # state variable(s) with(as.list(parameters), # tell R to look inside &#39;parameters&#39; { dP.dt &lt;- I -a*P # the balance eq. or rate func. return( list( dP.dt ) ) # return the value of the rate function }) } Next, we tell R what the values of the parameters are that we want. Let’s say the the import is 3 mg\\(\\cdot\\)m\\(^{-2}\\cdot\\)d\\(^{-1}\\), and that the fraction (or rate, really) is 0.5 d\\(^{-1}\\), or more intuitively, mg\\(\\cdot\\)m\\(^{-2}\\cdot\\)d\\(^{-1}\\) per mg\\(\\cdot\\)m\\(^{-2}\\) in the lake. Note the quantities cancel out, and we are left with d\\(^{-1}\\). ## parameters p &lt;- c(I = 10, # constant input rate mg / m^2 / day a = .5 # mass-specific output rate, day^-1 ) Now the exciting part. We tell R our starting point and what time points we want to integrate for, and then solve the differential equation and have R return the result for the time points we want. After that, we display the first five time points. initial.state &lt;- c(P=1) # Initial concentration of P in the lake t &lt;- seq(from=0, to=10, by=1/24) # 10 days, in hourly increments ## solve the equation in func, using parameter values in p, ## starting at &#39;initial.state&#39; and return the time points t out &lt;- ode(y=initial.state, times = t, func = lake, parms = p) ## display the first five rows of the solution out[1:5,] ## time P ## [1,] 0.00000000 1.000000 ## [2,] 0.04166667 1.391740 ## [3,] 0.08333333 1.775400 ## [4,] 0.12500000 2.151151 ## [5,] 0.16666667 2.519155 Pictures are informative, so here we plot the result. ## simple graph of the time series plot(out) We can also use ggplot2 to make a pretty graph, and then save it. out2 &lt;- as.data.frame(out) # re-classify the data set ggplot(data=out2, aes(x=time, y=P)) + geom_line() ggsave(&quot;myLake.png&quot;, height=4, width=4) In very simple situations, we can solve the equilibrium by hand. By definition, the equilibrium is a state at which the system stops changing, that is, its rate of change is zero. To find a value of \\(P\\) which is an equilibrium, we set the balance equation equal to zero, and solve for \\(P\\): \\[\\frac{dP}{dt} = 0 = I -aP\\] \\[P^* = \\frac{I}{a}\\] By convention, we denote the equilibrium value of \\(P\\) with an asterisk, or “star,” as in “P-star.” We call this the analytical solution. Questions: Is our graph consistent with the analytical prediction of \\(P^*\\)? Determine the units of \\(I\\) and \\(a\\); show your work and explain it to your cat. In more complex systems, we often (typically?) can’t solve for the analytical solution. Instead, to find the same result, we usually run a model for a long period of time, until the state variables stop changing very much. Alternatively, if we have a model of a real ecosystem, we made be interested in its short term dynamics, in addressing questions related to experimental results, making predictions about the consequences of landscape management, or in using a time series to compare different models. References "],["N.html", "3 Describing a nitrogen budget 3.1 Getting started 3.2 Mathematical forms 3.3 Paramaterization 3.4 Mathematical solution 3.5 Add self-limitation", " 3 Describing a nitrogen budget Background Readings: pp. 17-35 (Soetaert and Hermann 2009) Bormann et al. 1977. Nitrogen Budget for an aggrading northern hardwood forest ecosystem. Science 196:981-983. Figure 3.1: Nitrogen budget for a temperate northern hardwood forest (Hubbard Brook Watershed 6, Bormann et al. 1977). Figure 3.2: A simpler compartment model for Hubbard Brook Watershed 6, based on Bormann et al. (1977). 3.1 Getting started (S&amp;H 15-17) For any ecosystem, we want to start with paper and pencil and sketch out the pools and the fluxes that we think are important (see example, Fig. 3.2). Don’t sweat the details yet – you’ll revise it anyway. Go ahead an label the compartments and the fluxes with with both biological and physical processes as well as some simple abstract notation. Write balance equations (S&amp;H, 3, 17-20). For each flux, identify the underlying biological and physical processes. If you don’t completely know, don’t sweat it. Here are the balance equations for Fig. 3.2: Rate of change of vegetation = uptake - exudates and through fall - leaf and root litter loss \\[\\frac{dV}{dt} = f_1 - f_2 - f_3\\] Rate of change in available pool in soil solution = bulk precip + exudates and through fall + net mineralization - uptake - stream export \\[\\frac{dA}{dt} = I_1 + f_2 + f_4 - f_1 - E_1\\] Rate of change in the bound pool = net N fixation + leaf and root litter loss - net mineralization - stream export \\[\\frac{dB}{dt} = I_2 + f_3 - f_4 - E_2\\] Assess the total load or mass budget (S&amp;H, 19-22). For Fig. 3.2, we have \\[\\frac{d\\left(V+A+B\\right)}{dt} = \\frac{dV}{dt} + \\frac{dA}{dt} + \\frac{dB}{dt} = (f_1 - f_2 - f_3) + (I_1 + f_2 + f_4 - f_1 - E_1) + (I_2 + f_3 - f_4 - E_2)\\] \\[\\frac{d\\left(V+A+B\\right)}{dt} = I_1 + I_2 + f_1 - f_1 + f_2 - f_2 + f_3 - f_3 + f_4 - f_4 - E_1 - E_2\\] \\[\\frac{d\\left(V+A+B\\right)}{dt} = I_1 + I_2 - E_1 - E_2\\] From this we see that the total mass budget for this watershed depends simply on import or inputs, and exports or outputs. For each flux, decide which pools directly influence the flux. In a forest, the flux of nitrogen from vegetation to soil will depend enormously on the amount of vegetation present as leaves senesce and fall, fine roots die back, and water leaches nutrients out of leaves and bark. The flux is very unlikely to depend directly on the amount of nitrogen already in the soil. In a lake, the flux of phosphorous from the water column into phytoplankton depends on the amounts of both the amount of phytoplankton and the amount of phosphate in the water at any given instant. For the vegetation pool in Fig. 3.2, we see each flux (\\(f\\)) is a Function (\\(F()\\)) of one or two pools: Uptake: \\(f_1 = F(V,A)\\) Leaf and root litter, and throughfall: \\(f_3 = F(V)\\) Root exudates, and throughfall: \\(f_4 = F(V)\\) Next, we want to represent these fluxes in a mathematical form. 3.2 Mathematical forms A common starting point for dynamics depending on two pools is the law of mass action. This states that the reaction rate is proportional the product of the pools. In the case of plant uptake of N, which depends on the amounts of N in the available pool and the vegetation pool, this would be \\(aVA\\), where \\(a\\) is a proportionality constant. In some circumstances, these pools might also have exponents different than one (\\(aV^1A^1\\)), such as \\(aVA^2\\). This occurs in chemistry when a reaction requires two molecules of “A” for each molecule of “V.” It might occur in ecology if a rate depends differentially on A and B. Using the law of mass action for plant uptake, we will describe the fluxes in the simple N budget above (Fig. 3.2) with the following expressions. \\[\\begin{align} \\frac{dV}{dt} &amp;= a_{1}AV - a_{2}V - a_3 V\\\\ \\frac{dA}{dt} &amp;= I_1 + a_{2}V + a_{4}B - a_{5}A - a_{1}AV\\\\ \\frac{dB}{dt} &amp;= I_2 + a_{3}V - a_{4}B - a_{6}B \\end{align}\\] Take the time to identify each term and think about the biology or physics that might govern each term. 3.3 Paramaterization Parameteriztion is what we call assigning numerical values to mathematical parameters. Here, we find initial estimates for the parameters in our model. We use the literature for this purpose (Bormann, Likens, and Melillo 1977). If we have the data (we do) and relatively simple mathematical forms (we do), it is fairly straightforward to estimate parameters. For instance, we decided that net mineralization would be directly proportional to the size of the organic pool, \\(B\\), that is, \\(F_4 = a_4B\\). To calculate the annual incremental rate \\(a_4\\), we substitute data where we can, and solve what we need. \\[\\begin{align*} F_4 &amp;= a_4 B\\\\ 69.6 &amp;= a_4 B\\\\ a_4 &amp;= \\frac{69.6}{4700}\\\\ a_4 &amp;\\approx 0.0148 \\end{align*}\\] We use the same approach for second order equations as well. \\[\\begin{align*} F_1 &amp;= a_1 AV \\\\ 79.6 &amp;= a_1 26\\cdot 532 \\\\ a_1 &amp;= \\frac{79.6}{26\\cdot 532}\\\\ a_1 &amp;\\approx 0.0058 \\end{align*}\\] To create instantaneous rates, we take advantage of a result from calculus. It turns out that the natural logarithm of the ratio of masses gives us an estimate of an average instantaneous rate. Let \\(B_0\\) be the mass of N in the soil at time 0, and \\(B_1\\) would be the mass one year later if we summed \\(B\\) plus the flux coming out of \\(B\\): \\[B_0=4700 \\quad; \\quad B_1 = B_0 + F_4 = 4700 + 69.6\\] The average instantaneous rate, \\(\\alpha_4\\) required to get us from \\(B_0\\) to \\(B_1\\) in one year is \\[\\alpha_4 = \\log\\left(\\frac{B_1}{B_0}\\right) = \\log\\left(\\frac{4700+69.6}{4700}\\right) \\approx 0.0147\\] If you compare the annual incremental rate we first calculated to this instantaneous rate, you can see they are quite similar. They would be indistinguishable given the other uncertainties in actually estimating these fluxes and pools. However, that is not always the case. Table 3.1: Instantaneous rate parameters, variables, units and estimates for a simplified model of Bormann et al. (1977). All fluxes (\\(dX/ dy\\)) are in units of kg ha\\(^{-1}\\) y\\(^{-1}\\). Note that calculations should not be included in your final table, but are presented here for clarity and comparison to your own calculations. Also, note that different rates are presented with different signifcant figures; this is not intended to represent best practice, as we do not now the precision of the data we are using. In any model, we will always estimate different parts with different degrees of precision. p.or.V units Estimate \\(A, B, V\\) state variables kg ha\\(^{-1}\\) \\(26, 4700, 532\\) \\(a_1\\), uptake rate by V from A (kg ha\\(^{-1}\\))\\(^{-1}\\) y\\(^{-1}\\) \\(\\log( (79.6+26 \\cdot 532) / (26 \\cdot 532) ) = 0.0057\\) \\(a_2\\), loss rate from V to A y\\(^{-1}\\) \\(\\log( (6.6 + 0.8 + 532) / 532 ) = 0.014\\) \\(a_3\\), loss rate from V to B (kg ha\\(^{-1}\\))\\(^{-1}\\) y\\(^{-1}\\) \\(\\log( (54.2 + 2.7 + 0.1 + 6.2 + 532) / 532) = 0.011\\) \\(a_4\\), mineralization y\\(^{-1}\\) \\(\\log( (69.6 + 4700) / 4700) = 0.0147\\) \\(a_5\\), export from A y\\(^{-1}\\) \\(\\log( (3.9 + 26) / 26) = 0.14\\) \\(a_6\\), export from B y\\(^{-1}\\) \\(\\log( (0.1 + 4700)/4700) = 0.000021\\) \\(I_1\\), bulk precip kg ha\\(^{-1}\\) y\\(^{-1}\\) \\(6.5\\) \\(I_2\\), N fixation kg ha\\(^{-1}\\) y\\(^{-1}\\) \\(14.2\\) Enter parameters into R. params &lt;- c( i1 = 6.5, # precip i2 = 14.2, # fixation a1 = log( (26*532+79.6) / (26 * 532)), # uptake a2 = log( (6.6 + 0.8 + 532) / 532 ), # throughfall and inorganic exudates a3 = log( (54.2 + 2.7 + 0.1 + 6.2 + 532 ) / 532), # litter, throughfall, organic exudates a4 = log( (69.6+4700) / 4700), # net mineralization a5 = log( (3.9+26) /26 ), # export from available a6 = log( (0.1+4700) /4700) #export from bound ) # close parentheses params ## i1 i2 a1 a2 a3 a4 ## 6.500000e+00 1.420000e+01 5.738276e-03 1.381392e-02 1.122540e-01 1.469994e-02 ## a5 a6 ## 1.397619e-01 2.127637e-05 3.4 Mathematical solution The mathematical solution is the process of making the predictions using our model and our parameters. We solve the model. With simple models, we can sometimes find analytical solutions. For most ecosystem models, we have to solve the models numerically using numerical integration. Here we write a function that includes our system of differential equations. This will allow R to integrate change through time. bormann1 &lt;- function(t, y, p) { # time, vector of state variables and parameters must be in this order # we can use as.list for both the state variables and parameters # a1 = uptake # a2 = loss from veg to avail # a3 = loss from veg to bound # a4 = net mineralization # a5 = export from avail # a6 = export from bound with( as.list( c(y, p) ), { dV.dt &lt;- a1 * A * V - a2 * V - a3 * V dA.dt &lt;- i1 + a2 * V + a4 * B - a1 * A * V - a5 * A dB.dt &lt;- i2 + a3 * V - a4 * B - a6 * B # Here we return a LIST whose first element is the vector of # rates of change for the state variables. The first element must be these rates, # in the same order as the state variables in y # The second element, total, is the total N in the system return(list( c(dV.dt, dA.dt, dB.dt), total = V + A + B ) )}) } Now that we have the function, we tell R what to do with it. We will define the initial state of the system, and then tell R which time point we want it to return. The initial state of the system is the set of starting values for the state variables. We could choose any values, but I select the values given in Bormann, Likens, and Melillo (1977). ## starting values in kg/ha initial.state &lt;- c( V = 532, A = 26, B = 4700) Finally, let’s solve the system, and have R return the first 20 years. time &lt;- 0:20 out &lt;- ode(y = initial.state, times=time, func=bormann1, parms = params) head(out) # the first 6 lines ## time V A B total ## [1,] 0 532.0000 26.00000 4700.000 5258.000 ## [2,] 1 543.9726 25.64295 4705.372 5274.987 ## [3,] 2 554.8724 25.21977 4711.941 5292.033 ## [4,] 3 564.7106 24.85484 4719.569 5309.135 ## [5,] 4 573.6112 24.54316 4728.128 5326.283 ## [6,] 5 581.6881 24.27610 4737.507 5343.471 Use pivot_longer() and ggplot() to make a graph. We use pivot_longer() gather multiple columns into one with a new name (values_to=kg.N), keeping track of the names of the original columns in a new column (names_to=State.var). We can use pivot_wider() if we ever want to spread those columns back out. outL &lt;- out %&gt;% # select the data set as.data.frame() %&gt;% # make sure it is a data frame and not a matrix ## and the rearrange pivot_longer( cols=-time, names_to=&quot;State.var&quot;, values_to=&quot;kg.N&quot;) ## plot the dynamics ggplot(outL, aes(x=time, y=kg.N)) + # select variables to plot geom_line() + # select the form of the graph facet_wrap(~State.var, scale=&quot;free_y&quot;) # separate each state variable and plot each on its on scale. Figure 3.3: Dynamics of a simple N budget, based on Bormann et al. (1977). In some ways, we have been moderately successful in our first pass at converting a purely budgetary model into a dynamic process model. We mimicked total load, and see similar changes through time of all the state variables. Questions to ponder We replicated approximately the N budget of Bormann et al. (1977), but clearly vegetation cannot keep accumulating N indefinitely. What are our next steps? 3.5 Add self-limitation One logical step is to assume that as vegetation eventually gets limited by some factor or resource that is not in our model. If, at first approximation, the vegetation reaches a carrying capacity independent of high resource availability, we can use an approximation suggested by Soetaert and Hermann (2009) for self-limitation, \\[f(X)V\\left(1-\\frac{V}{K}\\right)\\] where \\(f(X)\\) is everything else that regulates mass-specific growth rate. Exercise Include self-limitation in your model of vegetation, estimate \\(K\\), and produce output. Remember that following our template, we have a maximum rate times resource and self limitation, and inhibition. Currently, we have \\[\\frac{dV}{dt} = a_{1}AV - a_{2}V - a_3 V\\] and rearranging, \\[\\frac{dV}{dt} = \\left(a_{1}A - a_{2} - a_3\\right) V\\] If we add self-limitation, we get \\[\\frac{dV}{dt} = \\left(a_{1}A - a_{2} - a_3\\right) V \\left(1-\\frac{V}{K}\\right)\\] where \\(K\\) is the maximum amount of live vegetation that the ecosystem can sustain, in kg,N,ha\\(^{-1}\\). We don’t know exactly what that is yet, but we may be able to get estimates from the literature. For know we can pretend that it is just a bit more than was there in the mid-1970s, say, \\(K=600\\). Now we rewrite the R function with self-limitation. bormann2 &lt;- function(t, y, p) { # time, vector of state variables and parameters must be in this order # we can use as.list for both the state variables and parameters # a1 = uptake # a2 = loss from veg to avail # a3 = loss from veg to bound # a4 = net mineralization # a5 = export from avail # a6 = export from bound with( as.list( c(y, p) ), { dV.dt &lt;- (a1 * A - a2 - a3) * V * (1-V/K) dA.dt &lt;- i1 + a2 * V + a4 * B - a1 * A * V - a5 * A dB.dt &lt;- i2 + a3 * V - a4 * B - a6 * B # Here we return a list whose first element is the vector of # rates of change for the state variables. The first element must be these rates, # in the same order as the state variables in y # The second element is the total N in the system return(list( c(dV.dt, dA.dt, dB.dt), total = V + A + B ) )}) } We can add a new parameter to our vector of parameters, and then solve our new function bormann2 for the same time interval, and plot it. params[&quot;K&quot;] &lt;- 600 ## starting values in kg/ha initial.state &lt;- c( V = 532, A = 26, B = 4700) time &lt;- 0:200 out &lt;- ode(y = initial.state, times=time, func=bormann2, parms = params) outL2 &lt;- out %&gt;% as.data.frame() %&gt;% pivot_longer(cols=-time, names_to=&quot;State.var&quot;, values_to=&quot;kg.N&quot;) ggplot(outL2, aes(time, kg.N)) + geom_line() + facet_wrap(~State.var, scale=&quot;free_y&quot;) Figure 3.4: Dynamics of an N budget, assuming density-dependence in vegetation with a fixed carrying capacity (Bormann et al. 1977). Unlike our first model of this system, we see state variables on curved trajectories and perhaps reaching asymptotes. This makes greater intuitive sense - over the short term, it is the same as the simple N budget shown in Bormann, Likens, and Melillo (1977) and it also shows a reasonable longterm trajectory for the vegetation, and the predicted consequences for the available and bound pools. Save your own R script of ‘bormann2’ Make a new script that contains nothing but the bormann2 function. Copy the block of text where we define it, paste it into a new R script, and save it as bormann2.R. We will use this function in a latter section. References "],["lake-metabolism.html", "4 Lake Metabolism 4.1 Estimating Productivity 4.2 By hand, in R 4.3 The LakeMetabolizer package", " 4 Lake Metabolism In this chapter, you’ll study how lakes breath. You’ll get real data from Acton Lake, look it, and measure the rate of an inhale and an exhale. You’ll do that by hand, by hand with a spreadsheet, by hand with R, and finally using an R package, LakeMetabolizer, which is designed to do that and much more. Figure 4.1: We can use lake oxygen level to measure net ecosystem production. 4.1 Estimating Productivity Most cells respire to do the work of growth and maintenance by consuming oxygen and using it as the final electron acceptor when O\\(_2\\) is reduced, creating water. Because individuals comprise cells, and ecosystems comprise individuals, ecosystems respire too, and we can measure their metabolic rate using oxygen consumption and production. We measure whole ecosystem metabolic rate as net ecosystem productivity which is the difference between gross primary productivity and respiration. Putting all of this in the same units of oxygen allows us to measure the rate, \\[NEP = GPP - R_{\\mathrm{total}}\\] where \\(R_{\\mathrm{total}}=R_A+R_H\\), or total respiration. Limnologists assume that, due to stoichiometric relations, rates of photosynthesis, respiration, and net primary production are correlated with rates of change in dissolved oxygen. Therefore, we can estimate respiration and net primary production by quantifying changes in oxygen concentration over time. Dissolved oxygen in the water column (DO) is relatively easy to measure, therefore it is relatively easy to estimate NEP. If we assume that respiration is constant throughout the 24 h cycle, we estimate GPP as the sum of NEP and R. This is known are the free-water dissolved oxygen method of estimating NEP, R and GPP. Using DO, we can estimate NEP and R, and then calculate GPP as the sum of NEP and R. Because we think about lake NEP as metabolism, we will use units of \\(R\\), \\(NEP\\), and \\(GPP\\) as mg\\(\\cdot\\)mL\\(^{-1}\\)\\(\\cdot\\)h\\(^{-1}\\). the rate \\(R\\) is the slope of DO vs. time at nighttime (when photoautrophs stop fixing carbon). the rate \\(NEP\\) is the slope of DO vs. time during the day (when photoautrophs are fixing carbon and respiration continues unabated). the rate \\(GPP\\) is \\(GPP = NEP + R_{\\mathrm{total}}\\) 4.2 By hand, in R Let’s practice R by importing, wrangling, and graphing data, then calculating slopes and estimating respiration, NPP, and GPP. Start by obtaining data for Acton Lake (acton.csv). Place it inside a folder labelled ‘data’ inside your working directory. Here you read in data and check whether it loaded properly. ## Either change the path name in this function, or ## make sure you have a folder named &#39;data&#39; inside your working directory acton &lt;- read_csv(&quot;data/acton.csv&quot;, ## tell R the format of one of your variables col_types = cols(date_time = col_datetime(format = &quot;%m/%d/%y %H:%M&quot;)), skip = 1# skips the first line of metadata ) # acton &lt;- read_csv(&quot;data/acton.csv&quot;, # ## tell R the format of one of your variables # col_types = cols(date_time = # col_character()), # skip = 1 # skips the first line of metadata # ) # acton$date_time &lt;- as.POSIXlt(acton$date_time, # format=&quot;%m/%d/%y %H:%M&quot;) # summary(acton$date_time) # acton &lt;- acton %&gt;% # mutate(date_time = mdy_hm(date_time)) # summary(acton) # library(lubridate) # date_time &lt;- mdy_hm(acton$date_time) # summary(date_time) Let’s check our understanding of lake oxygen dynamics by plotting the time series. Ggplot understands what to do with dates. ggplot(acton, aes(x=date_time, y=O_mg.L)) + geom_path() Figure 4.2: *Oxygen dynamics from Acton Lake. If we want to calculate the slope of the night time oxygen concentration, then we should identify what the endpoints of “day time” are so that we can do analyses on just daylight or nighttime data. Working with times in R is a little tricky, because times and dates, periods, durations, and intervals are inherently tricky. The date_time variable in the original data set contains all the information, and we extra tidbits from it. # determine day/night intervals ## Sunrise morning &lt;- 6.25 ## sunset evening &lt;- 21 acton &lt;- mutate(acton, ## dates, as factors. date = as.factor( format(date_time, &quot;%Y-%m-%d&quot;, tz=&quot;America/New_York&quot;) ), ## hour in decimal format hour.d = as.integer( format(date_time, &quot;%H&quot;)) + as.numeric(format(date_time, &quot;%M&quot;))/60, ## just the daylight hours daytime.d = ifelse(hour.d &gt;= morning &amp; hour.d &lt; evening, hour.d - morning, NA), ## just the nighttime hours nighttime.d = ifelse( hour.d &gt;= evening, hour.d-evening, ifelse(hour.d &lt; morning, 24 - evening + hour.d, NA)), ## an indicator of whether it is day or night daylight = as.factor( ifelse(hour.d &gt;= morning &amp; hour.d &lt; evening, &quot;day&quot;, &quot;night&quot;)) ) ## identify different day/night cycles, one cycle = one day ## first measurement of the day ( first &lt;- which(acton$hour.d==6.25) ) ## [1] 1 97 193 289 385 481 577 ## day 1 is in rows 1-96, day 2 is in rows 97-192, etc. ( rows.per.day &lt;- diff(first) ) ## [1] 96 96 96 96 96 96 ## there are 96 observations of each cycle ## make a new categorical variable, day acton &lt;- acton %&gt;% mutate(day=as.factor( rep(1:7, each=96))) The slope of oxygen concentration at night is respiration, \\(R\\). The slope of the oxygen concentration during the day is gross primary production minus respiration, or net ecosystem production. We will pull out one day’s worth of data and make a quick plot of the time series. acton2 &lt;- filter(acton, day==&quot;1&quot;) ## or # library(lubridate) # acton2 &lt;- filter(acton, # date_time &gt;=ymd_hms(&quot;2013-06-27 06:15:00&quot;) &amp; # date_time &lt;= ymd_hms(&quot;2013-06-28 06:00:00&quot;) ) qplot(x = date_time, y=O_mg.L, data=acton2, geom = &quot;path&quot;) The nighttime slope is the regression line for just the nighttime data # select the data frame, filter for &#39;date&#39; values within a range of dates names(acton2) ## [1] &quot;date_time&quot; &quot;cum_h&quot; &quot;O_mg.L&quot; &quot;O_perc&quot; &quot;temp&quot; ## [6] &quot;date&quot; &quot;hour.d&quot; &quot;daytime.d&quot; &quot;nighttime.d&quot; &quot;daylight&quot; ## [11] &quot;day&quot; night.R &lt;- lm(O_mg.L ~ nighttime.d, data=acton2) ## intercept and slope of nighttime regression coef(night.R) ## (Intercept) nighttime.d ## 10.2093741 -0.2405216 day.NEP &lt;- lm(O_mg.L ~ daytime.d, data=acton2) ## intercept and slope of daytime regression coef(day.NEP) ## (Intercept) daytime.d ## 7.447288 0.294097 To calculate time-averaged, 24 hour NEP, we assume that all GPP occurred during the day, and respiration is constant. Our day is 14 h 45 min, or 14.75. ## as.numeric() drops the name of the slope we used R24 &lt;- as.numeric( abs( coef(night.R)[2] ) ) NEP24 &lt;- as.numeric( coef(day.NEP)[2] * 14.75/24 ) GPP24 &lt;- NEP24 + R24 R24; NEP24; GPP24 ## [1] 0.2405216 ## [1] 0.1807471 ## [1] 0.4212687 These units are mg O\\(_2\\) / L / h. Here is what every night looks like. nights &lt;- filter(acton, daylight==&quot;night&quot;) ggplot(nights, aes(x=nighttime.d, y=O_mg.L, colour=day, linetype=day)) + geom_point() + # plot points geom_smooth(method=&quot;lm&quot;, se=FALSE) # fit linear models each night We can calculate the average slope for all nights, forcing a straight line through each night’s data. The estimates of uncertainty and the P values won’t make sense because the data are horribly autocorrelated, but we can rely on the estimates of the coefficients, and the average slope, in particular. ntd &lt;- acton$nighttime.d/24 m.resp &lt;- lm(O_mg.L ~ nighttime.d + day, data=acton) night.O2.rate &lt;- coef(m.resp)[&quot;nighttime.d&quot;] night.O2.rate ## nighttime.d ## -0.1969112 The estimate for night.d, -0.197, is our estimate of the respiration rate in mg_O\\(_2\\)/L per hour. We usually report this per day, which would just be 24 times as great. Here is what each day looks like. Recall the at daytime oxygen increase is GPP-R or NEP. days &lt;- filter(acton, daylight==&quot;day&quot;) ggplot(days, aes(x=daytime.d, y=O_mg.L, colour=day)) + geom_point() + # plot points geom_smooth(method=&quot;lm&quot;, se=FALSE) # fit linear models each night If we want to, we could throw out the days that have negative slopes, that is, days 6 and 7. acton3 &lt;- filter(acton, !(day == 6 | day == 7) ) # NOT day 6 OR 7 md.resp &lt;- lm(O_mg.L ~ daytime.d + day, data=acton3) NEP.day &lt;- coef(md.resp)[&quot;daytime.d&quot;] NEP.day ## daytime.d ## 0.2464898 To get 24 hour NEP, we have to weight daytime and nighttime GPP by the daylength. We stipulated that our morning began at 6:15 AM and ended at 9 PM, or a day time of 14 h 45 min, or about 61% of the 24 h cycle. Also, remember that respiration is positive, even though we are measuring it with a negative slope. NEP24 &lt;- as.numeric( NEP.day * 14.75/24 ) R24 &lt;- as.numeric( abs( night.O2.rate ) ) GPP24 &lt;- NEP24 + R24 # as.numeric drops the element name GPP24 ## [1] 0.3483997 Recall these are units of mg O\\(_2\\) / L / h. For units per day, we have c(GPP24, NEP24, R24) * 24 ## [1] 8.361593 3.635724 4.725869 4.3 The LakeMetabolizer package Here we do something similar but in a much more sophisticated way. To estimate the net ecosystem productivity, we need to know, at least, gas exchange rates, equilibrium oxygen saturation, the mixing depth, and the daylight hours. The metab function in LakeMetabolizer calculates GPP, R, and NEP given requisite data. The function can use several different approaches, depending upon what data you have and your quantitative preferences. Here we use the simplest approach, which the authors refer to as simple bookkeeping. First we load the package and then examine the help page for metab.bookkeep. # install.packages(&quot;LakeMetabolizer&quot;, dep=TRUE) library(LakeMetabolizer) ?metab.bookkeep On the help page you learn about how to use this function. Here we walk through the steps for acquiring or making educated guesses about the data we need. Here we acknowledge that the change in dissolved oxygen is a function of NEP and also the flux of oxygen due to diffusion, \\[\\Delta \\mathrm{DO} = \\mathrm{NEP}_{t-1} \\cdot \\Delta t + F_{t-1}\\] where \\(F_{t-1}\\) is the flux of oxygen due to diffusion (Winslow et al. 2016). This rate of diffusion depends on wind speed, temperature, lake size, and, ultimately, how much the of the lake waters mix (Winslow et al. 2016). Here we will simply provide a value for the gas exchange constant and a mixing depth. From its calculations, LakeMetabolizer can give us NEP, R, and GPP. This is because average respiration is the average difference between DO and \\(F\\), and NEP is the difference between GPP and R. LakeMetabolizer needs data in a particular form, and so we build that next. ## pick a reasonable gas exchange constant and mixing depth k.gas &lt;- 0.4 z.mix &lt;- 3 # meters of well mixed surface waters ## let midday PAR be 1300 mol photons/m^2/sec at water surface ## estimate ### dissolved oxygen at saturation (equilibrium) using the function, o2.at.sat.base() ### irradiance in PAR acton.LM &lt;- acton %&gt;% mutate( do.sat=o2.at.sat.base(temp, altitude=283), #saturated DO day= as.numeric(is.day(date_time, lat=39.5)), # day vs. night par = 1300 * sin(daytime.d/14.5 * pi), # PAR irr = ifelse(is.na(par), 0, par), # recode par to make night = 0 z.mix = z.mix, # add mixing depth k.gas = k.gas, # add gas exchange coefficient wtr=temp # rename observed water temperature ) %&gt;% ## select only some of the columns select(datetime=date_time, do.obs=O_mg.L, do.sat=do.sat, k.gas=k.gas, z.mix=z.mix, irr=irr, wtr=wtr) %&gt;% as.data.frame() # simplify the data structure (class tbl_df screws things up) One of the methods in LakeMetabolizer, ‘bookkeep,’ requires irradiance to be 0 or 1. We make a new data set with that. acton.bk &lt;- acton.LM %&gt;% mutate(irr = as.numeric( irr&gt;0 ) ) # converts TRUE/FALSE to 1,0 Finally, we use three different methods to estimate GPP, R, and NEP. ## calculate GPP, R, NEP in mg O2 / L / day out.b &lt;- metab(acton.bk, method=&quot;bookkeep&quot;) ## [1] &quot;Points removed due to incomplete day or duplicated time step: 96&quot; ## [1] &quot;NA&#39;s added to fill in time series: 0&quot; out.k &lt;- metab(acton.LM, method=&quot;kalman&quot;) ## [1] &quot;Points removed due to incomplete day or duplicated time step: 96&quot; ## [1] &quot;NA&#39;s added to fill in time series: 0&quot; out.m &lt;- metab(acton.LM, method=&quot;mle&quot;) ## [1] &quot;Points removed due to incomplete day or duplicated time step: 96&quot; ## [1] &quot;NA&#39;s added to fill in time series: 0&quot; ## combine data sets - first label each dataset out.b$method &lt;- &quot;bookkeep&quot; out.k$method &lt;- &quot;kalman&quot; out.m$method &lt;- &quot;mle&quot; ## now combine out.all &lt;- full_join( full_join(out.b, out.k), out.m) ## ... and stack the variables into a long form out.l &lt;- out.all %&gt;% pivot_longer(cols=GPP:NEP) ggplot(out.l, aes(doy, value, color=method)) + geom_line() + facet_grid(.~name) How does this compare with our previous estimate? What would happen if our gas exchange constant differs (Cole et al. 2010)? "],["npzd---a-simple-aquatic-ecosystem.html", "5 NPZD - a simple aquatic ecosystem 5.1 This task", " 5 NPZD - a simple aquatic ecosystem This section relies entirely on selections in pages 31-58 of Soetaert and Herman (2009). The object of this chapter is to understanding and explore a simple model of a nitrogen-limited aquatic ecosystem consisting of an inorganic pool, phytoplankton, zooplankton, and particulate detritus. The background reading is the following sections: 2.5-2.5.5: Functional responses and rate limitations. 2.6-2.6.2: Couple model equations. 2.7.2: Closure. 2.8.2: Light, as a physical factor. 2.9.1: NPZD, a simple model of an aquatic ecosystem. The background reading describes the anatomy of interactions and the use of rate limiting functions. One of the most common ways to describe these limitations is closely related the Michaelis-Menten model originally proposed to describe enzyme kinetics (Fig. 5.1). There, the rate of change in a product, \\(Y\\), is a decelerating function of substrate concentration \\(X\\), \\[\\frac{dY}{dt} = V_m\\frac{X}{k+X}\\], where \\(V_m\\) is the maximum rate of change, and \\(k\\) is the half-saturation constant. The half-saturation constant is the value of the substrate \\(X\\) at which the rate is half the maximum, because it simplifies the expression to \\(V_m\\frac{X}{X+X} = \\frac{1}{2}V_m\\). Figure 5.1: ‘Limitation’ arises when the benefit of a resource declines as the resource becomes super-abundant. ‘Inhibition’ is formulated as 1-limitation and arises when some factor such as a waste product or toxin increases in concentration. 5.1 This task Below, I supply some code from Soetaert and Herman (2009). Your assignment is to make a copy of it, and document heavily it with your own comments. Nearly every line of code should have a comment. Use comments to demarcate different sections of the script. Don’t forget to start with comments about the what the document is, your name, and the source of the model. Turn in your script, and the figure it should generate. To begin, create or obtain an R script of the following code. library(deSolve) NPZD&lt;-function(t,state,parameters) { with( as.list(c(state,parameters)), { PAR &lt;- 0.5*(540+440*sin(2*pi*t/365-1.4)) din &lt;- max(0,DIN) Nuptake &lt;- maxUptake * PAR/(PAR+ksPAR) * din/(din+ksDIN)*PHYTO Grazing &lt;- maxGrazing * PHYTO/(PHYTO + ksGrazing)*ZOO Faeces &lt;- pFaeces * Grazing Excretion &lt;- excretionRate * ZOO Mortality &lt;- mortalityRate * ZOO * ZOO Mineralisation &lt;- mineralisationRate * DETRITUS Chlorophyll &lt;- chlNratio * PHYTO TotalN &lt;- PHYTO + ZOO + DETRITUS + DIN dPHYTO &lt;- Nuptake - Grazing dZOO &lt;- Grazing - Faeces - Excretion - Mortality dDETRITUS &lt;- Mortality - Mineralisation + Faeces dDIN &lt;- Mineralisation + Excretion - Nuptake list( c(dPHYTO,dZOO,dDETRITUS,dDIN), c(Chlorophyll = Chlorophyll, PAR=PAR, TotalN= TotalN) ) }) } ## THE FLUXES ARE PER DAYS. SCAlE PARAMETERS ACCORDINGLY. parameters&lt;-c(maxUptake =1.0, # ksPAR =140, # ksDIN =0.5, # maxGrazing =1.0, # ksGrazing =1.0, # pFaeces =0.3, # excretionRate =0.1, # mortalityRate =0.4, # mineralisationRate =0.1, # chlNratio =1) # state &lt;-c(PHYTO =1, # ZOO =0.1, DETRITUS=5.0, DIN =5.0) times &lt;-c(0,365) out &lt;- as.data.frame( ode(state,times, NPZD, parameters) ) out num &lt;- length(out$PHYTO) # last element state &lt;- c(PHYTO=out$PHYTO[num], ZOO=out$ZOO[num], DETRITUS=out$DETRITUS[num], DIN=out$DIN[num]) times &lt;-seq(0,730,by=1) out &lt;-as.data.frame(ode(state, times, NPZD, parameters)) out.long &lt;- out %&gt;% as.data.frame() %&gt;% pivot_longer(-time, names_to=&quot;State_vars&quot;, values_to=&quot;Value&quot;) ggplot(data=out.long, aes(time, Value)) + geom_line() + facet_wrap(~State_vars, scales=&quot;free_y&quot;) ## provide a unique name for your PNG file ggsave(&quot;myNPZDplot.png&quot;) 5.1.1 Create an R script of NPZD Create an R script of just the system of ODEs of NPZD. Name the function NPZD, and name the script “NPZD.R.” We will use it in later chapters. "],["model-sensitivity.html", "6 Model Sensitivity 6.1 Model Sensitivity 6.2 Local sensitivity 6.3 Assessing sensitivity in a nitrogen budget model 6.4 Sensitivity in an aquatic ecosystem 6.5 Letting Sensitivity Analysis Guide Next Steps", " 6 Model Sensitivity Reading: Chapter 11, Soetart and Herman (2009) Model sensitivity is part of model testing and validation (Soetart and Herman 2009). In this chapter, we assess model sensitivity, that is, the sensitivity of model outcomes to model inputs. Sometimes, those inputs are small perturbations of the state variables, and we often refer to this as local stability analysis. In contrast, here we investigate the sensitivity of model output to the values of parameters we use as input. This is what we do when we calculate the sensitivity and elasticity of a demographic projection matrix. One of the reasons we assess the sensitivity of model output to the values of parameters is to determine which parameters are most important in driving the dynamics of the model. Identifying the most important parameters provides understanding of the model dynamics guides future research in making sure we have good estimates of those particular model parameters. Stability and sensitivity are, broadly speaking, opposites of each other. Stability is the tendency to remain intact, to persist, or return to a steady state, following a perturbation. Ecologists have a large lingua jargona to describe different types of stability. Sensitivity is the degree to which model outputs tend to deviate if one or more of their inputs change. Before we begin, it is worth listing questions we should ask ourselves about any model. Testing Solution Correctness Is there an analytical solution against which we can check the model output? Is there known data with which we can compare the output? Internal Logic Do the state variables remain in their expected range (e.g., biomass &gt;= 0)? If it is a closed system, do the state variables remain at zero if they are all set to zero? Is mass balance preserved? Model Verification and Validity. Our data and model don’t agree. How do we proceed? Are the data accurate and/or precise? Does the structure of the model approximate the important processes? Do the parameter values correspond to actual rates? These are important questions. We would also like to know how certain we need to be about parameter values. Model output can be very insensitive or very sensitive to small changes in the values of model parameters. Sensitivity analysis helps us pinpoint parameters to which model output might be most sensitive, that is, parameters that can have big effects on model outcomes. 6.1 Model Sensitivity How robust or stable is our model to inevitable untruths? Is our model likely to give approximately correct answers given only approximately correct structure and parameters? There are two forms of sensitivities that we can assess: responses to changes in parameters responses to changes in state variables Also, we can assess global or local behavior: Global changes broad, systematic variation in a parameter. permanence - do all state values remain &gt; 0? Local changes responses to very small changes in parameters. responses to very small changes to state variables at equilibria. 6.2 Local sensitivity Here we will assess the local sensitivity to parameters in our model of the Hubbard Brook nitrogen cycle. To assess the local sensitivity of our model to parameters, we will use code that does the following: Create a baseline, or reference data set of model output using our original parameter values. To assess long term average behavior, we typically run the model a long time, throw away early transient dynamics. We then consider the remaining output as our baseline. We could also assess short term responses instead, if we had a particular scenario in mind. Create a new parameter set that changes one parameter by a little bit. This deviate, or “little bit,” could be a very small percentage of the original value, or a small fixed value. Rerun the model using the new parameter set, and an initial state that is the same as the first state of the baseline. Calculate the difference between the baseline and new model outputs at each time step. This “difference” may take several forms, such as the absolute difference, the difference relative to the original, the squared difference, or others. Summarize the differences across time using the mean or median, or something else. Rinse and repeat steps 1-5, for each parameter in the model. Save the results in a form we can use. The above steps are an example of pseudocode. Pseudocode is a series of steps written in plain language that describe intermediate steps toward achieving some goal. The function we use follows these steps. It relies heavily on code in Chap. 11 of Soetart and Herman (2009). Below I describe arguments of the function. Let’s start by loading it. Obtain a copy of sensitivity.R and sens.fig.R and put them in an appropriate directory. I use a folder called code. The R function source() runs an R script; if the script defines a function (as these do), you will then be able to use that function. ## This script loads a function called, oddly enough, &quot;sensitivity&quot; source(&quot;code/sensitivity.R&quot;) ## ask for the arguments of the function args(&quot;sensitivity&quot;) ## function (y.initial, times, func, parms, burnin = NULL, tiny = 0.001, ## dev.type = &quot;normalized&quot;, summary.type = &quot;arithmetic_mean&quot;) ## NULL Some of the arguments are the same as for ode().Here is a bit of how the function works: y.initial is a named vector the starting point for the ODEs. By ‘named,’ we mean that the elements have names, as in y=c(A=1, B=2). times is the vector of times for which the user wants to assess sensitivity. func is the system of ODEs written for use with ode(). parms is the vector of parameter values used in func. burnin is the number of initial time steps to throw away. NULL causes the function to throw away the first half. Otherwise set an actual number (integer). tiny is the proportional deviation to the parameter (0.1 = 10% increase, -0.1 = 10% decrease). summary.type refers to how the time series of deviates are summarized. The current options are ‘arithmetic_mean,’ ‘mean_absolute’ (the arthmetic means of absolute values of the deviates), or ‘RMSE’ (the root mean squared error). dev.type refers to the type of sensitivity, either the simple ‘deviate’ (dy), ‘sensitivity’ (dy/dp), ‘elasticity’ (d(log[y])/d(log[p])), ‘normalized’ ( [dy/y] / [dp/p]). Default is ‘normalized’ which is nearly identical to ‘elasticity.’ The function returns a named list containing several elements: deviation.summary is a data frame of the summarized deviates for each parameter (rows), for each state variable (columns). dev.type is the type of deviate used. summary.type is the type of summary of the time series used. tiny is size of proportional perturbation to parameters sens.list a list of each of the original time series. 6.3 Assessing sensitivity in a nitrogen budget model Consult our previous chapter on the nitrogen budget of Hubbard Brook, watershed 6. If you have not already done so, make a script that contains nothing but the bormann2 function. Copy the block of text where we define it, paste it into a new R script, and save it as bormann2.R. Next we prepare our workspace, by removing extraneous objects. rm( list = ls() ) # delete (remove) everything in the workspace. The next line of code loads and runs two files. ## I put the script in a folder called &#39;code&#39; that resides inside ## my working directory. The following line of code runs the code in ## the script, and the script is located at code/bormann2.R source(&quot;code/bormann2.R&quot;) ## if you want to make sure that it loaded the right thing, type ## bormann2 on your command line, or highlight the word, and run it with ## Control-Enter ## We also load code to calculate the sensitivities, ## and also to create a time series figure of the deviations. source(&quot;code/sensitivity.R&quot;) # sensitivities of all variables source(&quot;code/sens_fig.R&quot;) # graph time series of sensitivity of one variable Next we begin to run it. params &lt;- c( i1 = 6.5, # precip i2 = 14.2, # fixation a1 = 79.6 / (26 * 532), # uptake a2 = (6.6 + 0.8) / 532, # throughfall and inorganic exudates a3 = (54.2 + 2.7 + 0.1 + 6.2 ) / 532, # litter, throughfall, organic exudates a4 = 69.6 / 4700, # net mineralization a5 = 3.9 /26, # export from available a6 = 0.1/4700, #export from bound K=600 ) # close parentheses initial.state &lt;- c(V = 532, A = 26, B = 4700) t &lt;- seq(from = 0, to = 500, by = 1) Next we run the sensitivity analysis. We start with parameters and initial conditions. params &lt;- c( i1 = 6.5, # precip i2 = 14.2, # fixation a1 = 79.6 / (26 * 532), # uptake a2 = (6.6 + 0.8) / 532, # throughfall and inorganic exudates a3 = (54.2 + 2.7 + 0.1 + 6.2 ) / 532, # litter, throughfall, organic exudates a4 = 69.6 / 4700, # net mineralization a5 = 3.9 /26, # export from available a6 = 0.1/4700, #export from bound K=600 ) # close parentheses initial.state &lt;- c(V = 532, A = 26, B = 4700) t &lt;- seq(from = 0, to = 500, by = 1) Next we do the sensitivity analysis. We will start with a graph, often a good place to start. The graph will show the actual deviations that arise when we alter each parameter one at a time by one percent. out &lt;- sens_fig(&quot;A&quot;, y.initial=initial.state, times=0:1000, func=bormann2, parms=params, burnin=0, tiny=0.01, relative=FALSE ) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ggplot(out, aes(Time, y, colour=Perturbation)) + geom_line() + facet_wrap(~Parameter, scales=&quot;free_y&quot;) Figure 6.1: Observed responses of the available N pool to a one percent change to each parameter. Note two features of these graphs. First, the change due to the system dynamics is relatively large and so obscures the change due the sensitivity. Second, note that sometimes a smaller parameter (here in blue) results in a higher value of the state variable. It might be more informative the (i) focus on model output after it has approach the steady state, and (ii) examine the relative departure from the reference state. Relative change of 1.0 would mean that for a 1% change in a parameter, the state variable would also change by 1%. out &lt;- sens_fig(&quot;A&quot;, y.initial=initial.state, times=0:1000, func=bormann2, parms=params, burnin=500, tiny=0.01, relative=TRUE ) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ggplot(out, aes(Time, y, colour=Perturbation)) + geom_line() + facet_wrap(~Parameter, scales=&quot;free_y&quot;) Figure 6.2: Relative responses of the available N pool to a one percent change to each parameter. Note big differences in the values of the y-axes. These graphs show that the state variable responds very consistently through time. In systems with more complicated dynamics, that will not always be the case. Next, we can assess each of the state variables and average across time. b1 &lt;- sensitivity(y.initial=initial.state, times=t, func=bormann2, parms=params, dev.type=&#39;normalized&#39;, summary.type=&quot;arithmetic_mean&quot;) The output generates a list with seven elements, but to see this structure you need to use str. ## Peak at the structure of the output str(b1) ## List of 7 ## $ deviation.summary:&#39;data.frame&#39;: 9 obs. of 5 variables: ## ..$ parameters: chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## ..$ V : num [1:9] 3.71e-05 2.92e-05 2.94e-05 -2.01e-07 -2.68e-04 ... ## ..$ A : num [1:9] 0.0648 0.1042 -0.9535 0.0831 0.5228 ... ## ..$ B : num [1:9] 3.79e-05 1.23e-01 3.73e-05 9.81e-06 6.15e-01 ... ## ..$ total : num [1:9] 0.000319 0.110914 -0.004113 0.000371 0.556306 ... ## $ dev.type : chr &quot;normalized&quot; ## $ summary.type : chr &quot;arithmetic_mean&quot; ## $ burnin : NULL ## $ tiny : num 0.001 ## $ parms : Named num [1:9] 6.5 14.2 0.00575 0.01391 0.1188 ... ## ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## $ sens.list :List of 4 ## ..$ V : num [1:251, 1:9] 0.00 4.51e-06 1.04e-05 1.61e-05 2.32e-05 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## ..$ A : num [1:251, 1:9] 0 0.0635 0.0654 0.0654 0.0654 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## ..$ B : num [1:251, 1:9] 0.00 5.70e-08 1.99e-07 8.61e-07 5.71e-06 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## ..$ total: num [1:251, 1:9] 0 0.000277 0.000286 0.000287 0.000292 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:9] &quot;i1&quot; &quot;i2&quot; &quot;a1&quot; &quot;a2&quot; ... ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;sensitivity&quot; &quot;list&quot; However, if you just type b1, it will spit out just the summary of the deviations. b1 ## parameters V A B total ## 1 i1 3.705840e-05 0.0647565594 3.790716e-05 0.0003194970 ## 2 i2 2.918269e-05 0.1041528053 1.225100e-01 0.1109138069 ## 3 a1 2.940582e-05 -0.9535105388 3.728004e-05 -0.0041134439 ## 4 a2 -2.014821e-07 0.0831473008 9.812943e-06 0.0003706962 ## 5 a3 -2.681428e-04 0.5227930990 6.145104e-01 0.5563064925 ## 6 a4 2.992411e-04 0.2238495782 -7.329505e-01 -0.6598425953 ## 7 a5 -2.021901e-05 -0.0414509485 -5.222392e-06 -0.0001870163 ## 8 a6 2.264389e-06 -0.0008868526 -1.040165e-03 -0.0009414854 ## 9 K 8.470909e-01 -0.3190643353 4.950298e-01 0.5246217855 If we want to use this summary, however, we need to specify b1$deviation.summary Next we would like to graph the summaries, for ease of interpretation. b1L &lt;- b1$deviation.summary %&gt;% pivot_longer(cols=&quot;V&quot;:&quot;total&quot;) ggplot(data=b1L, aes(x=parameters, y=value)) + geom_col() + facet_wrap(~name, scales=&quot;free_y&quot;) Figure 6.3: Model output is more sensitive to some parameters than others. In addition, different state variables are respond differently to different parameters. Note that the y-axis differs among the state variables. We can also summarize across state variables, so that we have one effect of each parameter. Here we use just V, A, and B, and calculate the root mean squared error. rmse.SV &lt;- b1$deviation.summary %&gt;% select(V:B) %&gt;% apply(MARGIN=1, function(x) sqrt( x*x / 3 )) SV &lt;- b1$deviation.summary %&gt;% select(V:B) rmse.SV &lt;- t( apply(SV, MARGIN=1, function(x) { m &lt;- mean(x) sq &lt;- (x-m)^2 sqrt( mean( sq )) }) ) rmse &lt;- data.frame(p=b1$deviation.summary$parameters, rmse=as.numeric(rmse.SV)) qplot(p, rmse, data=rmse, geom=&quot;col&quot;) So,…into what should we direct our effort? What data do we want most want to collect? These sensitivities are likely to help us direct our attention where it would be most useful. 6.4 Sensitivity in an aquatic ecosystem Here we revisit the aquatic ecosystem model, NPZD, of Soetaert and Herman. We load it, and define the parameters and initial state. Obtain a copy of the model, NPZD.R. ## load the model source(&quot;code/NPZD.R&quot;) #-----------------------# # define the model parameters: # #-----------------------# parameters&lt;-c(maxUptake =1.0, # /day ksPAR =140, # muEinst/m2/s ksDIN =0.5, # mmolN/m3 maxGrazing =1.0, # /day ksGrazing =1.0, # mmolN/m3 pFaeces =0.3, # - excretionRate =0.1, # /day mortalityRate =0.4, # /(mmolN/m3)/day mineralisationRate =0.1, # /day chlNratio =1) # mgChl/mmolN #-------------------------# # the initial conditions: # #-------------------------# state &lt;-c(PHYTO =1, # state variable initial conditions, units mmolN/m3 ZOO =0.1, DETRITUS=5.0, DIN =5.0) Next we run the dynamics for two years. times &lt;- 0:730 out &lt;- ode(state, times, NPZD, parameters) out %&gt;% as.data.frame() %&gt;% pivot_longer(cols=-time, names_to=&quot;State.var&quot;, values_to=&quot;value&quot;) %&gt;% ggplot(aes(time, value)) + geom_line() + facet_wrap(~State.var, scale=&quot;free_y&quot;) Next we check model sensitivity. outf &lt;- sens_fig(&quot;DIN&quot;, y.initial=state, times=0:1000, func=NPZD, parms=parameters, burnin=0, tiny=0.01, relative=TRUE ) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 ggplot(outf, aes(Time, y, colour=Perturbation)) + geom_line() + facet_wrap(~Parameter, scales=&quot;free_y&quot;) In this figure, we see that sensitivity varies through time. For most parameters, it can be small at some times and quite large at other times. We can see how sensitivity can depend to a huge degree on the time points at which we evaluate that sensitivity. Here we summarize those across time. # args(sensitivity) outd &lt;- sensitivity(state, times, NPZD, parameters, dev.type = &quot;normalized&quot;, summary.type = &quot;arithmetic_mean&quot;) outd$deviation.summary %&gt;% pivot_longer(cols=-parameters) %&gt;% ggplot(aes(x=parameters, y=value)) + geom_col() + facet_wrap(~name, ncol=2, scales=&quot;free&quot;) + coord_flip() # Flip x and y so we can see the parameter names Which parameters appear to have the greatest effect on model predictions? 6.5 Letting Sensitivity Analysis Guide Next Steps So, let’s say you find that the half saturation constant for grazing rate of phytoplankton by zooplankton, \\(ks_{\\mathrm{Grazing}}\\), is relatively important in governing several state variables. You also believe that it is very poorly estimated. Because you believe that your estimate is very uncertain, you should probably investigate and reveal to your collaborators the effect of wide variation in this parameter. . Make sure that you have exhausted your own data in its ability to estimate that constant. Perhaps we’ll explore that elsewhere. Make sure that you have exhausted the literature as you combed through older papers that might have estimates. * Make sure that you’re exhausted generally. One of the trickiest aspects of gathering parameter estimates from the literature or from other people’s data is converting their units to units you need. What units do you need? Are you trying to estimate a flux (the mass of material flowing from one compartment to another per unit time)? Are you trying to estimate a mass-specific rate constant? What do others provide? Estimates of moles or grams? Molecular or elemental quantities (e.g., NH\\(_4\\) or N)? All or at least the important sources of elemental quantities (NH\\(_4\\) and NO\\(_3\\) vs. just one)? Per unit area or volume? What is their time scale? Conversion between what others provide and what you need can bee tricky, but it is essential to get it right. Assuming that you have worked hard at the above tasks but are still more in the dark than you would like, what is left to you? What folks often do is to rely on first principles and logical constraints to find upper and lower limits to the parameter, and display or summarize numerical the variation in outputs that result. The local sensitivity analysis showed us the potential effects of variation in parameters. Here what we want to do is determine for ourselves the potential variation in the parameter itself, and then use simulation to discover the consequences of the a priori variation in the parameter. So, our current estimate of the grazing half saturation constant, \\(ks_{\\mathrm{Grazing}}\\), is 1.0. If we examine NPZD(), we find that the shape of this resource limitation function: ksGr &lt;- 1 ## let Phytoplankton be x, and vary between 0 and 6 mmolN/m3 ## because that is the approximate range we see in the dynamics curve(x/(ksGr + x), 0, 6) If we increase or decrease the half saturation constant, what will we see? ksGr &lt;- c(.5, 1, 2) # let Phytoplankton vary more curve(x/(ksGr[1] + x), 0, 10) curve(x/(ksGr[2] + x), 0, 10, add=TRUE, lty=2) curve(x/(ksGr[3] + x), 0, 10, add=TRUE, lty=3) legend(&quot;bottomright&quot;, legend=paste(ksGr), lty=1:3) Given this range, is there some basis for selecting one of these? Recall that this is multiplied by the maximum grazing rate (and zooplankton); therefore, we might want to ask ourselves how that maximum rate was determined. For instance, was it estimated at moderate phytoplankton concentrations or when phytoplankton were truly unlimited? If we have some data from the literature suggesting particular values, we might use these as maxima and minima, or we might go beyond them. Sometimes, all we have is a guess. If a parameter appears to be important, then we might simply pick a value and then double it and halve it, or investigate 10-fold increases. Here we do the former. Let the half saturation constant vary between 0.5 and 2. Generate the dynamics. Plot outputs together. ## reference run ## make a copy of the parameters myP &lt;- parameters ## integrate and make a data frame out_1 &lt;- as.data.frame( ode(state, times, NPZD, myP) ) ## add identifier out_1 &lt;- out_1 %&gt;% ## make a new column mutate(ksGrazing = 1.0) # code as text ## Change the half saturation constant myP[&quot;ksGrazing&quot;] &lt;- 0.5 ##rerun and make a data frame out_0.5 &lt;- as.data.frame( ode(state, times, NPZD, myP) ) ## add identifier out_0.5 &lt;- out_0.5 %&gt;% ## make a new column mutate(ksGrazing = 0.5) # code as text ## Change the half saturation constant myP[&quot;ksGrazing&quot;] &lt;- 2 ## rerun and make a dataframe out_2&lt;- as.data.frame( ode(state, times, NPZD, myP) ) ## add identifier out_2 &lt;- out_2 %&gt;% ## make a new column mutate(ksGrazing = 2.0) # code as text out.all &lt;- rbind(out_0.5, out_1, out_2) out.all %&gt;% pivot_longer(cols=PHYTO:TotalN, names_to=&quot;State.var&quot;, values_to=&quot;value&quot;) %&gt;% ggplot(aes(time, value, colour=as.factor(ksGrazing))) + geom_line() + facet_wrap(~State.var, scale=&quot;free_y&quot;) Well, then… . Aren’t you glad you tried that! 6.5.1 Your turn Take a stab at varying something in your preferred model. You can use anyone’s data - we are all in this together. Just give proper attribution. You can also come up with creative ways to quantify an output difference from a reference in an output. Often, we come up with a useful quantitative metric, run a larger number of scenarios (e.g., letting a parameter take 20 different values), and then plot the relation between the parameter and your quantitative measure of difference in output. Be creative. See what you find. "],["MEL.html", "7 Multiple Element Limitation 7.1 Rastetter et al. 1997 7.2 Implementing MEL", " 7 Multiple Element Limitation 7.1 Rastetter et al. 1997 Readings: Chapters 1, 2 Bormann, Likens, and Melillo (1977) E. Rastetter, Agren, and Shaver (1997) Soetaert and Herman (2009), pages … E. B. Rastetter and Shaver (1992) proposed a model of multiple-element limitation of vegetation growth (MEL). E. Rastetter, Agren, and Shaver (1997) extended that to include soil pools. We replicate this here. It is similar to our model of (Bormann, Likens, and Melillo 1977), but now includes both carbon and nitrogen.2 By now, you’ve had some practice working with models, so take a gander at this one (Fig. 7.1). Figure 7.1: Fig. 1 from Rastetter et al. 1997. Variables \\(B\\), \\(E\\), and \\(D\\) elements in vegetation biomass, inorganic form (e.g., soil solution, or atmospheric CO\\(_2\\)), and organic form in soil detritus. Variable \\(V_i\\) is allocation related to nutrient acquistion. \\(U\\) is uptake by vegetation or microbes in detritus (broadly defined). \\(R\\) is renewal into the inorganic pools. \\(R_{Ne}\\) is N deposition from an external source, and \\(L_{Ne}\\) is loss via leaching or denitrification. The little triangles or bowties on fluxes refer to some rate-controlling function; \\(B\\) and \\(V\\) exchange information (dotted lines) that influences fluxes. Other terms are explained in the Table below. Figure 7.2: Table 1 (Rastetter et al 1997). MEL uses the parameter \\(A\\) for “acclimation,” which we can think of as compensation by vegetation that is associated with homeostatic responses to resource supply rates that differ from a stoichiometric ideal. Acclimation has a perfectly good meaning in ecophysiology, which is short- to near-term reversible physiological adjustment in response to changing conditions. In MEL, “acclimation” is much broader, and in this early version, it means shifts in the relative rates of C and N accumulation in vegetation, resulting in and from changes in the C:N ratio of vegetation. This occurs due to the ratio of wood to non-woody tissue, defined by \\(q\\). Compensatory dynamics by vegetation, or “acclimation,” is a very important part of this model. “Vegetation” is a complex adaptive system that undergoes diverse changes across many time scales. This could happen through physiological, ecological, and evolutionary changes in plant species composition, ratios of root, stem, leaves and reproductive structures, or changes in tissue C:N ratios. “Acclimation” covers a range of responses with very different time scales, and the variable \\(A\\) is how these changes are incorporated into this ecosystem model. In a later section, we explore the meaning and implications of MEL acclimation. In MEL, vegetation “acclimates” through differential allocation to carbon vs. nitrogen uptake functions. Rastetter et al. refer to this as uptake “effort.” They use this anthropomorphic term “effort” for a pattern of allocation, as in “…increasing uptake effort to acquire limiting soil nutrients.” MEL uses the state variables, \\(V_C\\) and \\(V_N\\), to represent this “effort” or allocation of uptake of C and N respectively. In a later section, we explore the meaning and implications of MEL effort. Both vegetation and microbes acclimate in MEL, in a manner inversely proportion to the relative concentrations of nutrients. In both vegetation and microbes, this results in a tendency toward maintenance of optimal C:N ratios. However, the model handles them differently. It assumes that the acclimation in vegetation is slow while the acclimation in microbes is instantaneous. Exercise Identify terms or expressions in the model that link the two element cycles (C, N). That is, find expressions that are functions of state variables of the two elements. See if you can get a sense of the dependencies. Exercise Find examples of expression that are linear functions of state variables (Y = aX). Find examples of nonlinear functions of the Michaelis-Menten form (aka Type II functional response) (Y=a X/(k+X)). 7.1.1 The model Here we write out the model nearly as written in Rastetter et al. (1997), but with some reordering. By default, R is an interpreted language rather than a compiled language. What that means for us is that R processes commands one line at a time. Therefore, if a function F requires parameter x, we have to make sure parameter x is defined before we use function F. In our implementation of MEL below, Mc requires Ucm, so Ucm needs to be defined before Mc. Copy this into a new script and name it ‘mel2.R.’ In that script, add more comments to those I started. Once you have saved your script, you can use source('mel2.R') to load the function. mel2 &lt;- function(time, y, p){ with(as.list(c(y,p)), { ## Carbon is 1 and Nitrogen is 2 in ## E - inorganic; D - soil organic w/ microbes; B - veg biomass; ## V - acclimation ## Fig. 1 of Rastetter et al. (1997) ## Gross UPTAKE of inorganic nutrients by Microbes ## Note rearrangement of terms (cf. Rastetter et al. 1997) ## (uptake) X (carbon availability) X (stoichiometric optimization) Unm &lt;- alpha_n*En/(k_nm + En) * psi*Dc * Dc/(theta*Dn) Ucm &lt;- alpha_c*Ec/(k_cm + Ec) * psi*Dn * (theta*Dn)/Dc # alpha_c=0 ## Microbial consumption ## nutrient availability + gross uptake Mc &lt;- psi*Dc + Ucm Mn &lt;- psi*Dn + Unm Lambda_c &lt;- epsilon_c * theta*Mn/(Mc + theta*Mn) Lambda_n &lt;- epsilon_n * Mc/(Mc + theta*Mn) ## C:N ratio (units of C) q &lt;- q_w*Bc/(k_q + Bc) ## SURFACE area related to nutrient acquisition (0 &lt; z &lt; 1) Sc &lt;- (Bc + q*Bn)^zc Sn &lt;- (Bc + q*Bn)^zn ## UPTAKE by Vegetation ## uptake constant X rel. effort X Surface area X resource conc. Ucv &lt;- gc * Vc * Sc * Ec/(kc + Ec) Unv &lt;- gn * Vn * Sn * En/(kn + En) ## LOSSES Lcv &lt;- mc*Bc Lnv &lt;- mn*Bn Lce &lt;- beta_ce * Ec # beta_ce = 0 # export = 0 Lne &lt;- beta_ne * En # leaching or denitrification Lcd &lt;- beta_d * Dc Lnd &lt;- beta_d * Dn ## REPLENISHMENT to the inorganic pools (R) Rcm &lt;- Mc*(1-Lambda_c) # C respiration Rnm &lt;- Mn*(1-Lambda_n) # N mineralization Rcv &lt;- rc*Bc Rnv &lt;- rn*Bn ## Available elemental nutrients ## Set dE_i/dt = 0 to control the inorganic pools externally dE_N &lt;- Rne + Rnm + Rnv - Lne - Unm - Unv ## dE_C &lt;- Rce + Rcm + Rcv - Lce - Ucm - Ucv dE_C &lt;- 0 ## Nutrients in vegetation BIOMASS dB_C &lt;- Ucv - Rcv - Lcv dB_N &lt;- Unv - Rnv - Lnv ## Nutrients in DETRITUS dD_C &lt;- Lcv + Ucm - Lcd - Rcm dD_N &lt;- Lnv + Unm - Lnd - Rnm ## Acclimation (compensation) potential ## mass-specific growth rates Grc &lt;- dB_C/Bc Grn &lt;- dB_N/Bn ## symmetric C:N differential plus relative growth difference A &lt;- log(Bc/(q*Bn)) + tau * (Grc - Grn) ## Relative effort toward compensation ## if(A&gt;0) Vstar &lt;- Vc else Vstar &lt;- Vn Vstar &lt;- (A &gt; 0) * Vc + (A &lt; 0) * Vn dV_C &lt;- -1*a*A*Vstar dV_N &lt;- -1 * dV_C ## AUXILIARY ecosystem variables NPP &lt;- Ucv - Rcv Net_N_uptake &lt;- Unv - Rnv Ecosystem_N=Bn+En+Dn Ecosystem_C=Bc+Ec+Dc return(list(c(dE_C, dE_N, dB_C, dB_N, dD_C, dD_N, dV_C, dV_N), NPP=NPP, Net_N_uptake=Net_N_uptake, Ecosystem_C=Ecosystem_C) ) }) } 7.1.2 Parameters Here we start with a vector of model parameters whose description and units are found in Table 1 above. The numeric values come from Table 2 in Rastetter et al. (1997). You should copy these into your own script, and then add your own comments to each parameter, as I started to. ## Parameter set for a closed ecosystem p.c &lt;- c( # carbon, nitrogen Rce=0, Rne=0, # no renewal from outside the ecosystem gc=1116, gn =23.77, rc=0.02055, rn=0.01955, mc=0.02055, mn=0.08009, kc=350, kn=5, zc=0.11, zn=0.16, epsilon_c=0.6, epsilon_n=0.6471, alpha_c=0, alpha_n = 0.01092, k_cm = 1, k_nm=1, beta_ce = 0, beta_ne = 0, # Other a=5, tau=3, # damping coef q_w=439, k_q=26290, theta=8.628, psi=0.04321, beta_d=0 ) Exercise To help you better grasp what is going on in the model, redraw Fig. 7.1 on a chalkboard or a larger piece of paper and add as much detail as you can fit. Include model parameters and definitions and explanations – whatever helps you make sense of the processes, and their mathematical representation. 7.2 Implementing MEL Table 2 of Rastetter et al. also include initial values for the state variables, which we include here. init.states &lt;- c(Ec=350, En=1, # Inorganic nutrients g/m^2 Bc=22000, Bn=110, # Biomass nutrients g/m^2 Dc=13000, Dn=521, # Detritus nutrients g/m^2 Vc=0.5, Vn=0.5 # allocation &quot;effort&quot; unitless ) In addition to initial values of state variables, E. Rastetter, Agren, and Shaver (1997) (Table 2) list initial fluxes. We don’t have to worry about those; the model calculates the initial fluxes from parameters and from initial values of the state variables. 7.2.1 Increasing atmospheric carbon with events in ode() Rastetter et al. (1997) double CO\\(_2\\) concentrations at an early time point in their simulations. We do that using an events in the ode() function. An “event” in an ode() model occurs when the value of a state variable changes suddenly at a one or more time points. In one sense, these are like ecological disturbances. For instance, we could add 1 kilogram of phosphorus to a lake in a one-time pulse, or halve primary producer biomass at 10 random times over a one hundred year interval. We can create events using either a function, or with a data frame. Here we create a data frame, using the required four variables: ‘var’ selects the state variable to change. ‘time’ is the time at which the event occurs. ‘value’ is the numeric value associated with the change. ‘method’ is the operator that operates on the ‘value’ and the state variable: ‘add’ will add the value to the state variable, ‘mult’ will multiply the state variable by the value, and ‘replace’ will simply replace the state variable with the value (the value can be a positive or negative number). event.df &lt;- data.frame(var=&quot;Ec&quot;, time=0.005, value=2, method=&quot;mult&quot;) event.df ## var time value method ## 1 Ec 0.005 2 mult This event will double atmospheric carbon concentration very shortly after the start of the simulation. We will run the model for 1000 years as in Rastetter et al. (1997), and specify that R return values for a logarithm series of time steps. That way, we see detail in the short term response when the dynamics are quite rapid. ## 10^3 = 1000 years t=10^seq(-3, 3, by=.001) Next, we first run the model using parameters for a closed ecosystem, and then for a partly open system (E. Rastetter, Agren, and Shaver 1997). Check with Rastetter et al. to find out what they mean by closed vs. partly open, but I bet you can at least figure out what they mean by a closed system. ## Closed ecosystem out.c &lt;- ode(init.states, t, func=mel2, parms=p.c, events=list(data=event.df) ) ## Partly open p.o &lt;- p.c p.o[&quot;Rne&quot;] &lt;- 1 p.o[&quot;beta_ne&quot;] &lt;- 1 out.o &lt;- ode(init.states, t, func=mel2, parms=p.o, events = list(data = event.df) ) You will probably get a warning message that R has included our event time into the integration procedure. That is a good thing; if the time we selected for our event was not also a time for which we choose output, ode will include it automatically in order to generate the event. Next we convert the simulation data sets to data frames, add variables, combine, rearrange, and plot the results. ## convert and add a variable out.c &lt;- out.c %&gt;% as.data.frame() %&gt;% mutate(N.cycle = &quot;closed&quot;) out.o &lt;- out.o %&gt;% as.data.frame() %&gt;% mutate(N.cycle = &quot;open&quot;) ## combine &quot;row-bind&quot; out.all &lt;- rbind(out.c, out.o) ## rearrange to a &#39;tidy&#39; and long format outL &lt;- pivot_longer(out.all, cols=-c(time, N.cycle), names_to=&quot;State_var&quot;, values_to=&quot;g.sq.m&quot;) ## plot ggplot(outL, aes(x=time, y=g.sq.m, colour=N.cycle)) + geom_line() + scale_x_log10() + facet_wrap(~State_var, ncol=3, scales=&quot;free&quot;) Figure 7.3: Simulated dynamics of carbon and nitrogen in an ecosystem with both C- and N-limitation, in a closed and partly open ecosystem. Recreates Fig. 3 from Rastetter et al. (1997), in part. Cool, right? Compare this with Figure 3 (Rastetter et al. 1997). What does this leave out? Exercise Create a gradual rise in CO\\(_2\\) using ‘events.’ Determine a predicted CO\\(_2\\) scenario, and create a data frame with annual events for the next 100 y. Compare it to the first 100 years of instantaneous increase. Is this a reasonable comparison? References "],["optimization.html", "8 Optimization 8.1 Introduction 8.2 Bormann logistic model 8.3 Fitting a model to data 8.4 Trajectory matching 8.5 Two (or more) parameters at a time 8.6 Other considerations: weighting deviations", " 8 Optimization This chapter walks through an example of computational techniques to improve our estimates of ecosystem fluxes. 8.1 Introduction Sometimes we want to find values of parameters by picking those that cause the model output to match observed data as closely as possible. We call this optimization or calibration. Usually calibration refers to the entire modeling process, whereas optimization refers to the computational techniques used to make the model-data match as close as possible. In general, the goal of optimization is to minimize the differences between data and a model. Another way to think about this is that we want a model that would maximize the probability that the data could have been generated by that model. This approach is maximum likelihood. That is what we will do here. If you are interested in these topics, I recommend highly a great introduction by Ben Bolker (Bolker 2008). We will use our most recent Bormann model, with self-limiting vegetation, and an axiliary variable for stream export (‘loss’). We start by loading libraries we will need, and cleaning up the workspace. # rm( list = ls() ) # clean the workspace, with rm() ## load the libraries we need ## install these if you have not already done so. library(tidyverse) # data management and graphing library(deSolve) # ODE solver library(bbmle) # maximum likelihood estimation (Ben Bolker, MLE) 8.2 Bormann logistic model Acquire a copy of BormannLogistic.R. The following is code to run the ODE model. First we source the R script to load the model, and then we inspect it to make make sure that it is what we expect. # The next line of code loads and runs a file. It requires that this script # is in R&#39;s working directory. # Alternatively, you could include the entire path in the file name. # getwd() # will tell you what the working directory is. source(&quot;code/BormannLogistic.R&quot;) # bormann.logistic Next we recreate the parameters and integrate to remind ourselves of of the model. See Chapter 2 for details. p &lt;- c( I1 = 6.5, # deposition I2 = 14.2, # N fixation a1 = 79.6/(532*26), # uptake by vegetation (V) a2 = (6.6 + 0.8)/532, # root exudation into available pool a3 = (54.2 + 2.7 + 6.2 + 0.1)/532, # litter fall, etc. a4 = 69.6/4700, # net mineralization a5 = 3.9/26, # export from available pool to stream a6 = 0.1/4700, # export from bound pool to stream K = 600 # carrying capacity of vegetation ) ## Initial values y0 &lt;- c(V = 532, A = 26, B = 4700) ## times that we want R to return t &lt;- seq(from = 0, to = 10, by = .1) ## Integrate out &lt;- ode(y = y0, times = t, func = bormann.logistic, parms = p) ## rearrange to long format and plot out2 &lt;- out %&gt;% as.data.frame() %&gt;% pivot_longer(cols=-time) ggplot(out2, aes(time, value)) + geom_line() + facet_wrap(~ name, scales=&quot;free&quot;, nrow=1) Figure 8.1: Our model’s output. 8.3 Fitting a model to data When we ‘fit models to data,’ we typically use a computational tool to minimize the differences between data and the state variables in our model output. But what are our data, and what and how do we minimize these? Do we compare trajectories, or just end points? Do we minimize deviations between all state variables and the data, or just those we are confident about? Do we keep the deviations so that they are on the original scale of the data, such that large pools carry more weight than small pools? Do we scale them to give each of the state variables more similar weight? 8.3.1 Minimization In most optimization procedures, we follow a series of steps: select state variables in our model to compare to observed data, select model parameters to optimize, create an objective function that calculates the deviation between model predictions - the deviance, select an algorithm that varies our model parameter, recalculates the deviance, and compares the new deviance to previous deviances, repeats the previous step until a smaller deviance can no longer be found. Here is our example of an objective function that searches for values of \\(a_4\\) (mineralization rate) that minimize deviations between the model output (values of \\(V\\), \\(A\\), \\(B\\) at \\(t = 500\\)) and the data on pool size, provided by Bormann et al. (1977). In our first example, we will compare endpoints of model output for the three state variables (A, V, B) at year 500 to our 1977 snapshot of data from Boprmann et al. (1977). We also need to choose one or more parameters to optimize. We will select one that is associated with high sensitivity: mineralization rate, \\(a_4\\). 8.3.2 Creating the objective function The point of the objective function is to calculate the difference between model output and data. It needs to generate model output using the parameter(s) of interest, and then calculate a number that quantifies the total difference between the predictions and the data. This calculation can take many forms, but some are more useful than others: Sum or mean of the squared deviations, Entropy, Sum of the negative log-likelihoods. If we minimize the negative log-likelihoods, then we will have maximized the likelihood of the data, given the model we used. We can then use this quantity to calculate frequentist confidence intervals and Bayesian credible intervals, if we have made wise choices about our probability model (e.g., Normal, binomial) and if our data conform to certain underlying assumptions (e.g., conditional independence). These are important choices and assumptions which we won’t have a chance to explore. See Bolker (2008) for an excellent introduction. Below is our first objective function, with comments. First a word about our probability model. The probability model we use in this instance is the Normal or Gaussian distribution: we assume the deviates are normally distributed. We specify the likelihoods using the built-in R function dnorm() which returns the probability density for particular data, given a mean and a standard deviation. For instance, assuming of mean of 10 and a standard deviation of 1 results in a bell-shaped probability density function centered on ten: m &lt;- 10; SD &lt;- 1 ## calculating the probability density for all values ## between 7 and 13 curve(dnorm(x, m=m, sd=SD), 7, 13) If we want to know the probability density of \\(x=9\\), then we can find just that: dnorm(9, m=m, sd=SD) ## [1] 0.2419707 Now imagine our model output is 10, and our observed datum is 9. Got that? Okay, now imagine we collect all of those probability densities for all of our model-data comparisons. That is what our objective function will do. Our objective function will use this code: nll &lt;- - sum( dnorm(data, mean=model.output, sd=SD.data, log=TRUE)) If we analyze this from the inside out, we might see that the mean of our assumed Normal distribution is whatever the model predicted. The standard deviation of our assumed Normal distribution comes from the deviates. We use the argument log=TRUE because it allows the computer to be more accurate, and minimizing the log or the original scale has the same result. The function dnorm() calculates all of the probability densities for the data given the model. We then sum all of these. The sum is the log of the likelihood of the data, given the model. We finally use the negative value so that we can minimize this quantity, because nearly all such optimization algorithms minimize rather than maximize the objective function. We will use an optimization routine that measures the likelihood for many values of our parameter of interest, and identify the value that maximizes the likelihood. This procedure will thereby provide the maximum likelihood estimate of the parameter of interest. It is called this because the procedure selects the value of the parameter that maximizes the likelihood of the data, given the model. Our objective function takes as arguments the various R objects it needs to run. It then first generates model output, given the parameter estimate, then calculates the sum of the negative log-probability densities. nll.bormann.mineralization &lt;- function(a4, y0, t, p, our.data) { ## an objective function whose arguments includes ## a4 - the parameter of interest ## Assign a new value of the parameter to our set of parameters p[&#39;a4&#39;] &lt;- a4 ## run the model ## When you use this objective function later, ## you will need to make sure that ## -- y0 is what you intend ## -- t has the values that you need, where the end point is ## the endpoint you want, ## -- the parameters, p, are the original, or intended, set. out &lt;- ode(y = y0, times = t, func = bormann.logistic, parms = p) ## store the last values of the state variables ## determine the index for last row of output nr &lt;- nrow(out) ## extract the last row of the state variables you want model.output &lt;- out[nr,c(&quot;V&quot;, &quot;A&quot;, &quot;B&quot;)] ## Calculate the sum of the negative log-likelihoods ## first the deviates diffs &lt;- model.output - our.data ## next the standard dev of the deviates. SD &lt;- sqrt( sum(diffs^2)/length(diffs) ) ## neg log likelihood nll &lt;- -sum( dnorm(our.data, m=model.output, sd=SD, log=TRUE)) ## return the value nll } Next, we use our objective function with optim() to find the value of a4 that minimizes the objective function. First, we’ll define the data to which we compare the model. We need to use the same name that we used in the objective function. observed.data = c(V = 532, A=26, B=4700) 8.3.3 optim() Next, we use optim, where par is a named vector of the parameter(s). fn is the objective function. other R objects needed for the function. an optimization method;BFGS is method appropriate for when we have a single parameter. ## specify times we want. t &lt;- c(0, 500) fit0 &lt;- optim(par=c(a4=0.015), fn = nll.bormann.mineralization, our.data=observed.data, y0=y0, t=t, p=p, method=&quot;BFGS&quot; ) Now let’s examine the result of the optimization. fit0 ## $par ## a4 ## 0.01816917 ## ## $value ## [1] 15.26674 ## ## $counts ## function gradient ## 65 5 ## ## $convergence ## [1] 0 ## ## $message ## NULL The par is the value of the parameter for which the minimum was reached. Compare with the value of mineralzation rate that we started with was \\(a_4 = 69.6/4700 =\\) 0.015. The value is the value of the objective function, which was minimized by the parameter value par. Now let’s rerun the model with the optimized value, and look at the output. We first create a new parameter vector, and insert the optimized mineralization rate. p.opt &lt;- p p.opt[&#39;a4&#39;] &lt;- fit0$par[1] Now we can rerun the ODE model with the original and the optimized values of the mineralization rate. out.original &lt;- as.data.frame( ode(y = y0, times = 0:500, func = bormann.logistic, parms = p)) out.opt &lt;- as.data.frame( ode(y = y0, times = 0:500, func = bormann.logistic, parms = p.opt)) When we plot both sets of ouput and the data, we see the differences among all three. out.original &lt;- out.original %&gt;% mutate(run=rep(&quot;original&quot;, nrow(out.original))) out.opt &lt;- out.opt %&gt;% mutate(run=rep(&quot;optimized&quot;, nrow(out.opt)) ) out.b &lt;- rbind(out.original, out.opt) out.b.long &lt;- out.b %&gt;% pivot_longer(cols=V:loss, names_to=&quot;Variable&quot;, values_to=&quot;value&quot;) out.b.long &lt;- out.b.long %&gt;% mutate( our.data=case_when(Variable==&quot;V&quot; ~ observed.data[&#39;V&#39;], Variable==&quot;A&quot; ~ observed.data[&#39;A&#39;], Variable==&quot;B&quot; ~ observed.data[&#39;B&#39;]) ) ggplot(out.b.long, aes(x=time, y=value, colour=run)) + geom_line() + geom_line(aes(x=time, y=our.data), linetype=2) + facet_wrap(~Variable, scales=&quot;free&quot;) We can see that with our new value for mineralization, the model now correctly predicts the value of the bound soil nitrogen. This is in part because \\(B\\) has much larger values than (1000s) that either \\(V\\) (100s) or \\(A\\) (10s). At the end of this chapter, we cover the weighting of state variables a little more. 8.3.4 mle2() Above, we walked through he use of optim(), but now let’s do something vary similar, but where the output allows us to calculate a confidence interval. We use a function from Bolker’s bbmle package. The arguments to mle2() differ from those of optim(). In particular, our parameters that we want to optimize are specified in the start argument, and additional R objects are speified in the data argument. Both of these arguments must be lists. Also, the objective function must return the negative log-likelihood, See ?mle2 for more information. fit.mle &lt;- mle2(minuslogl = nll.bormann.mineralization, start=list(a4=0.015), data=list(our.data=observed.data, y0=y0, t=t, p=p), method=&quot;BFGS&quot; ) summary(fit.mle) ## Maximum likelihood estimation ## ## Call: ## mle2(minuslogl = nll.bormann.mineralization, start = list(a4 = 0.015), ## method = &quot;BFGS&quot;, data = list(our.data = observed.data, y0 = y0, ## t = t, p = p)) ## ## Coefficients: ## Estimate Std. Error z value Pr(z) ## a4 0.01816917 0.00016373 110.97 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## -2 log L: 30.53348 Now we calculate the likelihood profile of the parameter, and a 95% confidence interval. llh.profile &lt;- profile(fit.mle) plot(llh.profile) confint(llh.profile) ## 2.5 % 97.5 % ## 0.01774460 0.01859578 If we return to our choices described above, we might consider several things. First, we might think that we have relatively accurate and precise estimates of the vegetation and available pools, but poor estimates of the bound pool. Therefore, we might want to fit the model to just the vegetation and available pools. However, given that the system is all connected, we may not want to take such drastic action. These huge differences in uncertainty result in different estimates of mineralization rate. Thus, the more we know, the more we can learn. 8.4 Trajectory matching Up until now, we have compared model output to a data for just a single time point. Here we fit the model to time series, and try to match the trajectory of the model with that of the data, for a single state variable, stream nitrogen export. First we read in an environmental data set, and visually inspect the data. I have put my data sets in a subdirectory called, “data,” and so read it in from that directory. ## we skip the first 7 lines because those contain metadata HB.df &lt;- read.csv(&quot;data/env_data_HB.csv&quot;, skip=7) ggplot(data=HB.df, aes(x=year, y=value)) + geom_line() + facet_wrap(~factor, scales=&#39;free&#39;) Figure 8.2: Hubbard Brook Watershed 6 N export. We will use just the export data, so I’ll extract that. Ne.df &lt;- filter(HB.df, factor==&quot;ann.exp.TN.kg.y&quot;) As our model is based on Bormann et al. (1977), we will, somewhat arbitrarily, take 1975 as our year 0 date. With that start date, we have a total of forty years (2014-1974) worth of data that we will use to calibrate We might first want to ask which of our parameters might have the biggest effects on N export. We should start with an ecological rationale, and then c check that with a sensitivity analysis. ## load our sensitivity function source(&quot;code/sensitivity.R&quot;) # sensitivities of all variables b1 &lt;- sensitivity(y.initial=y0, times=0:100, func=bormann.logistic, parms=p, dev.type=&#39;normalized&#39;, summary.type=&quot;arithmetic_mean&quot;) b1$deviation.summary ## parameters V A B loss ## 1 I1 1.616114e-02 0.0648952101 0.0031875695 0.06312195 ## 2 I2 7.203745e-03 0.0439477822 0.0556313516 0.04428581 ## 3 a1 1.154075e-02 -0.9435602797 0.0025246797 -0.91633929 ## 4 a2 -2.045314e-02 0.0157857220 -0.0039319104 0.01521605 ## 5 a3 -1.403600e-01 0.3372101743 0.2237934605 0.33393548 ## 6 a4 1.488759e-01 0.5252500483 -0.2584417662 0.50271445 ## 7 a5 -9.176147e-03 -0.0367042540 -0.0017762039 0.93550236 ## 8 a6 -9.777624e-05 -0.0002706948 -0.0003959927 0.02848715 ## 9 K 1.490299e-01 -0.1155934256 0.0278000392 -0.11144794 From this we see that fluxes associated with the mineral pool (A, readily available for plant uptake) have the greatest effect on export (“loss”). Interestingly, plant uptake seems just as important as export rate itself. Let’s by fitting the rate of export. Next, we need an objective function that calculations the sum of squared deviations between the export state variable in our model and in our data. nll.bormann.a5.traj &lt;- function(y0, t, p, a5, export) { ## an objective function whose arguments are ## Assign a new value of the parameter to our set of parameters p[&#39;a5&#39;] &lt;- a5 ## run the model ## starting with &#39;initial.values&#39; and t for time ## t must be the same time points for which we have data. out &lt;- ode(y = y0, times = t, func = bormann.logistic, parms = p) model.output &lt;- out[,&quot;loss&quot;] ## Calculate the negative log-likelihood SD &lt;- sqrt( sum( (export - model.output)^2 )/length(export) ) nll &lt;- - sum( dnorm(export, mean=model.output, sd=SD)) nll } Now we select the data we want from the data set. We ‘filter’ out just the data we want. Below we use %in% to mean ‘within,’ so translating, the code reads “’”filter out the subset of Ne.df for which the variable ‘year’ is within the define years in ‘year.subset,’ and then make a new time variable ‘y’ where 0 is 1975.” year.subset &lt;- 1975:2014 # data.subset &lt;- Ne.df %&gt;% filter(year %in% year.subset) ##View(data.subset) t &lt;- data.subset$year-1975 y0 &lt;- c(V = 532, A = 26, B = 4700) p2 &lt;- p fit.a5 &lt;- optim(f = nll.bormann.a5.traj, par=c(a5 = 0.15), y0=y0, t=t, p=p2, export=data.subset$value, method=&quot;Brent&quot;, lower = 0.01, upper=0.5, hessian=TRUE) # list(est=fit.a5$par, # SD=sqrt( solve(fit.a5$hessian) ) ) fit.a5.mle2 &lt;- mle2(nll.bormann.a5.traj, start=list(a5=0.05), data=list(export=data.subset$value, y0=y0, t=t, p=p2), method=&quot;BFGS&quot;) summary(fit.a5.mle2) ## Maximum likelihood estimation ## ## Call: ## mle2(minuslogl = nll.bormann.a5.traj, start = list(a5 = 0.05), ## method = &quot;BFGS&quot;, data = list(export = data.subset$value, ## y0 = y0, t = t, p = p2)) ## ## Coefficients: ## Estimate Std. Error z value Pr(z) ## a5 0.052238 0.019427 2.689 0.007167 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## -2 log L: -15.46905 bp &lt;- profile(fit.a5.mle2) plot(bp) confint(bp) ## 2.5 % 97.5 % ## 0.006256104 0.100196747 fit.a5 ## $par ## [1] 0.05223814 ## ## $value ## [1] -7.734526 ## ## $counts ## function gradient ## NA NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ## ## $hessian ## [,1] ## [1,] 2647.194 ## original value p[&#39;a5&#39;] ## a5 ## 0.15 p.a5 &lt;- p p.a5[&#39;a5&#39;] &lt;- fit.a5$par[1] Now we can rerun the ODE model with the original and the optimized values of the mineralization rate. out.orig &lt;- as.data.frame( ode(y = y0, times = 0:39, func = bormann.logistic, parms = p) ) out.a5 &lt;- as.data.frame( ode(y = y0, times = 0:39, func = bormann.logistic, parms = p.a5) ) Now we would like to do is to compare the trajectories of the data and the model. plot(out.orig$time, out.orig$loss, type=&#39;l&#39;, lty=3, ylim=c(0,10)) lines(out.orig$time, out.a5$loss, lty=2) lines(out.orig$time, data.subset$value, lty=1) legend(&#39;topright&#39;, legend=c(&#39;original&#39;, &#39;fitted&#39;, &#39;data&#39;), lty=c(3,2,1)) This shows us the model tends to underestimate at first, and then consistently over estimate over time. So,…what do we make of this? Why might this make sense? A couple of things to consider when matching trajectories in data and models: are the averages similar? if the time series data have a trend (i.e. are non-stationary), does the model show a similar trend? is the variability similar in magnitude and timing? Another way to examine it is the examine how the deviations between model and data might vary over time. ## or the deviations over time (fitted - data) devs &lt;- out.a5$loss - data.subset$value qplot(x=out.a5$time, y=devs) + geom_point() + geom_smooth() If we want to check the effect of this we plot both sets of ouput and the data, we see the differences among all three. This time, we’ll take advantage of ggplot2 graphics. ## add a new column ## this will repeat the label for all rows of the data frame out.orig$version &lt;- &quot;Original&quot; out.a5$version &lt;- &quot;Fitted&quot; names(out.orig) ## [1] &quot;time&quot; &quot;V&quot; &quot;A&quot; &quot;B&quot; &quot;loss&quot; &quot;version&quot; ## combine (&#39;bind&#39;) all rows of both data frames... out2 &lt;- rbind(out.orig, out.a5) %&gt;% ## ... and then stack up all of the state variables, ## from V to loss pivot_longer(cols=V:loss, names_to=&quot;variable&quot;, values_to=&quot;value&quot;) ## plot what we want ggplot(data=out2, aes(x=time, y=value, colour=version)) + ## line graphs for time series geom_line() + ## separate each state variable in its own subfigure with its own scale facet_wrap(~variable, scales = &quot;free&quot;) 8.5 Two (or more) parameters at a time Here we use the same approach, in which we have an objective function, parameters of interest, and data. Here is our objective function, and you can see it is not very different. Find and explain the difference. nll.bormann.a5.4.traj &lt;- function(a5, a4, data, y0, t, p) { ## an objective function whose arguments are ## params - the parameter of interest ## data - data from Bormannm et al. ## Assign a new value of the parameter to our set of parameters p[&#39;a5&#39;] &lt;- a5 p[&#39;a4&#39;] &lt;- a4 ## run the model ## starting with &#39;initial.values&#39; and t for time out &lt;- ode(y = y0, times = t, func = bormann.logistic, parms = p) model.output &lt;- out[,&quot;loss&quot;] ## Negative log-likelihood ## assuming normally distributed observation error SD &lt;- sqrt(sum( (data - model.output)^2)/length(data) ) nll &lt;- - sum( dnorm(data, mean=model.output, sd=SD, log=TRUE)) } This will generate warnings, but we’ll ignore those for now. See Bolker (2008) for more information. fit2.mle &lt;- mle2(minuslogl = nll.bormann.a5.4.traj, start=list(a5=0.05, a4=0.015), data=list(data=data.subset$value, y0=y0, t=t, p=p), method=&quot;L-BFGS-B&quot;, lower=0) summary(fit2.mle) ## Maximum likelihood estimation ## ## Call: ## mle2(minuslogl = nll.bormann.a5.4.traj, start = list(a5 = 0.05, ## a4 = 0.015), method = &quot;L-BFGS-B&quot;, data = list(data = data.subset$value, ## y0 = y0, t = t, p = p), lower = 0) ## ## Coefficients: ## Estimate Std. Error z value Pr(z) ## a5 0.0275558 0.0041166 6.6939 2.173e-11 *** ## a4 0.1434689 0.0448726 3.1972 0.001387 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## -2 log L: 134.4075 Now we substitute the fitted values into a new copy of the parameters. ## original value p2 &lt;- p p2[&#39;a5&#39;] &lt;- coef(fit2.mle)[1] p2[&#39;a4&#39;] &lt;- coef(fit2.mle)[2] Now we can rerun the ODE model with the original and the optimized values of the mineralization rate. out.orig &lt;- as.data.frame( ode(y = y0, times = 0:39, func = bormann.logistic, parms = p) ) out.2 &lt;- as.data.frame( ode(y = y0, times = 0:39, func = bormann.logistic, parms = p2) ) Now we would like to do is to compare the trajectories of the data and the model. plot(out.orig$time, out.orig$loss, type=&#39;l&#39;, lty=3, ylim=c(0,10)) lines(out.orig$time, out.2$loss, lty=2) lines(out.orig$time, data.subset$value, lty=1) legend(&#39;topright&#39;, legend=c(&#39;original&#39;, &#39;fitted&#39;, &#39;data&#39;), lty=c(3,2,1)) Have at it. What can you learn today? What to we most want to know? Can we use the model to make hypotheses about processes that are not even in the model, like denitrification? 8.6 Other considerations: weighting deviations When we assess more than one state variable at a time, the model fit might be unduly affected by state variables with the largest absolute values. In our case, that might mean that our fits might be determined by how well our model describes the soil pool. Therefore, we might want to at least weight the pools more equally by using the log-transformations of the output and data. This accomplishes two things. First, it weights the variables on a more similar scale. This may not matter for the result with some methods of optimization, but it can. Second, it makes the computation more stable and reliable to use both smaller numbers, and each on a more similar scale. This can make a big difference if you are having trouble getting the optimization to give reliable, repeatable results. Another important issue is how confident we are in our observed data. If we believe that we are quite confident in some observations but know we are measuring others with low accuracy, then we can adjust for that uncertainty by using variance-weighted errors. 8.6.1 Variance weighted errors We can build on this approach by directly weighting the model-data differences by the uncertainty in data data (Soetaert and Hermann, Chap. 4), \\[\\mathrm{SSE} = \\sum_{i = 1}^{n}\\frac{\\left(m_i - d_i\\right)^2}{e_i}\\] where \\(i\\) is a particular state variable, \\(m\\) is model output, \\(d\\) is data, and \\(e\\) is the observed uncertainty of the data. Often this variance is calculated as the variance (\\(\\sigma^2\\)) of the data. The modified objective function includes a new argument, vars, which is a vector of the observed variances of the data. sse.bormann.mineralization3 &lt;- function(a4, data, vars) { ## an objective function whose arguments are ## params - the parameter of interest ## data - point estimates of the observed state variables from Bormannm et al. ## vars - observed variances of the data. ## Assign a new value of the parameter to our set of parameters p[&#39;a4&#39;] &lt;- a4 ## run the model out &lt;- ode(y = y0, times = t, func = bormann.logistic, parms = p) ## store the last values of the state variables nr &lt;- nrow(out) model.output &lt;- out[nr,c(&quot;V&quot;, &quot;A&quot;, &quot;B&quot;)] ## Calculate the sum of the squared differences ## squaring the differences makes them positives and ## weights big differences even more heavily diffs &lt;- model.output - data diffs2 &lt;- diffs^2 wdiffs2 &lt;- diffs2/vars sse &lt;- sum( wdiffs2 ) ## Return the SSE sse } Now we include estimates of the variances. Your guess is nearly as good as mine. Often the standard deviations approach or exceed the mean, which means that the coefficient of variation is \\(\\sigma/\\mu &gt; 1\\). Let’s pretend that we know the vegetation and the available pools pretty well (\\(\\sigma/\\mu \\ll 1\\)), but have great uncertainty about the bound pool(\\(\\sigma/\\mu \\gg 1\\)). vars &lt;- c(532/10, 26/10, 4700) Now we can fit the model to the data. fitv &lt;- optim(par=c(a4=0.014), fn = sse.bormann.mineralization3, data=observed.data, vars=vars, method=&quot;BFGS&quot;) Last, let’s compare the three estimates of the mineralization rate. fit0$par; fitv$par ## a4 ## 0.01816917 ## a4 ## 0.01585446 …and graph the outcomes. p.v &lt;- p p.v[&quot;a4&quot;] &lt;- fitv$par out.original &lt;- as.data.frame( ode(y = y0, times = 0:500, func = bormann.logistic, parms = p)) out.v &lt;- as.data.frame( ode(y = y0, times = 0:500, func = bormann.logistic, parms = p.v)) out.original &lt;- out.original %&gt;% mutate(run=rep(&quot;original&quot;, nrow(out.original))) out.v &lt;- out.opt %&gt;% mutate(run=rep(&quot;var.wt&quot;, nrow(out.opt)) ) out.b &lt;- rbind(out.original, out.v) out.b.long &lt;- out.b %&gt;% pivot_longer(cols=V:loss, names_to=&quot;Variable&quot;, values_to=&quot;value&quot;) ggplot(out.b.long, aes(x=time, y=value, colour=run)) + geom_line() + facet_wrap(~Variable, scales=&quot;free&quot;) Notice now that the fit to the bound pool does not dominate the fit. References "],["more-on-forcing-functions-using-data-inside-models.html", "9 More On Forcing Functions: Using data inside models 9.1 Preliminaries 9.2 N deposition 9.3 Denitrification 9.4 Annual temperature as a forcing function 9.5 In fine", " 9 More On Forcing Functions: Using data inside models In previous chapters, we showed two ways of using external data as forcing functions. In NPZD - a simple aquatic ecosystem, we used seasonal variation in sunlight (PAR) using a mathematical expression in our R ODE function. In Multiple Element Limitation, we used the events argument to force a doubling of CO\\(_2\\) concentration. Here we demonstrate two methods (one new, one old) to use environmental data inside our system of ODEs using our model of a [terrestrial nitrogen budget]{#N#}. For our new method, we introduce the use of data interpolation, implemented in R using approxfun(). Our scientific goal in this chapter is to explain the decline in nitrogen export. This is happening in New England watersheds outside of Hubbard Brook, and it is not clear why. 9.1 Preliminaries We’ll start by loading packages we need. # rm( list = ls() ) # clean the workspace, with rm() ## load the libraries we need ## install these if you have not already done so. library(tidyverse) # data management and graphing library(deSolve) # ODE solver Next, we load our own ODE function and parameters. Make sure you obtain an up to date copy of BormannLogistic.R. ## Make sure you obtain a new copy of BormannLogistic.R source(&quot;code/BormannLogistic.R&quot;) p &lt;- c( I1 = 6.5, # deposition I2 = 14.2, # N fixation a1 = 79.6/(532*26), # uptake by vegetation (V) a2 = (6.6 + 0.8)/532, # root exudation into available pool a3 = (54.2 + 2.7 + 6.2 + 0.1)/532, # litter fall, etc. a4 = 69.6/4700, # net mineralization a5 = 3.9/26, # export from available pool to stream a6 = 0.1/4700, # export from bound pool to stream K = 600 # carrying capacity of vegetation ) ## a02 0.0275559 0.0041165 6.6940 2.171e-11 *** ## a23 0.1434636 Finally, we’ll load environmental data. Note the trajectory of annual N export (kg/y, `ann.export.kg.y). ## we skip the first 7 lines because those contain metadata HB.df &lt;- read.csv(&quot;data/env_data_HB.csv&quot;, skip=7) HB.df[1,] ## year factor value ## 1 1970 ann.exp.TN.kg.y 6.802761 ggplot(data=HB.df, aes(x=year, y=value)) + geom_line() + facet_wrap(~factor, scales=&#39;free&#39;) Figure 9.1: At the Hubbard Brook LTER, annual stream N export and N deposition have declined, while average annual temperature has increased. Average annual sream flow has been highly variable. We see that average annual temperature has been increasing (meandegC) and N deposition has be decreasing. Could either of these be to blame? 9.2 N deposition We see N deposition declining (Fig. 8.2), and so perhaps that is the most obvious answer to why stream export is declining. We will use these data to determine N deposition in our model (parameter \\(I_1\\)). This is an example of a “time-varying parameter.” Creating a data-based time-varying parameter is not hard when we use interpolation. The reason we do this is because these data are only annual averages, but our ODE needs parameter values at potentially any instant in time. We will write a small function that will calculate a value for \\(I_1\\), based on surrounding data points. Below, we use the following steps: Data wrangling, learning what years we have data for and creating a time variable. Write a function to interpolate N deposition data between time points. Write code to update the model with N deposition. Generate model predictions and compare them visually to data, and to predictions from models without temperature. We first need to extract the N deposition data, and create a new variable time instead of year that starts at zero instead of 1979. ## Find out the range of years for each state variable or factor ## Exclude missing data HB.df %&gt;% filter( !is.na(value)) %&gt;% group_by(factor) %&gt;% # group the data by state vars ## calculate stats (for each state var) summarize(first_year=min(year), last_year=max(year)) ## # A tibble: 4 × 3 ## factor first_year last_year ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 ann.exp.TN.kg.y 1970 2014 ## 2 ann.flow.mm.y 1970 2014 ## 3 atmos.kgN.ha 1979 2017 ## 4 meandegC 1972 2012 Here we select the data we want. ndep &lt;- HB.df %&gt;% # select orig. data ##select non-missing deposition data only filter(!is.na(value) &amp; factor== &quot;atmos.kgN.ha&quot;) %&gt;% ## add new variable t = 0,1, 2, .... max mutate( time = year - min(year) ) Next we use approxfun() to create the function to approximate N deposition for an arbitrary time point. We then plot the data and show the linear interpolation. ndep.func &lt;- approxfun(x = ndep[,&quot;time&quot;], y = ndep[,&quot;value&quot;], method = &quot;linear&quot;, rule = 2) ## plot the data plot(x = ndep[,&quot;time&quot;], y = ndep[,&quot;value&quot;], ylab=&quot;N deposition (kg/ha/y&quot;, xlab=&quot;Year (1979-2017)&quot;) ## add the linear interpolation curve(ndep.func, 0, 38, add=TRUE, n=1001) (#fig:lin_int)Linear interpolation between annual N deposition data points, using approxfun(). We can fit smoothed lines, splines, across these points as well. R allows a variety of methods for fitting splines, and here we show two. ## natural ndep.func.nat &lt;- splinefun(x = ndep[,&quot;time&quot;], y = ndep[,&quot;value&quot;], method = &quot;natural&quot;) ## fmm: Forsythe, Malcolm and Moler (default) ndep.func.fmm &lt;- splinefun(x = ndep[,&quot;time&quot;], y = ndep[,&quot;value&quot;], method = &quot;fmm&quot;) ## plot the data plot(x = ndep[,&quot;time&quot;], y = ndep[,&quot;value&quot;], ylab=&quot;N deposition (kg/ha/y&quot;, xlab=&quot;Year (1979-2017)&quot;) ## add the smoothed interpolation curve(ndep.func.fmm, 0, 38, add=TRUE, n=1001, lty=3, lwd=2) curve(ndep.func.nat, 0, 38, add=TRUE, n=1001) legend(&quot;topright&quot;, legend=c(&quot;Natural&quot;, &quot;FMM&quot;), lty=1:2, bty=&quot;n&quot;) (#fig:spline_int)Smoothed interpolation between annual N deposition data points, using splinefun(). Next we write a new version of the ODE function that includes linear interpolation to create a new value of \\(I_1\\). bormann.dep &lt;- function(t, y, p) { with(as.list( c(y, p) ), { ### ODEs ## Rate of change = Gains - Losses ## I1 - deposition into the inorganic available pool ## I2 - N fixation ## a1 - plant uptake ## a2 - root exudate into the available pool ## a3 - litter fall ## a4 - mineralization ## a5 - is loss from Available to the stream ## a6 - is loss from the bound pool to the stream ############## ### Adding linearly interpolated N deposition I1.new &lt;- ndep.func(t) ############## ## ODEs dV.dt &lt;- (a1 * A * V - a2 * V - a3 * V) * (1 - V/K ) dA.dt &lt;- ( I1.new + a4 * B ) - ( a1 * V * A + a5 * A) dB.dt &lt;- I2 + a3 * V - (a4 * B + a6 * B) ## loss from the soil to the stream and out the watershed export &lt;- a5*A + a6*B # returning a list whose first element # is a vector of the ODE values return( list( c( dV.dt, dA.dt, dB.dt ), export=export, I1.new=I1.new ) ) } ) } Next we run the model, combine with our stream export data, and see what we get. ## This relies on the data set and functions we made above: ## ndep and ndep.func ## times that we want R to return ## just round years t &lt;- seq(from = 0, to = 38, by = 1) y0 &lt;- c(V = 532, A = 24, B = 4700) ## Integrate out &lt;- as.data.frame( ode(y = y0, times = t, func = bormann.dep, parms = p) ) ## combine with stream export data exp.data &lt;- HB.df %&gt;% filter(year &gt; 1978 &amp; factor== &quot;ann.exp.TN.kg.y&quot;) out$export.data &lt;- exp.data$value ## rearrange to long format, and reorder state variable levels out2 &lt;- out %&gt;% as.data.frame() %&gt;% pivot_longer(cols=V:export.data) %&gt;% transform(name=factor(name, levels=c( &quot;V&quot;, &quot;I1.new&quot;, &quot;export.data&quot;, &quot;B&quot;, &quot;A&quot;, &quot;export&quot;) ) ) ## plot ggplot(out2, aes(time, value)) + geom_line() + facet_wrap(~ name, scales=&quot;free&quot;, nrow=2) (#fig:ts.denitr)Declining deposition doesn’t guarantee declining export. 9.3 Denitrification Denitrification will occur in both the forest floor and from the stream, wherever there is moisture, a lack of oxygen, and a source of nitrate (Chapin III, Matson, and Mooney 2011). All else being equal, denitrification rate increases with increasing nitrate. It might cause declines in stream export because this is measured at the bottom of the watershed. Unfortunately, our model does not currently include terms for denitrification. Lucky for us, we can add them. Let’s model denitrification in both the forest floor and the stream as a first order process (\\(aX\\)). We will assume that denitrification causes loss from the forest floor, which is about 1/4 the total bound pool. Groffman and Tiedje (1989) found that well dranined Michigan forest soils lost 0.5–10 kg N/ha/y. If we take the midpoint of that range, we get 5 kg N/ha/y. We’ll assume that denitrification occurs in the stream channel after leaching from the available pool. Mulholland et al. (2008) found that about 20% of nitrate entering reference watersheds was denitrified. This means the stream retains about 80% of the nitrogen that gets leached out of the soil and into the stream. Here is a new version of our model, with both N deposition and dentrification. bormann.denitr &lt;- function(t, y, p) { with(as.list( c(y, p) ), { ### ODEs ## Growth = Gains - Losses ## I1 - deposition into the inorganic available pool ## I2 - N fixation ## a1 - plant uptake ## a2 - root exudate into the available pool ## a3 - litter fall ## a4 - mineralization ## a5 - is loss from Available to the stream ## a6 - is loss from the bound pool to the stream ### Adding N deposition I1.new &lt;- ndep.func(t) ### Adding denitrification ## a7 is the mass-specific denitrification rate in the forest floor ## a8 is mass-specific denitrification from the stream ## Thus, (1-a8) is rate of N retention in the stream ## ODEs dV.dt &lt;- (a1 * A * V - a2 * V - a3 * V) * (1 - V/K ) dA.dt &lt;- ( I1.new + a4 * B ) - ( a1 * V * A + a5 * A) ## Adding denitrification from the forest floor ## param = a7 * (~1/4 of the bound pool) dB.dt &lt;- ( I2 + a3 * V ) - a4 * B - a6 * B - a7 * B/4 ## loss from the soil to the stream leaching &lt;- a5*A + a6*B ## export out of the watershed ## a8 is loss through dentrification export &lt;- leaching - a8*leaching # returning a list whose first (and only) element # is a vector of the ODE values return( list( c( dV.dt, dA.dt, dB.dt ), I1.new=I1.new, export=export ) ) } ) } Here we add denitrification parameters. ## Forest floor denitrification (Groffman et al. 1989) ## total rate / forest floor N p[&quot;a7&quot;] &lt;- 5 / 1100 ## Stream denitrification (Mulholland et al. 2008) p[&quot;a8&quot;] &lt;- 0.2 # Next we run the model, combine with our stream export data, and see what we get. ## This relies on the data set and functions we made above: ## ndep and ndep.func ## times that we want R to return ## just round years t &lt;- seq(from = 0, to = 38, by = 1) y0 &lt;- c(V = 532, A = 24, B = 4700) ## Integrate out &lt;- as.data.frame( ode(y = y0, times = t, func = bormann.denitr, parms = p) ) ## combine with stream export data exp.data &lt;- HB.df %&gt;% filter(year &gt; 1978 &amp; factor== &quot;ann.exp.TN.kg.y&quot;) out$export.data &lt;- exp.data$value ## rearrange to long format and plot out2 &lt;- out %&gt;% as.data.frame() %&gt;% pivot_longer(cols=V:export.data) %&gt;% transform(name=factor(name, levels=c( &quot;V&quot;, &quot;I1.new&quot;, &quot;export.data&quot;, &quot;B&quot;, &quot;A&quot;, &quot;export&quot;) ) ) ggplot(out2, aes(time, value)) + geom_line() + facet_wrap(~ name, scales=&quot;free&quot;, nrow=2) Figure 9.2: Dynamics with N deposition and denitrification in forest floor and stream. 9.4 Annual temperature as a forcing function We see annual temperatures increasing so it is likely that these are influencing microbial processes associated with ecosystem flux. Increasing temperatures could be driving increases in mineralization, nitrification which could increase stream export if it is not taken up by plants or immobilized in microbial biomass. Increasing temperatures might increase denitrification rates, but like any of these, depends on precipitation as well. Below, we use the following steps: Data management, making sure years align. Write code to update the model. Generate model predictions and compare them visually to data, and to predictions from models without temperature. Here we use data on annual temperatures to scale selected rates using the \\(Q_{10}\\) relationship. We first make sure that we are modeling years for which we have both N deposition and temperature data. We need to use the same years for temperatures as we did for N deposition, in our ndep data subset. temps &lt;- filter(HB.df, factor== &quot;meandegC&quot; &amp; !is.na(value) &amp; ## year *within* the ndep years year %in% ndep$year) %&gt;% mutate( time = year - 1979 ) ## the min and max years in our data set range(temps$year) ## [1] 1979 2012 Next we can create the function to approximate N deposition for an arbitrary time point. We then plot the data and show the linear interpolation. temp.func &lt;- approxfun(x = temps[,&quot;time&quot;], y = temps[,&quot;value&quot;], method = &quot;linear&quot;, rule = 2) bormann.denitr.temp &lt;- function(t, y, p) { with(as.list( c(y, p) ), { ### ODEs ## We hypothesize that the growth function is all that Bormann et al. included. ## Therefore, our logistic inhibition term acts on the entire forest. ## Growth = Gains - Losses ## Rate of change = Growth x Self-inhibition ## I1 - deposition into the inorganic available pool ## I2 - N fixation ## a1 - plant uptake ## a2 - root exudate into the available pool ## a3 - litter fall ## a4 - mineralization ## a5 - is loss from Available to the stream ## a6 - is loss from the bound pool to the stream ### Adding N deposition I1.new &lt;- ndep.func(t) ### Adding denitrification ## a7 is the mass-specific denitrification rate in the forest floor ## a8 is mass-specific denitrification from the stream ## Thus, (1-a8) is rate of N retention in the stream ## Use temperatures ## interpolate to get instantaneous temperature inst.temp &lt;- temp.func(t) ## create a temp multiplier using Q10 = 2 in parameters TempFactor &lt;- exp( (inst.temp - ref.temp)/10 * log(Q10) ) a4.new &lt;- TempFactor * a4 a7.new &lt;- TempFactor * a7 a8.new &lt;- TempFactor * a8 dV.dt &lt;- (a1 * A * V - a2 * V - a3 * V) * (1 - V/K ) dA.dt &lt;- ( I1.new + a4.new * B ) - ( a1 * V * A + a5 * A) dB.dt &lt;- ( I2 + a3 * V ) - a4.new * B - a6 * B - a7.new * B/4 ## loss from the soil to the stream leaching &lt;- a5*A + a6*B ## export out of the watershed export &lt;- leaching - a8.new*leaching # returning a list whose first (and only) element # is a vector of the ODE values return( list( c( dV.dt, dA.dt, dB.dt ), I1.new=I1.new, export=export ) ) } ) } Here we define \\(Q_{10}\\). ## A common default value is 2 p[&quot;Q10&quot;] &lt;- 2 ## This relies on the data set and functions we made above: ## temps and temp.func ## times that we want R to return ## just round years for the years in THIS data set ## t &lt;- seq(from = 0, to = nrow(temps)-1, by = 1) p[&quot;ref.temp&quot;] &lt;- 3.5 # reference temperature from 1979 y0 &lt;- c(V = 532, A = 24, B = 4700) ## Integrate out &lt;- as.data.frame( ode(y = y0, times = t, func = bormann.denitr.temp, parms = p) ) ## combine with stream export data exp.data2 &lt;- filter(exp.data, year %in% 1979:2012) out$export.data &lt;- exp.data2$value ## combine with temperature data temp.data &lt;- HB.df %&gt;% filter(year %in% 1979:2012 &amp; factor %in% &quot;meandegC&quot;) out$temp.data &lt;- temp.data$value ## rearrange to long format and plot out2 &lt;- out %&gt;% as.data.frame() %&gt;% pivot_longer(cols=V:temp.data) ggplot(out2, aes(time, value)) + geom_line() + facet_wrap(~ name, scales=&quot;free&quot;, ncol=3) 9.5 In fine In this chapter, we described inclusion of forcing functions in an ODE model using interpolation. We did not yet answer the question of why stream N declines. One thing we did not do was explore the role of changing pool sizes. How would export respond to increasing pool sizes vs. stable pool sizes? References "],["references.html", "10 References", " 10 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
